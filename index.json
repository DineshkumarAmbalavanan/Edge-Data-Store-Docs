{
    "V1/Overview/EdgeSystemOverview.html":  {
                                                "href":  "V1/Overview/EdgeSystemOverview.html",
                                                "title":  "Edge Data Store overview",
                                                "keywords":  "Edge Data Store overview Edge Data Store is an embedded data server that runs on Linux and Windows. It is designed for small devices and you can install and run it on 64-bit Intel/AMD Intel AMD compatible and 32-bit ARM v7/v8 v7 v8 compatible chips. It offers REST programming, configuration, administrative interfaces, and a command line tool that can be used for configuring and administering EDS. EDS is not a replacement for any existing OSIsoft technology, but rather a supplement to existing products. In this release, EDS does not offer any built-in visualization or analytic support. However, if you use the REST programming capabilities built into EDS, you can write analytics or visualization programs on either Linux, Windows, or both in a variety of programming languages. Edge Data Store architecture Edge Data Store data flow Edge Data Store components Edge Data Store installation The Edge Data Store can be installed on both Linux and Windows: Edge Data Store Installation Overview . Data ingress to Edge Data Store Edge Data Store can ingress data in a number of ways. There are two built-in adapters - EDS Modbus TCP and EDS OPC UA . Additionally, data can be ingressed using OSIsoft Message Format (OMF) and the Sequential Data Store SDS REST APIs. During installation of Edge Data Store, options are presented to install either an EDS Modbus TCP adapter, or an EDS OPC UA adapter, or both. The EDS Modbus and EDS OPC UA adapters require additional configuration of data source and data selection before they can collect data in Edge Data Store. For OMF data ingress, once Edge Data Store is installed, you can start OMF ingress with no further configuration steps. The Edge Data Store is composed of components and is designed to allow the addition at a later date of additional EDS adapters if desired. Local data read and write access You can access all data in Edge Data Store by using the Sequential Data Store SDS REST API. You can use this data for local applications that perform analytics or visualization. Example EDS visualization application Example EDS analytics application Data egress from Edge Data Store The Edge Data Store can send data to both PI Data Archive (using PI Web API ) and OSIsoft Cloud Services ( OCS ). Additional configuration is necessary to send data to both OCS and PI Web API after Edge Data Store is installed."
                                            },
    "V1/OpcUa/PrinciplesOfOperationOPCUA.html":  {
                                                     "href":  "V1/OpcUa/PrinciplesOfOperationOPCUA.html",
                                                     "title":  "Principles of operation",
                                                     "keywords":  "Principles of operation The following topics provide and operational overview of the OPC UA EDS adapter, focusing on streams creation and error handling. Operational overview Adapter configuration In order for the OPC UA EDS adapter to start data collection, you need to configure the adapter. For more information, see OPC UA data source configuration and OPC UA data selection configuration . To configure the adapter, configure the following: Data source: Provide the information of the data source from where the adapter should collect data. Data selection: Perform selection of OPC UA items that adapter should should subscribe for data. Network communication The OPC UA EDS adapter communicates with the OPC UA server through TCP/IP TCP IP network using opc.tcp binary protocol. Stream creation The OPC UA EDS adapter creates types upon receiving the value update for a stream from OPC UA subscription per stream and streams are created for selected OPC UA items in the data selection configuration. One stream is going to be created in Edge Data Store for every selected OPC UA item in data selection configuration. Connection The OPC UA EDS adapter uses binary opc.tcp protocol to communicate with the OPC UA servers. The X.509-type client and server certificates are exchanged and verified (when security is enabled) and the connection to the configured OPC UA server is established. Data collection OPC UA EDS adapter is collecting time-series data from selected OPC UA dynamic variables through OPC UA subscriptions (unsolicited reads). This version of adapter supports Data Access (DA) part of OPC UA specification. Streams by OPC UA EDS adapter OPC UA EDS adapter creates a stream with two properties per selected OPC UA item. The properties are defined in the following table: Property name Data type Description Timestamp DateTime Timestamp of the given OPC UA item value update. Value Based on type of incoming OPC UA value Value of the given the OPC UA item value update. Stream ID is a unique identifier of each stream created by the adapter for a given OPC UA item. In case the Custom Stream ID is specified for the OPC UA item in data selection configuration, the OPC UA EDS adapter is going to use that as a stream ID for the stream. Otherwise, the adapter constructs the stream ID using the following format constructed from OPC UA item node ID: \u003cAdapter Component ID\u003e.\u003cNamespace\u003e.\u003cIdentifier\u003e Note: Naming convention is affected by StreamIdPrefix and ApplyPrefixToStreamID settings in data source configuration. For more informaton, see OPC UA data source configuration ."
                                                 },
    "V1/index.html":  {
                          "href":  "V1/index.html",
                          "title":  "OSIsoft Edge Data Store",
                          "keywords":  "OSIsoft Edge Data Store ======= Overview OMF Quick Start Modbus Quick Start Opc Ua Quick Start OCS Egress Quick Start PI Egress Quick Start SDS Read/Write Read Write Quick Start Visualization Quick Start Analytics Quick Start Command Line Quick Start - Linux Command Line Quick Start - Windows Design Installation Security System Configuration EDS data ingress OPC UA adapter Supported Features Principles of operation Data source configuration Data selection configuration Opc Ua Data Selection Adapter security Modbus TCP adapter Supported Features Principles of operation Data source configuration Data selection configuration Adapter security OMF Storage Sequential Data Store Types Streams Stream Views Indexes Writing Data Write Data API Reading Data Reading Data API Filter Expressions Table Format Units of Measure Request/Response Request Response Compression Searching Stream Metadata and Tags Egress Administration Management Tools Logging Configuration Configuration Schemas Edge Data Store Schema System System Schema System Port Schema EDS Modbus TCP adapter Modbus Data Source Schema Modbus Data Selection Schema Modbus Logging Schema EDS OPC UA adapter OPC UA Data Source Schema OPC UA Data Selection Schema OPC UA Logging Schema Storage Storage Schema Storage Logging Schema Storage Periodic Egress Endpoints Schema Storage Runtime Schema Health Diagnostics Platforms Docker Command Line Troubleshooting Release Notes"
                      },
    "V1/OpcUa/OpcUaOverview.html":  {
                                        "href":  "V1/OpcUa/OpcUaOverview.html",
                                        "title":  "OPC UA EDS adapter",
                                        "keywords":  "OPC UA EDS adapter Overview OPC UA is an open standard, which ensures interoperability, security, and reliability of industrial automation devices and systems. OPC UA is recognized as one of the key communication and data modeling technologies of Industry 4.0, due to the fact that it works with many software platforms, and is completely scalable and flexible. The OPC UA EDS adapter transfers time-series data from OPC UA devices into Edge Data Store. You can add a single OPC UA EDS adapter during installation. If you want multiple OPC UA EDS adapters, see Edge Data Store configuration on how to add a new component to Edge Data Store. As with other EDS adapters, the OPC UA EDS adapter is configured with data source and data selection JSON documents. The data source configurations are identical with other adapters, but OPC UA supports an option to generate a data selection file template that you can manually edit and use for subsequent configuration. See OPC UA data selection for details. Once you create a template file, you can reuse it on both Linux and Windows without changes."
                                    },
    "V1/OpcUa/OPCUADataSourceConfiguration.html":  {
                                                       "href":  "V1/OpcUa/OPCUADataSourceConfiguration.html",
                                                       "title":  "OPC UA data source configuration",
                                                       "keywords":  "OPC UA data source configuration To use the OPC UA EDS adapter, you must configure from which OPC UA data source it will be receiving data. Configure OPC UA data source Note: You cannot modify OPC UA data source configurations manually. You must use the REST endpoints to add or edit the configuration. Complete the following to configure the OPC UA data source: Using any text editor, create a file that contains an OPC UA data source in JSON form. For content structure, see OPC UA data source example . For a table of all available parameters, see Parameters for OPC UA data source . Save the file as DataSource.config.json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cEDS http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSource/ adapterId\u003e DataSource  . Note: During installation, it is possible to add a single OPC UA EDS adapter which is named OpcUa1. The following example uses this component name. Example using cURL (run this command from the same directory where the file is located): curl -v -d \"@DataSource.config.json\" -H \"Content-Type: application/json\" application json\" -X POST \"http://localhost:5590/api/v1/configuration/OpcUa1/DataSource\" \"http:  localhost:5590 api v1 configuration OpcUa1 DataSource\" Parameters for OPC UA data source The following parameters are available for configuring an OPC UA data source. Parameter Required Type Description EndpointURL Required string The endpoint URL of the OPC UA server. The following is an example of the URL format: opc.tcp://OPCServerHost:Port/OpcUa/SimulationServer opc.tcp:  OPCServerHost:Port OpcUa SimulationServer Note: If you change the EndpointURL on a configured OPC UA EDS adapter that has ComponentID_DataSelection.json file exported, you will need to relocate the ComponentID_DataSelection.json file from the configuration directory to trigger a new browse (export). UseSecureConnection Optional bool When set to true, the OPC UA EDS adapter connects to a secure endpoint using OPC UA certificate exchange operation. The default is true. When set to false, the OPC UA EDS adapter connects to an unsecured endpoint of the server and certificate exchange operation is not required. Note: OSIsoft recommends setting this option to false for testing purposes only. UserName Optional string User name for accessing the OPC UA server. Password Optional string Password for accessing the OPC UA server. Note: OSIsoft recommends using REST to configure the data source when the password must be specified. RootNodeIds Optional string List of comma-separated NodeIds of those objects from which the OPC UA EDS adapter browses the OPC UA server address space. This option allows selecting only subsets of the OPC UA address by explicitly listing one or more NodeIds which are used to start the initial browse. For example: ns=5;s=85/0:Simulation, ns=5;s=85 0:Simulation, ns=3;s=DataItems. If not specified, it means that the whole address space will be browsed. IncomingTimestamp Optional string Specifies whether the incoming timestamp is taken from the source, from the OPC UA server, or should be created by the OPC UA EDS adapter instance. Source - Default and recommended setting. The timestamp is taken from the source timestamp field. The source is what provides data for the item to the OPC UA server, such as a field device. Server - In case the OPC UA item has an invalid source timestamp field, the Server timestamp can be used. Connector - The OPC UA EDS adapter generates a timestamp for the item upon receiving it from the OPC UA server. StreamIdPrefix Optional string Specifies what prefix is used for Stream IDs. Naming convention is StreamIdPrefixNodeId. Note: An empty string means no prefix will be added to the Stream IDs. Null value means ComponentID followed by dot character will be added to the stream IDs (example: OpcUa1.NodeId). OPC UA data source example The following is an example of valid OPC UA data source configuration: { \"EndpointUrl\": \"opc.tcp://IP-Address/TestOPCUAServer\", \"opc.tcp:  IP-Address TestOPCUAServer\", \"UseSecureConnection\": true, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": null }"
                                                   },
    "V1/OpcUa/OPCUADataSelectionConfiguration.html":  {
                                                          "href":  "V1/OpcUa/OPCUADataSelectionConfiguration.html",
                                                          "title":  "OPC UA data selection configuration",
                                                          "keywords":  "OPC UA data selection configuration In addition to the data source configuration, you need to provide a data selection configuration to specify the data you want the OPC UA EDS adapter to collect from the data sources. Configure OPC UA data selection Note: You cannot modify OPC UA data selection configurations manually. You must use the REST endpoints to add or edit the configuration. Complete the following to configure OPC UA data selection: Using any text editor, create a file that contains an OPC UA data selection in JSON form. For content structure, see OPC UA data selection example . For a table of all available parameters, see Parameters for OPC UA data selection . Save the file as DataSelection.config.json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cEDS http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSelection/ adapterId\u003e DataSelection  Example using cURL (run this command from the same directory where the file is located): curl -v -d \"@DataSelection.config.json\" -H \"Content-Type: application/json\" application json\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cEDS \"http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSelection\" adapterId\u003e DataSelection\" Parameters for OPC UA data selection Parameter Required Type Description Selected Optional bool This field is used to select or clear a measurement. To select an item, set to true. To remove an item, leave the field empty or set to false. If not configured, the default value is false. Name Optional string The optional friendly name of the data item collected from the data source. If not configured, the default value will be the stream id NodeId Required string The NodeId of the variable. StreamID Required string The custom stream ID that will be used to create the streams. If not specified, the OPC UA EDS adapter will generate a default stream ID based on the measurement configuration. A properly configured custom stream ID follows these rules: Is not case-sensitive. Can contain spaces. Cannot start with two underscores (\"__\"). Can contain a maximum of 260 characters. Cannot use the following characters: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % \u003c \u003e | Cannot start or end with a period. Cannot contain consecutive periods. Cannot consist of only periods. OPC UA data selection example The following is an example of valid OPC UA Data Selection configuration: [ { \"Selected\": true, \"Name\": \"Random1\", \"NodeId\": \"ns=5;s=Random1\", \"StreamId\": \"CustomStreamName\" }, { \"Selected\": false, \"Name\": \"Sawtooth1\", \"NodeId\": \"ns=5;s=Sawtooth1\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Sinusoid1\", \"NodeId\": \"ns=5;s=Sinusoid1\", \"StreamId\": null } ]"
                                                      },
    "V1/OpcUa/OpcUaDataSelection.html":  {
                                             "href":  "V1/OpcUa/OpcUaDataSelection.html",
                                             "title":  "Generate a template OPC UA data selection configuration file",
                                             "keywords":  "Generate a template OPC UA data selection configuration file When a data source is added, the OPC UA EDS adapter browses the entire OPC UA server address space and exports the available OPC UA variables into a .json file for data selection. Data is collected automatically based upon user demands. OPC UA data from OPC UA variables is read through subscriptions (unsolicited reads). A default OPC UA data source template file will be created if there is no OPC UA data selection configuration, but a valid OPC UA data source exists. Complete the following steps in order for this template data selection to be generated: Add an OPC UA EDS adapter with a unique ComponentId. During install of the Edge Data Store, enabling the OPC UA adapter results in addition of a unique component that also satisfies this condition. Configure a valid OPC UA Data Source . Once these steps are taken, a template OPC UA data selection will be generated in the Configuration directory for the corresponding platform, see Linux and Windows platform differences . The following are example locations of the file created - in this example it is assumed that the ComponentId of the OPC UA component is the default OpcUa1: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\Configuration\\OpcUa1_DataSelection.json Linux: /usr/share/OSIsoft/EdgeDataStore/Configuration/OpcUa1_DataSelection.json  usr share OSIsoft EdgeDataStore Configuration OpcUa1_DataSelection.json Copy the file to a different directory to edit it. The contents of the file will look something like: [ { \"Selected\": false, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"StreamId\": null } ] In a text editor, edit the file and change the value of any Selected key from false to true in order to transfer the OPC UA data to be stored in the Edge Data Store. In the same directory where you edited the generated file, run the following curl command: curl -i -d \"@OpcUa1_DataSelection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection To see the streams that have been created in Edge Storage to store the data you are writing, you can run the following curl script: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/ http:  localhost:5590 api v1 tenants default namespaces default streams  To view the data in the streams being written, you can refer to the SDS part of this documentation. To egress the data to OSIsoft Cloud Services or the PI System, see the egress documentation or quick starts."
                                         },
    "V1/OpcUa/OPCUAAdapterSecurityConfiguration.html":  {
                                                            "href":  "V1/OpcUa/OPCUAAdapterSecurityConfiguration.html",
                                                            "title":  "OPC UA EDS adapter security configuration",
                                                            "keywords":  "OPC UA EDS adapter security configuration The OPC UA security standard is concerned with the authentication of client and server applications, the authentication of users and confidentiality of their communication. As the security model relies heavily on Transport Level Security (TLS) to establish a secure communication link with an OPC UA server, each client, including the OSIsoft Adapter, must have a digital certificate deployed and configured. Certificates uniquely identify client applications and machines on servers, and allow for creation of a secure communication link when trusted on both sides. OSIsoft Adapter for OPC UA generates a self-signed certificate when the first secure connection attempt is made. Each OPC UA EDS adapter instance creates a certificate store where its own certificates, as well as those of the server, will be persisted. Configure OPC UA EDS adapter security Configure the data source to use secure connection (Set UseSecureConnection as true). Add server\u0027s certificate to the adapter\u0027s trust store. Add adapter\u0027s certificate to the server\u0027s trust store. Data source configuration { \"EndpointUrl\": \"opc.tcp://IP-Address/TestOPCUAServer\", \"opc.tcp:  IP-Address TestOPCUAServer\", \"UseSecureConnection\": true, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\" } Note: OSIsoft strongly recommends using secure connections in production environment(s). Adapter Certificate store Adapter Certificate store location: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates  usr share OSIsoft EdgeDataStore {ComponentId} Certificates Adapter Trust store location: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates\\Trusted\\certs Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates/Trusted/certs  usr share OSIsoft EdgeDataStore {ComponentId} Certificates Trusted certs Adapter Rejected certificates location: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates\\RejectedCertificates\\certs Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates/RejectedCertificates/certs  usr share OSIsoft EdgeDataStore {ComponentId} Certificates RejectedCertificates certs The adapter verifies whether the server certificate is present in the adapter trust store location and is therefore trusted. In case the certificates were not exchanged upfront, the adapter persists the server certificate within the RejectedCertificates folder and the following warning message about the rejected server certificate will be printed: ~~2019-09-08 11:45:48.093 +01:00~~ [Warning] Rejected Certificate: \"DC=MyServer.MyDomain.int, O=Prosys OPC, CN=Simulation After the certificate is reviewed and approved, it can be manually moved from the ???RejectedCertificates\\certs??? folder to the ???Trusted\\certs\" folder using a file explorer or command-line interpreter. Linux example using command-line: mv /usr/share/OSIsoft/EdgeDataStore/OpcUa1/Certificates/RejectedCertificates/certsSimulationServer\\  usr share OSIsoft EdgeDataStore OpcUa1 Certificates RejectedCertificates certsSimulationServer\\ \\[F9823DCF607063DBCECCF6F8F39FD2584F46AEBB\\].der /usr/share/OSIsoft/EdgeDataStore/OpcUa1/Certificates/Trusted/certs/  usr share OSIsoft EdgeDataStore OpcUa1 Certificates Trusted certs  Note: Administrator or root privileges are required to perform this operation. When the certificate is in the adapter trust store, the adapter trusts the server and the connection attempt proceeds in making the connection call to the configured server. Connection succeeds only when the adapter certificate is trusted on the server side. See your OPC UA server documentation for more details on how to make a client certificate trusted. In general, servers work in a similar fashion to the clients; hence a similar approach for making the server certificate trusted on the client side can be taken on the server. When certificates are mutually trusted, the connection attempt succeeds and the adapter is connected to the most secure endpoint provided by the server."
                                                        },
    "V1/OMF/OMFOverview.html":  {
                                    "href":  "V1/OMF/OMFOverview.html",
                                    "title":  "OSIsoft Message Format (OMF) ingress with Edge Storage",
                                    "keywords":  "OSIsoft Message Format (OMF) ingress with Edge Storage The Edge Storage component supports both OMF version 1.0 and OMF version 1.1 for data ingress. The OMF ingress functionality is the same technology that is used in OSIsoft Cloud Services (OCS). Writing an OMF application to run on EDS is very similar to writing an OMF application to write data to OCS. No authentication is necessary for writing to Edge Data Store, as long as the application is running on the same device as Edge Data Store. Remote access to OMF data ingress is currently not supported. OMF specification OMF specification: http://omf-docs.osisoft.com http:  omf-docs.osisoft.com EDS uses OMF technology developed for use on OSIsoft Cloud Services (OCS). The behavior of OMF in EDS is very similar to OCS. Dynamic messages are supported, but static messages (usually used for creating PI AF assets) are not supported by EDS. Any static OMF messages sent to the EDS OMF REST endpoint will be ignored. OMF endpoint The route to the OMF endpoint provided by the Edge Storage component is the following: Method: POST Endpoint: http://localhost:5590/api/v1/tenants/default/namespaces/default/omf http:  localhost:5590 api v1 tenants default namespaces default omf Supported functionality The OMF endpoint for the Edge Storage component supports both OMF version 1.0 and OMF version 1.1 for data ingress. If you specify a later version of OMF, errors will be returned. The OMF endpoint for the Edge Storage component does not support the update action, but can only create messages. If a create data message is sent with the same time index, the values will be replaced at that index value. For efficiency reasons, OSIsoft recommends batching OMF messages that are sent to the EDS endpoint. Sending single messages or a small number of messsages to the OMF endpoint can be successful, but it is relatively inefficient. When a single message or a small number of messages are sent at a time, the HTTP overhead of creating the request and processing the response on a small device is more expensive than the processing of the OMF message itself. While a large number of OMF messages per second can be processed by EDS platforms, OSIsoft recommends keeping the number of HTTP calls per second fairly low for efficiency reasons."
                                },
    "V1/Modbus/SupportedFeaturesModbus.html":  {
                                                   "href":  "V1/Modbus/SupportedFeaturesModbus.html",
                                                   "title":  "Supported features",
                                                   "keywords":  "Supported features Register types The Modbus TCP EDS adatper supports 6 register types, corresponding to 4 function codes (1-4). Since one function code can return two types of registers, either 16-bit or 32-bit register depending on the device, either the register type, or the register type code, intead of the function code, is required when configuring the data selection for the adapter. The following table lists all the register types supported in the Modbus TCP EDS adatper. Register Type Register Type Code Description Function Code Coil 1 Read Coil Status 1 Discrete 2 Read Discrete Input Status 2 Holding16 3 Read 16-bit Holding Registers 3 Holding32 4 Read 32-bit Holding Registers 3 Input16 6 Read 16-bit Input Registers 4 Input32 7 Read 32-bit Input Registers 4 When reading from function codes 1 and 2 the adapter expects these to be returned as single bits. For function codes 3 and 4 , the adapter expects 16 bits to be returned from devices that contain 16-bit registers and 32 bits to be returned from devices that contain 32-bit registers. Data types The Modbus TCP EDS adapter converts readings from single or multiple registers into the data types specified by the data type code and populates the value into streams created in the Edge Data Store. The following table lists all data types with their corresponding type codes supported by the Modbus TCP EDS adapter. Data type code Data type name Value type Register type Description 1 Boolean Bool Bool 0 = false 1 = true 10 Int16 Int16 Bool/16-bit Bool 16-bit Read 1 Modbus TCP register and interpret as a 16-bit integer. Bytes [BA] read from the device are stored as [AB]. 20 UInt16 UInt16 Bool/16-bit Bool 16-bit Read 1 Modbus TCP register and interpret as an unsigned 16-bit integer. Bytes [BA] read from the device are stored as [AB]. 30 Int32 Int32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit integer. Bytes [DCBA] read from the device are stored as [ABCD]. 31 Int32ByteSwap Int32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit integer. Bytes [BADC] read from the device are stored as [ABCD]. 100 Float32 Float32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit float. Bytes [DCBA] read from the device are stored as [ABCD]. 101 Float32ByteSwap Float32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit float. Bytes [BADC] read from the device are stored as [ABCD]. 110 Float64 Float64 16-bit/32-bit 16-bit 32-bit Read 64 bits from the Modbus TCP device and interpret as a 64-bit float. Bytes [HGFEDCBA] read from the device are stored as [ABCDEFGH]. 111 Float64ByteSwap Float64 16-bit/32-bit 16-bit 32-bit Read 64 bits from the Modbus TCP device and interpret as a 64-bit float. Bytes [BADCFEHG] read from the device are stored as [ABCDEFGH]. 1001 - 1250 String String 16-bit/32-bit 16-bit 32-bit 1001 reads a one-character string, 1002 reads a two-character string, and 1003 reads a three-character string and so on. Bytes [AB] are interpreted as \"AB\". 2001 - 2250 StringByteSwap String 16-bit/32-bit 16-bit 32-bit 2001 reads a one-character string, 2002 reads a two-character string, and 2003 reads a three-character string and so on. Bytes [BA] are interpreted as \"AB\". Apply bitmap The Modbus TCP EDS adapter supports applying bitmaps to the value converted from the readings from the Modbus TCP devices. A bitmap is a series of numbers used to extract and reorder bits from a word register. The format of the bitmap is uuvvwwxxyyzz, where uu, vv, ww, yy, and zz each refer to a single bit. A leading zero is required if the referenced bit is less than 10. The low-order bit is 01 and high-order bit is either 16 or 32. Up to 16 bits can be referenced for a 16-bit word (data types 10 and 20) and up to 32 bits can be referenced for a 32-bit word (data type 30 and 31). For example, the bitmap 0307120802 will map the second bit of the original word to the first bit of the new word, the eighth bit to the second bit, the twelfth bit to the third bit, and so on. The high-order bits of the new word are padded with zeros if they are not specified. Not all data types support applying bitmap. The data types supporting bitmap are: Int16 (Data type code 10) UInt16 (Data type code 20) Int32 (Data type code 30 and 31) Apply data conversion The Modbus TCP EDS adapter supports applying data conversion to the value converted from reading from the Modbus TCP devices. A conversion factor and conversion offset can be specified. The conversion factor is used for scaling up or down the value, and the conversion offset is used for shifting the value. The mathematical equation used in conversion is the following: \u003cAfter Conversion\u003e = \u003cBefore Conversion\u003e /   Factor - Offset Not all data types support applying data conversion. The data types supporting data conversion are: Int16 (Data type code 10) UInt16 (Data type code 20) Int32 (Data type code 30 and 31) Float32 (Data type code 100 and 101) The value with data conversion applied will always be converted to the 32-bit float type to maintain the precision of the conversion factor and conversion offset."
                                               },
    "V1/Modbus/PrinciplesOfOperationModbus.html":  {
                                                       "href":  "V1/Modbus/PrinciplesOfOperationModbus.html",
                                                       "title":  "Principles of operation",
                                                       "keywords":  "Principles of operation The following topics provide an operational overview of the Modbus TCP EDS adapter, focusing on streams creation and error handling. Operational overview Adapter configuration In order to provide the necessary information for the Modbus TCP EDS adapter to be ready for data collection, you need to configure the adapter. For more details, see Modbus TCP data source configuration and Modbus TCP data selection configuration . To configure the adapter, do the following: Data source: Provide the information of the data sources from which the connector pulls data Data selection: Provide the selected measurements for which the adapter collects data from the data source Logging: Set up the logging attributes to manage the adapter logging behavior Network communication The Modbus TCP EDS adapter communicates with the Modbus TCP devices through the TCP/IP TCP IP network by sending request packets that are constructed based on the data selection configurations, and collects the response packets returned by the devices. Stream creation From the parsed data selection configurations, the Modbus TCP EDS adapter creates types, streams and data based on the information provided. For each measurement in the data selection configuration, a stream is created in the Edge Data Store to store time series data. Connection Data collection The Modbus TCP EDS adapter collects data from the Modbus TCP devices at the polling rates that you specify. The rates are set in each of the data selection configurations and can range from 0 milliseconds (as fast as possible) up to 1 day per polling. The adapter automatically optimizes the data collection process by grouping the requests to reduce the I/O I O load imposed to the Modbus TCP networks. Streams by Modbus TCP EDS adapter For each data selection configuration, the Modbus TCP EDS adapter creates a stream with two properties. The properties are defined in the following table: Property name Data type Description Timestamp String The response time of the stream data from the Modbus TCP device. Value Specified by the data selection The value of the stream data from the Modbus TCP device. There is a unique identifier (Stream ID) for each stream created for the selected measurement. If a custom stream ID is specified for the measurement in the data selection configuration, the Modbus TCP EDS adapter will use that stream ID to create the stream. Otherwise, the connector constructs the stream ID using the following format: \u003cAdapter Component ID\u003e.\u003cUnit ID\u003e.\u003cRegister Type\u003e.\u003cRegister Offset\u003e Note: Naming convention is affected by StreamIdPrefix and ApplyPrefixToStreamID settings in data source configuration. For more information, see Modbus TCP data source configuration ."
                                                   },
    "V1/Docker/EdgeDocker.html":  {
                                      "href":  "V1/Docker/EdgeDocker.html",
                                      "title":  "Using Edge Data Store with Docker",
                                      "keywords":  "Using Edge Data Store with Docker Docker is a set of tools that can be used on Linux to manage application deployments. If you want to use Docker, you must be familiar with the underlying technology and have determined it is appropriate for your planned use of the Edge Data Store. The objective of this document is to provide examples of how to successfully create a Docker container with the Edge Data Store if you decide that you want Docker. Docker is not a requirement to use Edge Data Store. Create a Docker container containing the Edge Data Store ARM32 Processor Create the following Dockerfile in the directory where you want to create and/or and or run the container: FROM mcr.microsoft.com/dotnet/core/sdk:2.2 mcr.microsoft.com dotnet core sdk:2.2 ARG source WORKDIR /   ADD ./EdgeDataStore_linux-arm.tar . EdgeDataStore_linux-arm.tar . ENTRYPOINT [\"./EdgeDataStore_linux-arm/OSIsoft.Data.System.Host\"] [\". EdgeDataStore_linux-arm OSIsoft.Data.System.Host\"] Copy the EdgeDataStore_linux-arm.tar file to the same directory as the Dockerfile. Run the following command line (sudo may be necessary): docker build -t EdgeDataStore . AMD64 (x64) Processor Create the following Dockerfile in the directory where you want to create and/or and or run the container: FROM mcr.microsoft.com/dotnet/core/sdk:2.2 mcr.microsoft.com dotnet core sdk:2.2 ARG source WORKDIR /   ADD ./EdgeDataStore_linux-x64.tar . EdgeDataStore_linux-x64.tar . ENTRYPOINT [\"./EdgeDataStore_linux-x64/OSIsoft.Data.System.Host\"] [\". EdgeDataStore_linux-x64 OSIsoft.Data.System.Host\"] Copy the EdgeDataStore_linux-x64.tar file to the same directory as the Dockerfile. Run the following command line (sudo may be necessary): docker build -t EdgeDataStore . Run the Edge Data Store Docker containers REST access from the local machine from Docker To run the container you can use the command line (sudo may be necessary): docker run -d --network host EdgeDataStore Port 5590 will be accessible from the host and REST calls can be made to Edge Data Store from applications on the local host computer. With this configuration, all data stored by the Edge Data Store is stored in the container itself, and when the container is deleted the data stored will also be deleted. Persistent storage on the local file system from Docker To run the container you can use the command line (sudo may be necessary): docker run -d --network host -v /edgeds:/usr/share/OSIsoft/  edgeds: usr share OSIsoft  EdgeDataStore Port 5590 will be accessible from the host and REST calls can be made to Edge Data Store from applications on the local host computer. In addition, in this example, all data that would be written to the container is instead written to the host directory /edgeds.  edgeds. This directory can be anything you want - this example just uses a simple directory on the local machine. Changing Port number from Docker If you want a port other than 5590, see the section regarding Port configuration of Edge Data Store. Changing the configuration of the Edge Data Store running in the container will change the port exposed to the local machine. Limiting local host access to Docker If the --network host option is removed from the docker run command, no REST access is possible from outside the container. This may be of value where you want to host an application in the same container as Edge Data Store, and does not want to have external REST access enabled."
                                  },
    "V1/Diagnostics/Diagnostics.html":  {
                                            "href":  "V1/Diagnostics/Diagnostics.html",
                                            "title":  "Edge Data Store diagnostics",
                                            "keywords":  "Edge Data Store diagnostics Edge Data Store and its components produce diagnostics data which is stored locally in the Storage component, and may be queried locally or egressed to PI Web API endpoints or the OSIsoft Cloud Services or both. Diagnostics data is stored within the \u0027diagnostics\u0027 namespace of Edge Storage. Local access to this data is available through the Sds methods. Egress diagnostics data through PeriodicEgressEndpoints To egress diagnositcs related data, configure a periodic egress endpoint and specify diagnostics as the NamespaceId in the periodic egress endpoint configuration. Diagnostics produced by Edge Data Store Edge Data Store produces the following diagnostics streams: The Diagnostics.System dynamic type includes these values which are logged in a stream with the id System.Diagnostics. This diagnostic stream contains system level information related to the host platform that Edge Data Store is running on. Type Property Description string timestamp Timestamp of event int ProcessIdentifier Process id of the host process string StartTime When the host process started long WorkingSet Amount of physical memory, in bytes, allocated for the host process double TotalProcessorTime (uom=s) Total processor time for the host process expressed in seconds double TotalUserProcessorTime (uom=s) User processor time for the host process expressed in seconds double TotalPrivilegedProcessorTime (uom=s) Privileged processor time for the host process expressed in seconds int ThreadCount Number of threads in the host process int HandleCount Number of handles opened by the host process double ManagedMemorySize (uom=MB) Number of bytes currently thought to be allocated in managed memory double PrivateMemorySize (uom=MB) Amount of paged memory, in bytes, allocated for the host process double PeakPagedMemorySize (uom=MB) Maximum amount of memory in the virtual memory paging file, in bytes, used by the host process. double StorageTotalSize (uom=MB) Total size of the storage medium in use by the Edge Data Store double StorageFreeSpace (uom=MB) Free space available EDS adapter diagnostics Each EDS adapter of the Edge Data Store produces its own diagnostics streams. Stream count The Diagnostics.StreamCountEvent dynamic type includes these values, which are logged in a stream with the id {componentid}.StreamCount. The stream count and type count include only types and streams created for sequential data received from a data source. Type Property Description string timestamp Timestamp of event int StreamCount Number of streams created by the adapter instance int TypeCount Number of types created by the adapter instance IO rate The Diagnostics.Adapter.IORate dynamic type includes these values, which are logged in a stream with the id {componentid}.IORate. IO rate includes only sequential data collected from a data source. Type Property Description string timestamp Timestamp of event double IORate 10-minute rolling average of data rate (streams/second) (streams second) Error rate The Diagnostics.Adapter.ErrorRate dynamic type includes these values, and are logged in a stream with the id {componentid}.ErrorRate. Type Property Description string timestamp Timestamp of event double ErrorRate 10-minute rolling average of error rate (streams/second) (streams second) Edge Storage diagnostics The Storage component of Edge Data Store produces the following diagnostics streams. Storage.default.default.Counts The Storage.default.default.Counts stream includes counts of the types, streams and stream views of the default namespace. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views Storage.default.diagnostics.Counts The Storage.default.default.Counts stream includes counts of the types, streams and stream views of the diagnostics namespace. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views Storage.Total.Counts The Storage.Totals.Counts stream includes counts of the types, streams and stream views of all namespaces of the storage component. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views"
                                        },
    "V1/Design/ScalePerformance.html":  {
                                            "href":  "V1/Design/ScalePerformance.html",
                                            "title":  "Edge Data Store design considerations",
                                            "keywords":  "Edge Data Store design considerations Edge Storage role The Edge Storage component integrated with the Edge Data Store is a new item in the OSIsoft software ecosystem. It is not designed to replace any existing storage technology produced by OSIsoft. The Edge Storage component is intended as a data store that is resilient and reliable but limited in duration and scope, as appropriate for an Edge software component. The storage component is configured by default to roll off data in a FIFO (first in first out) process: as new data comes in and the size of streams exceeds the configured limits, older data is purged. If data exists in the Edge Storage component that needs to be permanently retained, you should egress it to either PI Data Archive (using the PI Web API OMF endpoint) or to OSIsoft Cloud Services, using the OCS OMF ingress endpoint. Edge Storage scale The Edge Storage component provides an appropriate level of storage performance for small devices. For the smallest of these devices, throughput may be limited to tens of events per second. For larger devices with faster processors, memory and storage, this could increase to up to 3,000 events per second. The Edge Storage component\u0027s design is focused on small devices in Edge scenarios: if high throughput or large stream counts are required, OSIsoft Cloud Services or PI Data Archive are more appropriate choices. Sizing of Edge devices For Edge Data Store, there are three supported tiers of performance: Small Devices: 1 Core CPU, 512 MB RAM. 30 events/second, events second, 200 streams total. Medium Devices: 2 Core CPU, 1 GB RAM. 300 events/second, events second, 2000 streams total. Large Devices: 4 Core CPU, 4 GB RAM, SSD storage. 3000 events/second, events second, 3000 streams total. These performance metrics assume solid state storage, which is commonly used in Edge devices."
                                        },
    "V1/DataIngress/EDSDataIngress.html":  {
                                               "href":  "V1/DataIngress/EDSDataIngress.html",
                                               "title":  "Edge Data Store data ingress",
                                               "keywords":  "Edge Data Store data ingress There are a number of ways to ingress or store data in the Edge Data Store: OMF Ingress - use the OSIsoft Message Format to send data from a custom application into EDS. OMF is a simple REST and JSON based data format designed for simplicity of custom application design. EDS Modbus TCP adapter - use standard Modbus TCP equipment and protocols to send data into EDS. EDS OPC UA adapter - use standard OPC UA equipment and protocols to send data into EDS. SDS Ingress - use the OSIsoft Sequential Data Store (SDS) REST to send data from a custom application into EDS. SDS offers the most options for how to send, store, and retrieve data from EDS. Important Note for Linux Operating Systems For data ingress scenarios that include a large number of streams consider increasing the operating systems maximum number of open file descriptors per process. For more information, see (Linux and Windows Platform Differences)[xref:linuxWindows]."
                                           },
    "V1/SDS/SdsStreamExtra.html":  {
                                       "href":  "V1/SDS/SdsStreamExtra.html",
                                       "title":  "Stream metadata and tags",
                                       "keywords":  "Stream metadata and tags SdsStream metadata is represented as a dictionary of string keys and associated string values. It can be used to associate additional information with a stream. SdsStream tags are represented as a list of strings. Tags can be used to categorize or denote special attributes of streams. The SdsStream Metadata API and SdsStream Tags API do not accept the search query parameter in their respective Get calls. However, stream tags and metadata can be used as criteria in search query strings to return SdsStream results with the Stream API. SdsStream Metadata API Get stream metadata Returns the metadata dictionary for the specified stream. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Metadata api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Metadata Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code and a response body. Response body The metadata for the specified SdsStream. Sample response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"a metadata key\":\"a metadata value\", \"another key\":\"another value\" } Security Allowed for administrator and user accounts Get stream metadata value Returns the value for the specified key in the metadata dictionary of the specified stream. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Metadata/{key} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Metadata {key} Parameters string namespaceId default or diagnostics string streamId The stream identifier string key The key specifying the metadata value of interest Response The response includes a status code and a response body. Response body The metadata for the specified SdsStream. Sample response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"a metadata value??? } Security Allowed for administrator and user accounts Update stream metadata Replaces the metadata for the specified stream with the metadata in the request body. Overwrites any existing metadata; does not merge. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Metadata api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Metadata Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code. Security Allowed for administrator accounts Delete stream metadata Deletes the metadata for the specified stream. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Metadata api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Metadata Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code. Security Allowed for administrator accounts SdsStream Tags API Get stream tags Returns the tag list for the specified stream. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Tags api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Tags Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code and a response body. Response body The tags for the specified SdsStream. Sample response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ \"a tag\", \"another tag\" ] Security Allowed for administrator and user accounts Update stream tags Replaces the tag list for the specified stream with the tags listed in the request body. Overwrites any existing tags; does not merge. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Tags api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Tags Parameters string namespaceId default or diagnostics string streamId The stream identifier The request content is the serialized list of tags. Response The response includes a status code. Security Allowed by administrator accounts. Delete stream tags Deletes the tag list for the specified stream. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Tags api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Tags Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code. Security Allowed for administrator accounts."
                                   },
    "V1/SDS/Reading_Data_API.html":  {
                                         "href":  "V1/SDS/Reading_Data_API.html",
                                         "title":  "API calls for reading data",
                                         "keywords":  "API calls for reading data Example type, stream, and data Many of the API methods described below contain example requests and responses in JSON to highlight usage and specific behaviors. The following type, stream, and data are used in the examples. Example type SimpleType is an SdsType with a single index. This type is defined in Python and Javascript: Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class SimpleType(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2, } var SimpleType = function () { this.Time = null; this.State = null; this.Value = null; } Example stream Simple is an SdsStream of type SimpleType . Example data Simple has stored values as follows: 11/23/2017 11 23 2017 12:00:00 PM: Ok 0 11/23/2017 11 23 2017 1:00:00 PM: Ok 10 11/23/2017 11 23 2017 2:00:00 PM: Ok 20 11/23/2017 11 23 2017 3:00:00 PM: Ok 30 11/23/2017 11 23 2017 4:00:00 PM: Ok 40 All times are represented at offset 0, GMT. Get First Value Returns the first value in the stream. If no values exist in the stream, null is returned. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/First api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data First Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code and a response body containing a serialized event. Get Last Value Returns the last value in the stream. If no values exist in the stream, null is returned. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Last api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Last Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code and a response body containing a serialized event. Find Distinct Value Returns a stored event based on the specified index and searchMode . Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?index={index}\u0026searchMode={searchMode} Parameters string namespaceId default or diagnostics string streamId The stream identifier string index The index string searchMode The SdsSearchMode , the default is exact Response The response includes a status code and a response body containing a serialized collection with one event. Depending on the request index and searchMode , it is possible to have an empty collection returned. Example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?index=2017-11-23T13:00:00Z\u0026searchMode=Next The request has an index that matches the index of an existing event, but since a SdsSearchMode of next was specified, the response contains the next event in the stream after the specified index: Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?index=2017-11-23T13:30:00Z\u0026searchMode=Next The request specifies an index that does not match an index of an existing event. The next event in the stream is retrieved. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Get Values Returns a collection of stored values at indexes based on request parameters. SDS supports three ways of specifying which stored events to return: Filtered : A filtered request accepts a filter expression . Range : A range request accepts a start index and a count. Window : A window request accepts a start index and end index. This request has an optional continuation token for large collections of events. Filtered Returns a collection of stored values as determined by a filter . The filter limits results by applying an expression against event fields. Filter expressions are explained in detail in the Filter expressions section. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?filter={filter} Parameters string namespaceId default or diagnostics string streamId The stream identifier string filter The filter expression (see Filter expressions ) Response The response includes a status code and a response body containing a serialized collection of events. Example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?filter=Measurement gt 10 The events in the stream with Measurement greater than 10 are returned. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note: State is not included in the JSON as its value is the default value. Range Returns a collection of stored values as determined by a startIndex and count . Additional optional parameters specify the direction of the range, how to handle events near or at the start index, whether to skip a certain number of events at the start of the range, and how to filter the data. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026count={count}[\u0026skip={skip}\u0026reversed={reversed} \u0026boundaryType={boundaryType}\u0026filter={filter}] Parameters string namespaceId default or diagnostics string streamId The stream identifier string startIndex Index identifying the beginning of the series of events to return int count The number of events to return int skip Optional value specifying the number of events to skip at the beginning of the result bool reversed Optional specification of the direction of the request. By default, range requests move forward from startIndex, collecting events after startIndex from the stream. A reversed request will collect events before startIndex from the stream. SdsBoundaryType boundaryType Optional SdsBoundaryType specifies the handling of events at or near startIndex string filter Optional filter expression Response The response includes a status code and a response body containing a serialized collection of events. Example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100 This request will return a response with up to 100 events starting at 13:00 and extending forward toward the end of the stream: Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note: State is not included in the JSON as its value is the default value. Example To reverse the direction of the request, set reversed to true. The following request will return up to 100 events starting at 13:00 and extending back toward the start of the stream: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\" } ] Note: State is not included in the JSON as its value is the default value. Further, Measurement is not included in the second, 12:00:00, event as zero is the default value for numbers. The following request specifies a boundary type of Outside for a reversed-direction range request. The response will contain up to 100 events. The boundary type Outside indicates that up to one event outside the boundary will be included in the response. For a reverse direction range request, this means one event forward of the specified start index. In a default direction range request, it would mean one event before the specified start index. GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true\u0026boundaryType=2 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 0 } ] The event outside of the index is the next event or the event at 14:00 because the request operates in reverse. Adding a filter to the request means only events that meet the filter criteria are returned: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true\u0026boundaryType=2\u0026filter=Measurement gt 10 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Window Returns a collection of stored events based on the specified startIndex and endIndex . For handling events at and near the boundaries of the window, a single SdsBoundaryType that applies to both the start and end indexes can be passed with the request, or separate boundary types may be passed for the start and end individually. Paging is supported for window requests with a large number of events. To retrieve the next page of values, include the continuationToken from the results of the previous request. For the first request, specify a null or empty string for the continuationToken . Requests GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026endIndex={endIndex} GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026endIndex={endIndex}\u0026boundaryType={boundaryType} GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026startBoundaryType={startBoundaryType} \u0026endIndex={endIndex}\u0026endBoundaryType={endBoundaryType} GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026endIndex={endIndex} \u0026count={count}\u0026continuationToken={continuationToken} GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026startBoundaryType={startBoundaryType} \u0026endIndex={endIndex}\u0026endBoundaryType={endBoundaryType}\u0026filter={filter}\u0026count={count} \u0026continuationToken={continuationToken} Parameters string namespaceId default or diagnostics string streamId The stream identifier string startIndex Index bounding the beginning of the series of events to return string endIndex Index bounding the end of the series of events to return int count Optional maximum number of events to return. If count is specified, a continuationToken must also be specified. SdsBoundaryType boundaryType Optional SdsBoundaryType specifies handling of events at or near the start and end indexes SdsBoundaryType startBoundaryType Optional SdsBoundaryType specifies the first value in the result in relation to the start index. If startBoundaryType is specified, endBoundaryType must be specified. SdsBoundaryType endBoundaryType Optional SdsBoundaryType specifies the last value in the result in relation to the end index. If startBoundaryType is specified, endBoundaryType must be specified. string filter Optional filter expression string continuationToken Optional token used to retrieve the next page of data. If count is specified, a continuationToken must also be specified. Response The response includes a status code and a response body containing a serialized collection of events. A continuation token can be returned if specified in the request. Example The following requests all stored events between 12:30 and 15:30: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z The response will contain the event stored at the specified index: Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 } ] Note: State is not included in the JSON as its value is the default value. Example When the request is modified to specify a boundary type of Outside, the value before 13:30 and the value after 15:30 are included: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026boundaryType=2 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T12:00:00Z\" }, { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note that State is not included in the JSON as its value is the default value. Further, Measurement is not included in the second, 12:00:00, event as zero is the default value for numbers. If instead a start boundary of Inside, only values inside the start boundary (after 13:30) are included in the result. With an end boundary of Outside one value outside the end index (after 15:30) is included: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026\u0026startBoundaryType=1 \u0026endIndex=2017-11-23T15:30:00Z\u0026endBoundaryType=2 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 } ] In order to page the results of the request, a continuation token may be specified. This requests the first page of the first two stored events between start index and end index by indicating count is 2 and continuationToken is an empty string: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026count=2\u0026continuationToken= Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Results\": [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ], \"ContinuationToken\": \"2017-11-23T14:00:00.0000000Z\" } This request uses the continuation token from the previous page to request the next page of stored events: GET api/v1/Tenants/default}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026count=2\u0026continuationToken=2017-11-23T14:00:00Z Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Results\": [ { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 } ], \"ContinuationToken\": null } In this case, the results contain the final event. The returned continuation token is null. Get Interpolated Values Returns a collection of values based on request parameters. The stream\u0027s read characteristics determine how events are calculated for indexes at which no stored event exists. Interpolation is not supported for streams with compound indexes. SDS supports two ways of specifying which interpolated events to return: Index Collection : One or more indexes can be passed to the request in order to retrieve events at specific indexes. Interval : An interval can be specified with a start index, end index, and count. This will return the specified count of events evenly spaced from start index to end index. Index Collection Returns events at the specified indexes. If no stored event exists at a specified index, the stream\u0027s read characteristics determine how the returned event is calculated. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data  Interpolated?index={index}[\u0026index={index}...] Parameters string namespaceId default or diagnostics string streamId The stream identifier string index One or more indexes Response The response includes a status code and a response body containing a serialized collection of events. Depending on the specified indexes and read characteristics of the stream, it is possible to have less events returned than specified indexes. An empty collection can also be returned. Example Consider a stream of type Simple with the default InterpolationMode of Continuous and ExtrapolationMode of All . In the following request, the specified index matches an existing stored event: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default Namespaces {namespaceId} Streams Simple Data  Interpolated?index=2017-11-23T13:00:00Z The response will contain the event stored at the specified index. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 } ] The following request specifies an index for which no stored event exists: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default Namespaces {namespaceId} Streams Simple Data  Interpolated?index=2017-11-23T13:30:00Z Because the index is a valid type for interpolation and the stream has a InterpolationMode of Continuous , this request receives a response with an event interpolated at the specified index: Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:30:00Z\", \"State\": 0, \"Measurement\": 15 } ] Consider a stream of type Simple with an InterpolationMode of Discrete and ExtrapolationMode of All . In the following request, the specified indexes only match two existing stored events: GET api/v1/Tenants/default}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default} Namespaces {namespaceId} Streams Simple Data Interpolated?index=2017-11-23T12:30:00Z\u0026index=2017-11-23T13:00:00Z\u0026index=2017-11-23T14:00:00Z For this request, the response contains events for two of the three specified indexes. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Interval Returns events at evenly spaced intervals based on the specified start index, end index, and count. If no stored event exists at an index interval, the stream\u0027s read characteristics determine how the returned event is calculated. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data  Interpolated?startIndex={startIndex}\u0026endIndex={endIndex}\u0026count={count} Parameters string namespaceId default or diagnostics string streamId The stream identifier string startIndex The index defining the beginning of the window string endIndex The index defining the end of the window int count The number of events to return. Read characteristics of the stream determine how the events are constructed. Response The response includes a status code and a response body containing a serialized collection of events. Depending on the read characteristics and input parameters, it is possible for a collection to be returned with less events than specified in the count. For a stream, named Simple, of type Simple for the following request: GET api/v1/Tenants/default}/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default} Namespaces {namespaceId} Streams Simple Data  Interpolated?startIndex=2017-11-23T13:00:00Z\u0026endIndex=2017-11-23T15:00:00Z\u0026count=3 The start and end fall exactly on event indexes, and the number of events from start to end match the count of three (3). Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 } ] Get Summaries Returns summary intervals between a specified start and end index. Index types that cannot be interpolated do not support summary requests. Strings are an example of indexes that cannot be interpolated. Summaries are not supported for streams with compound indexes. Interpolating between two indexes that consist of multiple properties is not defined and results in non-determinant behavior. Summary values supported by SdsSummaryType enum: Summary Enumeration value Count 1 Minimum 2 Maximum 4 Range 8 Mean 16 StandardDeviation 64 Total 128 Skewness 256 Kurtosis 512 WeightedMean 1024 WeightedStandardDeviation 2048 WeightedPopulationStandardDeviatio 4096 Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data  Summaries?startIndex={startIndex}\u0026endIndex={endIndex}\u0026count={count}\u0026filter={filter} Parameters string namespaceId default or diagnostics string streamId The stream identifier string startIndex The start index for the intervals string endIndex The end index for the intervals int count The number of intervals requested string filter Optional filter expression string streamViewId Optional stream view identifier Response The response includes a status code and a response body containing a serialized collection of SdsIntervals. Each SdsInterval has a start, end, and collection of summary values. Property Details Start The start of the interval End The end of the interval Summaries The summary values for the interval, keyed by summary type. The nested dictionary contains property name keys and summary calculation result values. Example The following request calculates two summary intervals between the startIndex and endIndex : GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default Namespaces {namespaceId} Streams Simple Data  Summaries?startIndex=2017-11-23T12:00:00Z\u0026endIndex=2017-11-23T16:00:00Z\u0026count=2 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Start\": { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 0 }, \"End\": { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, \"Summaries\": { \"Count\": { \"Measurement\": 2 }, \"Minimum\": { \"Measurement\": 0 }, \"Maximum\": { \"Measurement\": 20 }, \"Range\": { \"Measurement\": 20 }, \"Total\": { \"Measurement\": 20 }, \"Mean\": { \"Measurement\": 10 }, \"StandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"PopulationStandardDeviation\": { \"Measurement\": 5 }, \"WeightedMean\": { \"Measurement\": 10 }, \"WeightedStandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"WeightedPopulationStandardDeviation\": { \"Measurement\": 5 }, \"Skewness\": { \"Measurement\": 0 }, \"Kurtosis\": { \"Measurement\": -2 } } }, { \"Start\": { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, \"End\": { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, \"Summaries\": { \"Count\": { \"Measurement\": 2 }, \"Minimum\": { \"Measurement\": 20 }, \"Maximum\": { \"Measurement\": 40 }, \"Range\": { \"Measurement\": 20 }, \"Total\": { \"Measurement\": 60 }, \"Mean\": { \"Measurement\": 30 }, \"StandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"PopulationStandardDeviation\": { \"Measurement\": 5 }, \"WeightedMean\": { \"Measurement\": 30 }, \"WeightedStandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"WeightedPopulationStandardDeviation\": { \"Measurement\": 5 }, \"Skewness\": { \"Measurement\": 0 }, \"Kurtosis\": { \"Measurement\": -2 } } } ] Get Sampled Values Returns data sampled by intervals between a specified start and end index. Sampling is driven by a specified property or properties of the stream\u0027s Sds Type. Property types that cannot be interpolated do not support sampling requests. Strings are an example of a property that cannot be interpolated. For more information see Interpolation. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data  Sampled?startIndex={startIndex}\u0026endIndex={endIndex}\u0026intervals={intervals}\u0026sampleBy={sampleBy} \u0026boundaryType={boundaryType}\u0026startBoundaryType={startBoundaryType} \u0026endBoundaryType={endBoundaryType}\u0026filter={filter}\u0026streamViewId={streamViewId} Parameters string namespaceId default or diagnostics string streamId The stream identifier string startIndex The start index for the intervals string endIndex The end index for the intervals int intervals The number of intervals requested string sampleBy Property or properties to use when sampling SdsBoundaryType boundaryType Optional SdsBoundaryType specifies the handling of events at or near the startIndex and endIndex SdsBoundaryType startBoundaryType Optional SdsBoundaryType specifies the handling of events at or near the startIndex SdsBoundaryType endBoundaryType Optional SdsBoundaryType specifies the handling of events at or near the endIndex string filter Optional filter expression Response The response includes a status code and a response body containing a serialized collection of events. Example The following request returns two sample intervals between the startIndex and endIndex : GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default Namespaces {namespaceId} Streams Simple Data  Sampled?startIndex=2019-01-01T00:00:00Z\u0026endIndex=2019-01-02T00:00:00Z\u0026intervals=2\u0026sampleBy=Measurement Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2019-01-01T00:00:01Z\", \"State\": 1, \"Measurement\": 1 }, { \"Time\": \"2019-01-01T00:11:50Z\", \"State\": 2, \"Measurement\": 0.00006028870675578446 }, { \"Time\": \"2019-01-01T11:55:33Z\", \"Measurement\": 6.277981349066863 }, { \"Time\": \"2019-01-01T12:00:00Z\", \"Measurement\": 3.101013140344655 }, { \"Time\": \"2019-01-01T12:00:01Z\", \"State\": 1, \"Measurement\": 4.101013140344655 }, { \"Time\": \"2019-01-01T12:01:50Z\", \"State\": 2, \"Measurement\": 0.0036776111121028521 }, { \"Time\": \"2019-01-01T23:57:23Z\", \"State\": 2, \"Measurement\": 6.2816589601789659 }, { \"Time\": \"2019-01-02T00:00:00Z\", \"Measurement\": 6.20202628068931 } ] Note: State is not included in the JSON when its value is the default value. Join Values Returns data from multiple streams, which are joined based on the request specifications. The streams must be of the same SdsType. SDS supports the following types of joins: SdsJoinMode Enumeration value Operation Inner 0 Results include the stored events with common indexes across specified streams. Outer 1 Results include the stored events for all indexes across all streams. Interpolated 2 Results include events for each index across all streams for the request index boundaries. Some events may be interpolated. MergeLeft 3 Results include events for each index across all streams selecting events at the indexes based on left to right order of the streams. MergeRight 4 Results include events for each index across all streams selecting events at the indexes based on right to left order of the streams. SDS supports two types of join requests: GET : The stream, joinMode, start index, and end index are specified in the request URI path. POST : Only the SdsJoinMode is specified in the URI. The streams and read specification for each stream are specified in the body of the request. GET Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams={streams}\u0026joinMode={joinMode} \u0026startIndex={startIndex}\u0026endIndex={endIndex} Parameters string namespaceId default or diagnostics string streams Commas separated list of stream identifiers SdsJoinMode joinMode Type of join, that is inner, outer, and so on. string startIndex Index identifying the beginning of the series of events to return string endIndex Index identifying the end of the series of events to return Response The response includes a status code and a response body containing multiple serialized events. See examples for specifics. Examples To join multiple streams, for example Simple1 and Simple2, assume that Simple1 presents the following data: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 } ] And assume that Simple2 presents the following data: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] The following are responses for various Joins request options: Inner join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=inner \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response Measurements from both streams with common indexes. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 } ] ] Outer join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=outer \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response All Measurements from both Streams, with default values at indexes where a Stream does not have a value. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, null ], [ null, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, null ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 } ], [ null, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 } ], [ { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, null ], [ null, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] ] Interpolated join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=interpolated \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response All measurements from both Streams with missing values interpolated. If the missing values are between valid measurements within a stream, they are interpolated. If the missing values are outside of the boundary values, they are extrapolated. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 15 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 55 } ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 } ], [ { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 35 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 } ], [ { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 75 } ], [ { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] ] MergeLeft join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=mergeleft \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response This is similar to OuterJoin , but the value at each index is the first available value at that index when iterating the given list of streams from left to right. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] MergeRight join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=mergeright \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response This is similar to OuterJoin , but the value at each index is the first available value at that index when iterating the given list of streams from right to left. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] POST request POST api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?joinMode={joinMode} Parameters string namespaceId default or diagnostics SdsJoinMode joinMode Type of join, that is inner, outer, and so on. Request body Read options specific to each stream. Response The response includes a status code and a response body containing multiple serialized events. Consider the following outer join request, POST api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?joinMode=outer where in the request body, different start indexes and end indexes are specified per stream, [ { \"StreamId\": \"Simple1\", \"Options\": { \"StartIndex\": \"2017-11-23T11:00:00Z\", \"EndIndex\": \"2017-11-23T14:00:00Z\", \"StartBoundaryType\": \"Exact\", \"EndBoundaryType\": \"Exact\", \"Count\": 100, \"Filter\": \"\" } }, { \"StreamId\": \"Simple2\", \"Options\": { \"StartIndex\": \"2017-11-23T15:00:00Z\", \"EndIndex\": \"2017-11-23T17:00:00Z\", \"StartBoundaryType\": \"Exact\", \"EndBoundaryType\": \"Exact\", \"Count\": 100, \"Filter\": \"\" } } ] Only events within the stream\u0027s specified index boundaries are considered for the outer join operation Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, null ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, null ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, null ], [ null, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 } ], [ null, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] ] Note: Not all the values from streams were included since they are restricted by individual queries for each Stream."
                                     },
    "V1/SDS/Reading_Data.html":  {
                                     "href":  "V1/SDS/Reading_Data.html",
                                     "title":  "Reading data",
                                     "keywords":  "Reading data The REST APIs provide programmatic access to read and write data. This section identifies and describes the APIs used to read streams data. Results are influenced by types , stream views , filter expressions , and table format . Single stream reads The following methods for reading a single value are available: Get First Value returns the first value in the stream. Get Last Value returns the last value in the stream. Find Distinct Value returns a value based on a starting index and search criteria. In addition, the following methods support reading multiple values: Get Values retrieves a collection of stored values based on the request parameters. Get Interpolated Values retrieves a collection of stored or calculated values based on the request parameters. Get Summaries retrieves a collection of evenly spaced summary intervals based on a count and specified start and end indexes. Get Sampled Values retrieves a collection of sampled data based on the request parameters. All single stream reads are HTTP GET actions. Reading data involves getting events from streams. The base reading URI from a single stream is as follows: api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Parameters string namespaceId default or diagnostics string streamId The stream identifier Bulk reads SDS supports reading from multiple streams in one request. The following method for reading data from multiple streams is available: Join Values retrieves a collection of events across multiple streams and joins the results based on the request parameters. Multi-stream reads can be HTTP GET or POST actions. The base reading URI for reading from multiple streams is the following: api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Parameters string namespaceId default or diagnostics Response format Supported response formats include JSON, verbose JSON, and SDS. JSON is the default response format for SDS, which is used in all examples in this documentation. Default JSON responses do not include any values that are equal to the default value for their type. Verbose JSON responses include all values, including defaults, in the returned JSON payload. To specify verbose JSON return, add the header Accept-Verbosity with a value of verbose to the request. SDS format is specified by setting the Accept header in the request to application/sds application sds . Indexes and reading data Most read operations take at least one index as a parameter. Indexes may be specified as strings. You can find additional details about working with indexes on the Indexes page. Read characteristics When you request data at an index for which no stored event exists, the read characterisitics determine whether the result is an error, no event, interpolated event, or extrapolated event. The combination of the type of the index and the interpolation and extrapolation modes of the SdsType and the SdsStream determine the read characteristics. For more information on read characteristics, see Types and Streams . Filter expressions You can apply filter expressions to any read that returns multiple values, including Get Values, Get Range Values, Get Window Values, and Get Intervals. The filter expression is applied to the collection events conditionally filtering events that do not meet the filter conditions. For details on filter expressions, see the Filter expressions section. Table format You can organize results of a query into tables by directing the form parameter to return a table. Two forms of table are available: table and header table. When you specify the form parameter as table, ?form=table , events are returned in row column form. Results include a collection named Columns that lists column name and type and a collection named Rows containing a collection of rows matching the order of the columns. When you specify a form of type table-headers , ?form=tableh , it results in a collection where the Rows collection contains a column header list. For details on table formats, see the Table format section. SdsBoundaryType The SdsBoundaryType enum defines how data on the boundary of queries is handled: around the start index for range value queries, and around the start and end index for window values. The following are valid SdsBoundaryType values: Boundary Enumeration value Operation Exact 0 Results include the event at the specified index boundary if a stored event exists at that index. Inside 1 Results include only events within the index boundaries Outside 2 Results include up to one event that falls immediately outside of the specified index boundary. ExactOrCalculated 3 Results include the event at the specified index boundary. If no stored event exists at that index, one is calculated based on the index type and interpolation and extrapolation settings. SdsSearchMode The SdsSearchMode enum defines search behavior when seeking a stored event near a specified index. The following are valid values for SdsSearchMode : Mode Enumeration value Operation Exact 0 If a stored event exists at the specified index, that event is returned. Otherwise, no event is returned. ExactOrNext 1 If a stored event exists at the specified index, that event is returned. Otherwise, the next event in the stream is returned. Next 2 Returns the stored event after the specified index. ExactOrPrevious 3 If a stored event exists at the specified index, that event is returned. Otherwise, the previous event in the stream is returned. Previous 4 Returns the stored event before the specified index. Transforming data SDS provides the ability to transform data upon reads. The supported data transformations are: Reading with SdsStreamViews : Changing the shape of the returned data Unit of Measure Conversions : Converting the unit of measure of the data Data transformations are supported for all single stream reads, but transformations have specific endpoints. The following are the base URIs for the tranformation endpoints: api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/First api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform First api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Last api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Last api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Interpolated api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Interpolated api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Summaries api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Summaries api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Sampled api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Sampled Reading with SdsStreamViews When you transform data with an SdsStreamView, the data read is converted to the target type specified in the SdsStreamView. For details on working with stream views, see the Stream Views section. All stream view transformations are GET HTTP requests. You specify the stream view by appending the stream view identifier to requests to the transformation endpoint. For example, the following request would return the first event in the stream as the target type in the stream view specified by the streamViewId : GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/First?streamViewId={streamViewId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform First?streamViewId={streamViewId} All single stream data reads support stream view transformations. When you request data with an SdsStreamView, the read characteristics defined by the target type of the SdsStreamView determine what is returned. The read characteristics are discussed in the code samples. Unit conversion of data SDS supports assigning Units of Measure (UOM) to stream data. If stream data has UOM information associated, SDS supports reading data with unit conversions applied. On each read data request, unit conversions are specified by a user defined collection of SdsStreamPropertyOverride objects in read requests. The SdsStreamPropertyOverride object has the following structure: Property Type Optionality Description SdsTypePropertyId String Required Identifier for an SdsTypeProperty with a UOM assigned Uom String Required Target unit of measure InterpolationMode SdsInterpolationMode N/A N A Currently not supported in context of data reads This is supported in the REST API through HTTP POST calls with a request body containing a collection of SdsStreamPropertyOverride objects. All unit conversions are POST HTTP requests. The unit conversion transformation URI is as follows: POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Request body The Request Body contains a collection of SdsStreamPropertyOverride objects. The example request body below requests SDS to convert the Measurement property of the returned data from meter to centimeter. [ { \"SdsTypePropertyId\" : \"Measurement\", \"Uom\" : \"centimeter\" } ] All single stream data reads with streams that have specified UOMs support UOM conversions."
                                 },
    "V1/SDS/indexes.html":  {
                                "href":  "V1/SDS/indexes.html",
                                "title":  "Indexes",
                                "keywords":  "Indexes Indexes speed up and order the results of searches. A key uniquely identifies a record within a collection of records. Keys are unique within the collection. In Sds, the key of an SdsType is also an index. The key is often referred to as the primary index, while all other indexes are referred to as secondary indexes or secondaries . An SdsType that is used to define an SdsStream must specify a key. When inserting data into an SdsStream, every key value must be unique. Sds will not store more than a single event for a given key. An event with a particular key may be deleted or updated, but two events with the same key cannot exist. Secondary indexes are defined on SdsStreams and are applied to a single property. You can define many secondary indexes. Secondary index values do not need to be unique. The following table contains supported index types: Type SdsTypeCode Boolean 3 Byte 6 Char 4 DateTime 16 DateTimeOffset 20 Decimal 15 Double 14 Guid 19 Int16 7 Int32 9 Int64 11 SByte 5 Single 13 String 18 TimeSpan 21 UInt16 8 UInt32 10 UInt64 12 Working with indexes The following discusses the types defined in the Python and Java Script samples. Samples in other languages can be found here . To build an SdsType representation of the following sample class, see Sample : Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2 } var Simple = function () { this.Time = null; this.State = null; this.Value = null; } Sample The following code is used to build an SdsType representation of the sample class above: Python # Create the properties # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime # State is not a pre-defined type. An SdsType must be defined to represent the enum stateTypePropertyOk = SdsTypeProperty() stateTypePropertyOk.Id = \"Ok\" stateTypePropertyOk.Measurement = State.Ok stateTypePropertyWarning = SdsTypeProperty() stateTypePropertyWarning.Id = \"Warning\" stateTypePropertyWarning.Measurement = State.Warning stateTypePropertyAlarm = SdsTypeProperty() stateTypePropertyAlarm.Id = \"Alarm\" stateTypePropertyAlarm.Measurement = State.Alarm stateType = SdsType() stateType.Id = \"State\" stateType.Name = \"State\" stateType.Properties = [ stateTypePropertyOk, stateTypePropertyWarning,\\ stateTypePropertyAlarm ] state = SdsTypeProperty() state.Id = \"State\" state.Name = \"State\" state.SdsType = stateType # Measurement property is a simple non-indexed, pre-defined type measurement = SdsTypeProperty() measurement.Id = \"Measurement\" measurement.Name = \"Measurement\" measurement.SdsType = SdsType() measurement.SdsType.Id = \"Double\" measurement.SdsType.Name = \"Double\" # Create the Simple SdsType simple = SdsType() simple.Id = str(uuid.uuid4()) simple.Name = \"Simple\" simple.Description = \"Basic sample type\" simple.SdsTypeCode = SdsTypeCode.Object simple.Properties = [ time, state, measurement ] JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    State is not a pre-defined type. A SdsType must be defined to represent the enum var stateTypePropertyOk = new SdsObjects.SdsTypeProperty({ \"Id\": \"Ok\", \"Value\": State.Ok }); var stateTypePropertyWarning = new SdsObjects.SdsTypeProperty({ \"Id\": \"Warning\", \"Value\": State.Warning }); var stateTypePropertyAlarm = new SdsObjects.SdsTypeProperty({ \"Id\": \"Alarm\", \"Value\": State.Alarm }); var stateType = new SdsObjects.SdsType({ \"Id\": \"State\", \"Name\": \"State\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Int32Enum, \"Properties\": [stateTypePropertyOk, stateTypePropertyWarning, stateTypePropertyAlarm, stateTypePropertyRed] }); //    Value property is a simple non-indexed, pre-defined type var valueProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Value\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"doubleType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Double }) }); //    Create the Simple SdsType var simpleType = new SdsObjects.SdsType({ \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": \"This is a simple Sds type\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [timeProperty, stateProperty, valueProperty] }); The Time property is identified as the Key by defining its SdsTypeProperty as follows: Python # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); Note: The time.IsKey field is set to true. To read data using the key, you define a start index and an end index. For DateTime, use ISO 8601 representation of dates and times. To query for a window of values between January 1, 2010 and February 1, 2010, you would define indexes as ???2010-01-01T08:00:00.000Z??? and ???2010-02-01T08:00:00.000Z???, respectively. For additional information, see Reading data ."
                            },
    "V1/SDS/Filter_Expressions.html":  {
                                           "href":  "V1/SDS/Filter_Expressions.html",
                                           "title":  "Filter expressions: values",
                                           "keywords":  "Filter expressions: values You can apply filter expressions to certain read operations that return Sequential Data Store values, including: Get Values and Get Summaries . SdsTypeCodes The following types are supported for use within a filter expression: Boolean Byte Char DateTime DateTimeOffset Decimal Double Guid Int16 Int32 Int64 Sbyte Single String Timespan UInt16 UInt32 Uint64 The following types are not supported for use within a filter expression: Array IEnumerable IDictionary IList SdsType SdsTypeProperty Nullable Types Logical operators The following logical operators are supported for use within a filter expression: Operator Description eq Equal to ne Not equal ge Greater than or equal to le Less than or equal to lt Less than gt Greater than ( ) Parenthesis can be used to affect the order of the operation or Or logical operator and And logical operator not Not logical operator - Negation Logical operator examples For the following examples, you can assume that the SDS Type event includes a field named Value of type double : Value eq 1.0 Value ne 15.6 Value ge 5.0 Value le 8.0 Value gt 5.0 Value lt 4.0 Value gt 2.0 and Value lt 9.0 Value gt 6.0 or Value lt 2.0 not (Value eq 1.0) Math functions The following math functions are supported for use within a filter expression: Function Description add Addition sub Subtraction mul Multiplication div Division mod Modulo round Rounds to the nearest numeric component without a decimal, with the midpoint rounded away from 0. For example, 0.5 rounds to 1; -0.5 rounds to -1) floor Rounds down to the nearest numeric component without a decimal ceiling Rounds up to the nearest numeric component without a decimal Math function examples For the following examples, you can assume that the SDS Type event includes a field named Value of type double : Value eq (6.0 add 3.0) Value eq (6.0 sub 3.0) Value eq (6.0 mul 3.0) Value eq (6.0 div 3.0) Value eq (7.0 mod 3.0) round(Value) eq 16 floor(Value) eq 15 ceiling(Value) eq 16 String functions String operations are case sensitive. The character index in a string is 0-based. The following string functions are supported for use within a filter expression: Function Description endswith Compare the character at the end of the input string startswith Compare the character at the start of the input string length Examines the string length indexof Examines the character starting at a given index substring Examine characters within another string at a specific location contains Search for characters anywhere in another string tolower Convert characters to lowercase toupper Convert characters to uppercase trim Remove whitespace from front and end of a string concat Concatenate strings together replace Replace one set of characters with another String function examples For the following examples, you can assume that the SDS Type event includes a field named sValue of type string : Example Result endswith(sValue, \u0027XYZ\u0027) True if sValue ends with the characters ???XYZ??? startswith(sValue, \u0027Val\u0027 True if sValue starts with the characters ???Val??? length(sValue) eq 11 True if sValue is 11 characters indexof(sValue, \u0027ab\u0027) eq 4 True if the 5th and 6th characters are ???ab??? contains(sValue, \u0027ab\u0027) True if characters ???ab??? are found anywhere in sValue substring(sValue, 10) eq \u0027a b\u0027 True if ???a b??? is found in sValue at index 10 tolower(sValue) eq \u0027val5\u0027 Change sValue to lowercase and compare to ???val5??? toupper(sValue) eq \u0027ABC\u0027 Change sValue to uppercase and compare to ???ABC??? trim(sValue) eq \u0027vall22\u0027 Trim whitespace from front and end of sValue and compare to ???val22??? concat(sValue,\u0027xyz\u0027) eq \u0027dataValue_7xyz\u0027 Add characters to sValue and compare to ???dataValue_7xyz??? replace(sValue,\u0027L\u0027,\u0027D\u0027) eq \u0027Dog1\u0027 Replace any ???L??? in sValue with ???D??? and compare to ???Dog1??? DateTime functions The following DateTime functions are supported for use within a filter expression: Function Description year Get year value from DateTime month Get month value from DateTime day Get day value from DateTime hour Get hour value from DateTime minute Get minute value from DateTime second Get second value from DateTime DateTime function examples For the following examples, you can assume that the SDS Type event includes a field named TimeId of type DateTime : year(TimeId) eq 2015 month(TimeId) eq 11 day(TimeId) eq 3 hour(TimeId) eq 1 minute(TimeId) eq 5 second(TimeId) eq 3 TimeSpan functions The following TimeSpan functions are supported for use within a filter expression: Function Description years Get year value from TimeSpan days Get day value from TimeSpan hours Get hour value from TimeSpan minutes Get minute value from TimeSpan seconds Get second value from TimeSpan TimeSpan function examples For the following examples, you can assume that the SDS Type event includes a field named TimeSpanValue of type TimeSpan : years(TimeSpanValue) eq 1 days(TimeSpanValue) eq 22 hours(TimeSpanValue) eq 1 minutes(TimeSpanValue) eq 1 seconds(TimeSpanValue) eq 2"
                                       },
    "V1/SDS/Compression.html":  {
                                    "href":  "V1/SDS/Compression.html",
                                    "title":  "Compression",
                                    "keywords":  "Compression To more efficiently utilize network bandwidth, the EDS Sequential Data Store supports compression for reading data and writing data through the REST API. Supported compression schemes gzip deflate Request compression (writing data) You can compress the body content of an HTTP request by using the supported compression schemes allowing you to send stream values to the REST API more efficiently. You must use the Content-Encoding HTTP header to specify the compression scheme of compressed-content requests. This header provides context to the API to properly decode the request content. Response compression (reading data) You can request compressed responses from the REST API by specifying one of the supported compression schemes using the Accept-Encoding HTTP header. Compressed responses from the REST API will include a Content-Encoding HTTP header indicating the compression scheme used to compress the response content. Note: Specifying a compression scheme through the use of the* Accept-Encoding HTTP header does not guarantee a compressed response. Always refer to presence and value of the* Content-Encoding HTTP header of the response to properly decode the response content.*"
                                },
    "V1/Installation/InstallationOverview.html":  {
                                                      "href":  "V1/Installation/InstallationOverview.html",
                                                      "title":  "Installation of Edge Data Store",
                                                      "keywords":  "Installation of Edge Data Store Overview Edge Data Store is supported on a variety of platforms and processors. OSIsoft provides ready to use install kits for the following platforms: Windows 10 x64 - EdgeDataStore.msi (Intel/AMD (Intel AMD 64 bit processors) Debian 9 or later x64/AMD64 x64 AMD64 - EdgeDataStore_linux-x64.deb (Intel/AMD (Intel AMD 64 bit processors) Debian 9 or later ARM32 - EdgeDataStore_linux-arm.deb (Raspberry PI 2,3,4, BeagleBone devices, other ARM v7 and ARM v8 32 bit processors) Additionally, OSIsoft also provides examples of how to create Docker containers . For customers who want to build their own custom installers or containers for Linux, tar.gz files are provided with binaries. Install Edge Data Store on a device using an install kit To use any of the installers, copy the appropriate file to the file system of the device. With the installers, you can configure the port assignment at install time. The default port is 5590. You can specify any numeric value in the range of 1024 to 65535. Any other characters or values will be considered invalid. You should select a port not already in use by another program on the host because the installer will not check for this case. Note You can change the port assignment after installation. For more information, see configuration . Windows (Windows 10 x64) You must have administrative privileges to run the installer. To start the installer UI, double-click the EdgeDataStore.msi file in Windows Explorer. In the OSIsoft Edge Data Store Setup window, click Next . Optional: Change the install folder and port number (default 5590) and select the Modbus or OpcUa component or both. Click Next \u003e Install . When the install finishes, Edge Data Store will be installed and running on the port specified. Click Finish . Note The UI based installer will prompt for a port value, and will not proceed if an invalid port is specified. The installer can be started from the command line with the following command: msiexec /i  i EdgeDataStore.msi PORT=5590 INSTALLFOLDER=\"C:\\otherdir\" The PORT (shown previously as the default value; must be in all caps) is optional, and can be changed to a valid value that you want. If you omit PORT=nnnn, the default will be used. The UI will start with the port pre-set to the value specified; validity will be checked as mentioned previously, with the install proceeding only when a valid port number is provided. If, however, the \"quiet\" or \"no ui\" flag for msiexec is specified, and the PORT value on the command line is not valid, the install will proceed with the default 5590 value. The INSTALLFOLDER (must be all caps) is also optional. You can specify an alternate location for Edge Data Store\u0027s binary components. The default value is \"C:\\Program Files\\OSISoft\\EdgeDataStore\". OSIsoft recommends you use the default value. Windows uninstallation To remove the EdgeDataStore program files from a computer, use the Windows Control Panel uninstall application process. The configuration, data, and log files will not be removed by the uninstallation process. To remove data, configuration and log files, remove the directory C:\\ProgramData\\OSIsoft\\EdgeDataStore. This will result in deletion of all data stored in the Edge Storage component in addition to configuration and log files. Debian 9 or later Linux (Ubuntu Raspberry PI, BeagleBone, other Debian based Linux distros) You must have administrative privileges to install the software, for example root or sudo privilege. The following examples assume a user with permission to use sudo. Open a terminal window and type: sudo apt install ./EdgeDataStore_linux-\u003ceither . EdgeDataStore_linux-\u003ceither x64 or arm depending upon processor\u003e.deb A validation check for prerequisites will be completed. If the Linux OS is up to date, the install will succeed. If the install fails, run the following commands from the terminal window and try the install again: sudo apt update sudo apt upgrade After the check for prerequisites succeeds, you will be prompted if you want to change the default port (5590). If you want to change the port type to another value in the acceptable range, type the port value you want and press Enter. If 5590 is acceptable, press Enter. You will then be prompted if you want to install a Modbus or OPC UA EDS adapter in addition to the default Storage component. The default is not to install them, so you can press enter to proceed if you want to install neither one. You can add them after the installation is complete if you want. Note If you specify an invalid value for the port, the install will proceed with the default value of 5590. The install will complete and Edge Data Store will be running on your device. Linux uninstallation To remove Edge Data Store software from a Linux computer, open a terminal window and run the command: sudo apt remove osisoft.EdgeDataStore Running this command will not delete the data, configuration, or log files. To remove data, configuration, and log files, remove the directory /usr/share/OSIsoft/EdgeDataStore/.  usr share OSIsoft EdgeDataStore . This will result in deletion of all data stored in the Edge Storage component, in addition to configuration and log files. You can do this with the following command: sudo rm -r /usr/share/OSIsoft/EdgeDataStore/  usr share OSIsoft EdgeDataStore  After installation (Windows and Linux) You can verify that Edge Data Store is correctly installed by running the following script from the terminal window. Depending upon the processor, memory, and storage, it may take the system a few seconds to start up: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If the installation was successful, a JSON copy of the default system configuration will be returned: { \"Storage\": { \"PeriodicEgressEndpoints\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableTransactionLog\": true }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ] } } If you receive an error, wait a few seconds and try it again. On a device with limited processor, memory, and slow storage, it may take some time before the Edge Data Store is fully initialized and running for the first time."
                                                  },
    "V1/Health/Health.html":  {
                                  "href":  "V1/Health/Health.html",
                                  "title":  "Edge Data Store health",
                                  "keywords":  "Edge Data Store health Insight into the health of the Edge Data Store and the components that make it up can be critical for ensuring that your needs for data collection are being met. To that end, Edge Data Store and its components produce health information. When configured, Edge Data Store transfers health information to OSIsoft OMF endpoints, including the types and containers that represent available health information. Configure Edge Data Store health endpoints Edge Data Store has the ability to report system health to one or more OMF endpoints capable of receiving health messages. To enable this functionality, you must configure one or more health endpoints. Table 1. Configuration parameters for Edge Data Store health endpoints Parameter Required Description Id Optional The ID can be any alphanumeric string, for example Endpoint1. If you do not specify an ID, Edge Data Store generates one automatically. Endpoint Required The URL of the ingress endpoint which accepts OMF health messages. UserName Required for PI Web API endpoints The user name used for authentication to PI Web API OMF endpoint. Password Required for PI Web API endpoints The password used for authentication to PI Web API OMF endpoint. ClientId Required for OSIsoft Cloud Services. The Client Id used for authentication to OSIsoft Cloud Services. ClientSecret Required for OSIsoft Cloud Services. The Client Secret used for authentication to OSIsoft Cloud Services. Buffering Optional Options are memory, disk, or none. The default is none. MaxBufferSizeMB Optional The limit on the maximum megabytes of data to buffer for messages to this endpoint if an integer is \u003e0. This parameter is useful if you want to limit memory or disk usage growth in the event of disconnection to the endpoint. If the buffer is full, old messages will be discarded for new messages. The default is 0. ValidateEndpointCertificate Optional Edge EDS adapter validates the endpoint certificate if set to true (recommended). If set to false, Edge EDS adapter accepts any endpoint certificate. OSIsoft recommends you to use disabled endpoint certificate validation for testing purposes only. EDS adapter health The following health types and streams are created to reflect the health of EDS adapters. The Connectors static type includes these properties and servers as a root AF element with the id Connectors. Type Property Description string Id Connectors - root AF element string Description Collection of Connector assets EDS adapter health The Connector Health static type includes the following properties, which are logged in a stream with the id {machinename}.{componentid}. The stream is linked to root AF element (Connectors). Type Property Description string Id {machinename}.{componentId} string Description {productname} health string Connector Type {adaptertype} string Version {adapterversion} Device status The DeviceStatus dynamic type includes the following values, which are logged in a stream with the id Connectors.{machinename}.{componentid}.DeviceStatus. The stream is linked to {machinename}.{componentid} static stream. Type Property Description string Time Timestamp of event string DeviceStatus Device status value Next health message expected The NextHealthMessageExpected dynamic type includes the following values, which are logged in a stream with the id Connectors.{machinename}.{componentid}.NextHealthMessageExpected. The stream is linked to {machinename}.{componentid} static stream. Heard beat message is expected once a minute. Type Property Description string Time Timestamp of event string NextHealthMessageExpected Time when next health message is expected. Storage component health The following health types and streams are created to reflect the health of the Storage component. The Storage static type includes the following properties and servers as a root AF element with the id Storage. Type Property Description string Id Storage - root AF element string Description Storage Health Storage health The Storage Health static type includes the following properties, which are logged in a stream with the id {machinename}.Storage. The stream is linked to root AF element (Storage). Type Property Description string Id {machinename}.Storage string Description {productname} health string Connector Type {adaptertype} string Version {storageversion} Storage device status The DeviceStatus dynamic type includes the following values, which are logged in a stream with the id Storage.{machinename}.DeviceStatus. The stream is linked to {machinename}.Storage static stream. Type Property Description string Time Timestamp of event string DeviceStatus Device status value Storage next health message expected The NextHealthMessageExpected dynamic type includes the following values, which are logged in a stream with the id Storage.{machinename}.NextHealthMessageExpected. The stream is linked to {machinename}.Storage static stream. Heard beat message is expected once a minute. Type Property Description string Time Timestamp of event string NextHealthMessageExpected Time when next health message is expected."
                              },
    "V1/Egress/Egress.html":  {
                                  "href":  "V1/Egress/Egress.html",
                                  "title":  "Egress from Edge Data Store",
                                  "keywords":  "Egress from Edge Data Store Edge Data Store provides an egress mechanism to copy and transfer data to another device or destination. Data is transferred through OMF. Supported destinations are OSIsoft Cloud Services or a PI Server. Configuration of egress includes specifying zero or more endpoints. An egress endpoint represents a destination to which data will be sent. Each egress endpoint is comprised of the properties specified in the Parameters section. It is executed independently of all other egress endpoints, and is expected to accept OMF messages. More than one endpoint for the same destination is allowed. Note: Some types, and consequently containers and data, cannot be egressed. For more information, see Egress Execution Details . One tenant and two namespaces are supported in the Edge Data Store. The tenant is default, and the two namespaces are default (where adapter and OMF data is written) and diagnostics. Diagnostics is where the system and its components write information that can be used locally or egressed to a remote PI server or OCS for monitoring. To egress both namespaces two egress definitions are required. Configuration Prior to configuring egress on the Edge Data Store, follow Destination Preparation steps to make available one or more OMF destinations. Procedure Note: You cannot add egress configurations manually, because some parameters are stored to disk encrypted. You must use the REST endpoints to add/edit add edit egress configuration. For additional endpoints, see REST Urls . Complete the following to create new egress endpoints: Using any text editor, create a file that contains one or more egress endpoints in JSON form For content structure, see the following Examples . For a table of all available egress parameters, see Parameters . Save the file. Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/storage/periodicegressendpoints/ http:  localhost:5590 api v1 configuration storage periodicegressendpoints  Example using cURL: curl -v -d \"@Storage_PeriodicEgressEndspoints.config.json\" -H \"Content-Type: application/json\" application json\" -X POST \"http://localhost:5590/api/v1/configuration/storage/periodicegressendpoints\" \"http:  localhost:5590 api v1 configuration storage periodicegressendpoints\" Parameters Parameter Required Type Description Enabled Optional bool An indicator of whether egress is enabled when the egress endpoint is loaded. Defaults to true. Backfill Optional bool An indicator of whether data should be backfilled. Backfilling will occur when the egress endpoint is run for the first time after application startup. Enabling backfilling will result in all data from the earliest index to the latest stored index being egressed, after applying the egress filter. Defaults to false. ValidateEndpointCertificate Optional bool Used to disable verification of destination certificate. Use for testing only with self-signed certificates. Defaults to true. EgressFilter Optional string A filter used to determine which streams and types are egressed. For more information on valid filters, see Searching . StreamPrefix Optional string Prefix applied to any streams that are egressed. A null string or a string containing only empty spaces will be ignored. The following restricted characters will not be allowed: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % TypePrefix Optional string Prefix applied to any types that are egressed. A null string or a string containing only empty spaces will be ignored. The following restricted characters will not be allowed: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % DebugExpiration Optional string A property that enables persistence of detailed information, for each outbound HTTP request pertaining to this egress endpoint, to disk. The value of this property represents the date and time this detailed information should stop being persisted. For more information, see Troubleshooting . ======= | EgressFilter | Optional | string | A filter used to determine which streams and types are egressed. See Searching for more information on valid filters. | | Enabled | Optional | bool | An indicator of whether egress is enabled when the egress endpoint is loaded. Defaults to true. | | Endpoint | Required | string | Destination that accepts OMF v1.1 messages. Supported destinations include OCS and PI. | | ExecutionPeriod | Required | string | Frequency of time between each egress action. Must be a string in the following format d.hh:mm:ss.## | | Id | Optional | string | Unique identifier | | Name | Optional | string | Friendly name | Examples The following are valid egress configuration examples. Egress data to OCS. All streams, every 15 seconds. [{ \"Id\": \"OCS\", \"ExecutionPeriod\" : \"00:00:15\", \"Endpoint\" : \" https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\" : \"{clientId}\", \"ClientSecret\" : \"{clientSecret}\" }] Egress data to OCS - streams with a specific TypeId value, every 15 seconds. [{ \"Id\": \"OCS\", \"ExecutionPeriod\" : \"00:00:15\", \"EgressFilter\" : \"TypeId:myType\", \"Endpoint\" : \" https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\" : \"{clientId}\", \"ClientSecret\" : \"{clientSecret}\" }] Egress data to OCS - all streams, every 15 seconds, including backfilling. [{ \"Id\": \"OCS\", \"ExecutionPeriod\" : \"00:00:15\", \"Backfill\" : true, \"Endpoint\" : \" https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\" : \"{clientId}\", \"ClientSecret\" : \"{clientSecret}\" }] Egress diagnostic data to OCS - every 1 hour. [{ \"Id\": \"OCS\", \"ExecutionPeriod\" : \"01:00:00\", \"Endpoint\" : \" https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\" : \"{clientId}\", \"ClientSecret\" : \"{clientSecret}\", \"NamespaceId\" : \"diagnostics\" }] Egress data to PI - all streams, every 15 seconds, including both type and stream prefix. All properties explicitly listed. [{ \"Id\": \"PI\", \"Name\" : null, \"Description\" : null, \"ExecutionPeriod\" : \"00:00:15\", \"Enabled\" : true, \"Backfill\" : false, \"EgressFilter\" : null, \"Endpoint\" : \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"ClientId\" : null, \"ClientSecret\" : null, \"Username\" : \"{username}\", \"Password\" : \"{password}\", \"StreamPrefix\" : \"1ValidPrefix.\", \"TypePrefix\" : \"AlsoValid_\", \"DebugExpiration\" : null, \"NamespaceId\" : \"default\", \"TokenEndpoint\" : null, \"ValidateEndpointCertificate\" : true }] Egress data to PI - streams whose Id contains \"Modbus\" or \"Opc\", every 1 minute. [{ \"Id\": \"PI\", \"ExecutionPeriod\" : \"00:01:00\", \"EgressFilter\" : \"Id:*Modbus* OR Id:*Opc*\", \"Endpoint\" : \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"Username\" : \"{username}\", \"Password\" : \"{password}\" }] Egress data to PI - streams containing a field that begins with \"Unique\", every 1 hour. [{ \"Id\": \"PI\", \"ExecutionPeriod\" : \"01:00:00\", \"EgressFilter\" : \"Unique*\", \"Endpoint\" : \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"Username\" : \"{username}\", \"Password\" : \"{password}\" }] REST URLs Relative URL HTTP verb Action api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints GET Gets all configured egress endpoints api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints DELETE Deletes all configured egress endpoints api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints POST Add an array of egress endpoints, will fail if any endpoint already exists api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints POST Add a single egress endpoints, will fail if endpoint already exists api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints PUT Replace all egress endpoints api/v1/configuration/storage/periodicegressendpoints/{id} api v1 configuration storage periodicegressendpoints {id} GET Get configured endpoint with id api/v1/configuration/storage/periodicegressendpoints/{id} api v1 configuration storage periodicegressendpoints {id} DELETE Delete configured endpoint with id api/v1/configuration/storage/periodicegressendpoints/{id} api v1 configuration storage periodicegressendpoints {id} PUT Replace egress endpoint with id , will fail if endpoint doesn\u0027t exist api/v1/configuration/storage/periodicegressendpoints/{id} api v1 configuration storage periodicegressendpoints {id} PATCH Allows partial updating of configured endpoint with id Egress execution details After you add configuration for an egress endpoint, egress execution will periodically occur for that endpoint. Egress is handled individually per configured endpoint. Only the first execution types and containers will be egressed; subsequently only new or changed types/containers types containers will be egressed. Only streams with a single, timeseries-based index can be egressed . Type creation must be successful in order to perform container creation; likewise container creation must be successful in order to perform data egress. Type, container and data items are batched into one or more OMF messages when egressing. Per the requirements defined in OMF, a single message will not exceed 192KB in size. Compression is automatically applied to outbound egress messages. On the destination, failure to add a single item will result in the message failing. In that case the Edge Data Store will fall back to egressing each item individually, per type or stream (that is each type, each stream, all data for a single stream). Types, containers, and data will continue to be egressed as long as the destination continues to respond to HTTP requests - retrying previous failures as needed. Certain HTTP failures during egress will result in a retry. The Edge Data Store will retry an HTTP request a maximum of five times with exponentially increasing delays between each request. The total time waiting and retrying is currently set at 1 minute. During that time egress of other messages will be delayed. List of retryable occurrences: TimeoutException HttpRequestException HttpStatusCode RequestTimeout (408) HttpStatusCode BadGateway (502) HttpStatusCode ServiceUnavailable (503) HttpStatusCode GatewayTimeout (504) For data collection and egress, in-memory and on-disk storage are used to track the last successfully-egressed data event, per stream. Data is egressed in order and includes future events. Note When an event with a future timestamp is successfully egressed, only values after the associated timestamp of that event will be egressed. Destination Preparation The various OSIsoft OMF destinations may require additional configuration. See details below to prepare an OSIsoft destination to receive OMF messages. OCS To prepare OCS to receive OMF messages from EDS, add an OMF connection. Creating an OMF connection results in an available OMF endpoint that can be used by the EDS egress mechanism. The basics steps associated with creating an OMF connection are as follows (see OCS documentation for more help): Create a Client The Client Id and Client Secret will be used for the corresponding properties in the egress configuration Create an OMF type Connection The connection should link the created client to an existing namespace where the data will be stored The OMF Endpoint URL for the connection will be used as the egress configuration Endpoint property PI To prepare a PI Server to receive OMF messages from EDS, a PI Web API OMF endpoint must be available. The basic steps are as follows (see the PI Web API documentation for complete steps, as well as best practices and recommendations): Install PI Web API and enable the OSIsoft Messge Format (OMF) Services feature Configure PI Web API to use Basic authentication Note The certificate used by PI Web API must be trusted by the device running EDS, otherwise the egress configuration ValidateEndpointCertificate property needs to be set to false (this can be the case with a self-signed certificate but should only be used for testing purposes)."
                              },
    "V1/Configuration/Schemas/System_Components_schema.html":  {
                                                                   "href":  "V1/Configuration/Schemas/System_Components_schema.html",
                                                                   "title":  "Sample Edge Data Store components configuration",
                                                                   "keywords":  "Sample Edge Data Store components configuration [ { \"ComponentId\": \"OpcUa1\", \"ComponentType\": \"OpcUa\" }, { \"ComponentId\": \"Modbus1\", \"ComponentType\": \"Modbus\" }, { \"ComponentId\": \"Storage\", \"ComponentType\": \"EDS.Component\" } ] Edge Data Store components schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden System_Components_schema.json EdgeDataStoreConfig definitions Property Type Group ComponentId string #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig ComponentType string #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig ComponentId ComponentId is optional type: string defined in this schema ComponentId type string , nullable ComponentType ComponentType is optional type: string defined in this schema ComponentType type string , nullable EdgeDataStoreConfig properties Property Type Required Nullable Defined by ComponentConfigurations reference Optional Yes EdgeDataStoreConfig (this schema) ComponentConfigurations ComponentConfigurations is optional type: reference defined in this schema ComponentConfigurations type Array type: reference All items must be of the type: ??? #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required ComponentConfigurations array Optional ComponentConfigurations ComponentConfigurations is optional type: reference ComponentConfigurations type Array type: reference All items must be of the type: ??? #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig"
                                                               },
    "V1/Configuration/Schemas/Storage_schema.html":  {
                                                         "href":  "V1/Configuration/Schemas/Storage_schema.html",
                                                         "title":  "StorageConfiguration schema",
                                                         "keywords":  "StorageConfiguration schema \"Storage\": { \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\" }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"PeriodicEgressEndpoints\": [] } Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_Logging_schema.json Properties Property Type Required Nullable Defined by Runtime StorageRuntimeConfiguration Optional Yes StorageRuntimeConfiguration Logging StorageLoggingConfiguration Optional Yes StorageLoggingConfiguration PeriodicEgressEndpoints [PeriodicEgressEndpointsConfiguration] Optional Yes PeriodicEgressEndpointsConfiguration Runtime is optional type: StorageRuntimeConfiguration Logging is optional type: StorageLoggingConfiguration PeriodicEgressEndpoints is optional type: [PeriodicEgressEndpointsConfiguration]"
                                                     },
    "V1/Configuration/Schemas/Storage_Runtime_schema.html":  {
                                                                 "href":  "V1/Configuration/Schemas/Storage_Runtime_schema.html",
                                                                 "title":  "Sample storage runtime configuration",
                                                                 "keywords":  "Sample storage runtime configuration { \"StreamStorageLimitMb\": 2, \"StreamStorageTargetMb\": 1, \"IngressDebugExpiration\": \"0001-01-01T00:00:00\" } Storage runtime configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Storage_Runtime_schema.json StorageRuntimeConfiguration properties Property Type Required Nullable Defined by IngressDebugExpiration string Required No StorageRuntimeConfiguration (this schema) StreamStorageLimitMb integer Required No StorageRuntimeConfiguration (this schema) StreamStorageTargetMb integer Required No StorageRuntimeConfiguration (this schema) IngressDebugExpiration IngressDebugExpiration is required type: string defined in this schema Ingress Debug Expiration is a property that can be used when debugging OMF. If the date and time is the future incoming OMF messages will be logged until the date and time specified. Once the configured time is past OMF messages will no longer be logged for debugging purposes. IngressDebugExpiration type string format: date-time ??? date and time (according to RFC 3339, section 5.6 ) minimum length: 1 characters StreamStorageLimitMb StreamStorageLimitMb is required type: integer defined in this schema StreamStorageLimitMb type integer minimum value: 2 maximum value: 2147483647 StreamStorageLimitMb is the maximum size in megabytes that a stream can reach. When a stream exceeds the size specified, older data will be deleted from the file. Data will be removed from the stream until the stream is at or below the StreamStorageTargetMb value. It is recommended that the target value be smaller than the limit since trimming can be an expensive operation and should be done infrequently. StreamStorageTargetMb StreamStorageTargetMb is required type: integer defined in this schema StreamStorageTargetMb type integer minimum value: 1 maximum value: 2147483647 StreamStorageTargetMb is the size in megabytes that a stream will be reduced to after StreamStorageLimitMb size is reached for a single stream. When a stream exceeds the size specified, older data will be deleted from the file. Data will be removed from the stream until the stream is at or below the StreamStorageTargetMb value. It is recommended that the target value be smaller than the limit since trimming can be an expensive operation and should be done infrequently. All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required IngressDebugExpiration string Required StreamStorageLimitMb integer Required StreamStorageTargetMb integer Required"
                                                             },
    "V1/Configuration/Schemas/Storage_PeriodicEgressEndpoints_schema.html":  {
                                                                                 "href":  "V1/Configuration/Schemas/Storage_PeriodicEgressEndpoints_schema.html",
                                                                                 "title":  "Periodic egress configuration schema",
                                                                                 "keywords":  "Periodic egress configuration schema This schema is used to configure data egress from Edge Data Store to a PI Server or to OCS. Sample periodic egress configuration file [{ \"Id\": \"OCS Data\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"PI Web API Data\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cyourserver\u003e/piwebapi/omf/\", \"https:  \u003cyourserver\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"OCS Diagnostics\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"PI Web API Diagnostics\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cyourserver\u003e/piwebapi/omf/\", \"https:  \u003cyourserver\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true } ] Abstract Extensible Status Identifiable Custom properties Additional properties Defined Ii Can be instantiated Yes Experimental No Forbidden Forbidden Storage_PeriodicEgressEndpoints_schema.json PeriodicEgressConfiguration properties Property Type Required Nullable Defined by Backfill boolean Optional No PeriodicEgressConfiguration (this schema) ClientId string Optional Yes PeriodicEgressConfiguration (this schema) ClientSecret string Optional Yes PeriodicEgressConfiguration (this schema) DebugExpiration string Optional Yes PeriodicEgressConfiguration (this schema) Description string Optional Yes PeriodicEgressConfiguration (this schema) EgressFilter string Optional Yes PeriodicEgressConfiguration (this schema) Enabled boolean Optional No PeriodicEgressConfiguration (this schema) Endpoint string Required No PeriodicEgressConfiguration (this schema) ExecutionPeriod string Required No PeriodicEgressConfiguration (this schema) Id string Optional Yes PeriodicEgressConfiguration (this schema) Name string Optional Yes PeriodicEgressConfiguration (this schema) NamespaceId string Optional Yes PeriodicEgressConfiguration (this schema) Password string Optional Yes PeriodicEgressConfiguration (this schema) StreamPrefix string Optional Yes PeriodicEgressConfiguration (this schema) TokenEndpoint string Optional Yes PeriodicEgressConfiguration (this schema) TypePrefix string Optional Yes PeriodicEgressConfiguration (this schema) UserName string Optional Yes PeriodicEgressConfiguration (this schema) ValidateEndpointCertificate boolean Optional No PeriodicEgressConfiguration (this schema) Backfill Backfill is optional type: boolean defined in this schema Backfill type boolean ClientId ClientId is optional type: string defined in this schema ClientId type string , nullable ClientSecret ClientSecret is optional type: string defined in this schema ClientSecret type string , nullable DebugExpiration DebugExpiration is optional type: string defined in this schema DebugExpiration type string , nullable format: date-time ??? date and time (according to RFC 3339, section 5.6 ) Description Description is optional type: string defined in this schema Description type string , nullable EgressFilter EgressFilter is optional type: string defined in this schema EgressFilter type string , nullable Enabled Enabled is optional type: boolean defined in this schema Enabled type boolean Endpoint Endpoint is required type: string defined in this schema Endpoint type string minimum length: 1 characters ExecutionPeriod ExecutionPeriod is required type: string defined in this schema ExecutionPeriod type string minimum length: 1 characters Id Id is optional type: string defined in this schema Id type string , nullable Name Name is optional type: string defined in this schema Name type string , nullable NamespaceId NamespaceId is optional type: string defined in this schema NamespaceId type string , nullable Password Password is optional type: string defined in this schema Password type string , nullable StreamPrefix StreamPrefix is optional type: string defined in this schema StreamPrefix type string , nullable TokenEndpoint TokenEndpoint is optional type: string defined in this schema TokenEndpoint type string , nullable TypePrefix TypePrefix is optional type: string defined in this schema TypePrefix type string , nullable UserName UserName is optional type: string defined in this schema UserName type string , nullable ValidateEndpointCertificate ValidateEndpointCertificate is optional type: boolean defined in this schema ValidateEndpointCertificate type boolean All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Backfill boolean Optional ClientId string Optional ClientSecret string Optional DebugExpiration string Optional Description string Optional EgressFilter string Optional Enabled boolean Optional Endpoint string Required ExecutionPeriod string Required Id string Optional Name string Optional NamespaceId string Optional Password string Optional StreamPrefix string Optional TokenEndpoint string Optional TypePrefix string Optional UserName string Optional ValidateEndpointCertificate boolean Optional Backfill Backfill is optional type: boolean Backfill type boolean ClientId ClientId is optional type: string ClientId type string , nullable ClientSecret ClientSecret is optional type: string ClientSecret type string , nullable DebugExpiration DebugExpiration is optional type: string DebugExpiration type string , nullable format: date-time ??? date and time (according to RFC 3339, section 5.6 ) Description Description is optional type: string Description type string , nullable EgressFilter EgressFilter is optional type: string EgressFilter type string , nullable Enabled Enabled is optional type: boolean Enabled type boolean Endpoint Endpoint is required type: string Endpoint type string minimum length: 1 characters ExecutionPeriod ExecutionPeriod is required type: string ExecutionPeriod type string minimum length: 1 characters Id Id is optional type: string Id type string , nullable Name Name is optional type: string Name type string , nullable NamespaceId NamespaceId is optional type: string NamespaceId type string , nullable Password Password is optional type: string Password type string , nullable StreamPrefix StreamPrefix is optional type: string StreamPrefix type string , nullable TokenEndpoint TokenEndpoint is optional type: string TokenEndpoint type string , nullable TypePrefix TypePrefix is optional type: string TypePrefix type string , nullable UserName UserName is optional type: string UserName type string , nullable ValidateEndpointCertificate ValidateEndpointCertificate is optional type: boolean ValidateEndpointCertificate type boolean"
                                                                             },
    "V1/Configuration/Schemas/Storage_Logging_schema.html":  {
                                                                 "href":  "V1/Configuration/Schemas/Storage_Logging_schema.html",
                                                                 "title":  "Storage logging configuration schema",
                                                                 "keywords":  "Storage logging configuration schema The Storage logging configuration schema specifies how to formally describe the logging parameters for Storage. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden StorageLoggerConfiguration Properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) LogFileCountLimit LogFileCountLimit is optional type: integer defined in this schema LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer defined in this schema LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference defined in this schema LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional"
                                                             },
    "V1/Configuration/Schemas/OpcUa_schema.html":  {
                                                       "href":  "V1/Configuration/Schemas/OpcUa_schema.html",
                                                       "title":  "OpcUaConfiguration Schema",
                                                       "keywords":  "OpcUaConfiguration Schema Abstract Extensible Status Identifiable Custom Properties Additional Properties Defined In Can be instantiated Yes Experimental No Forbidden Forbidden OpcUa_Logging_schema.json Properties Property Type Required Nullable Defined by Logging OpcUaLoggingConfiguration Optional Yes EdgeLoggerConfiguration DataSource DataSourceConfiguration Optional Yes ComponentsConfiguration DataSelection [OpcUaDataSelectionConfiguration] Optional Yes DataSelectionConfiguration Logging is optional type: OpcUaLoggingConfiguration DataSource is optional type: DataSourceConfiguration DataSelection is optional type: [OpcUaDataSelectionConfiguration]"
                                                   },
    "V1/Configuration/Schemas/OpcUa_Logging_schema.html":  {
                                                               "href":  "V1/Configuration/Schemas/OpcUa_Logging_schema.html",
                                                               "title":  "OPC UA logger configuration schema",
                                                               "keywords":  "OPC UA logger configuration schema The OPC UA logger configuration schema specifies how to formally describe the logging parameters for OPC UA. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden OpcUa_Logging_schema.json OpcUaLoggerConfiguration Properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) LogFileCountLimit LogFileCountLimit is optional type: integer defined in this schema LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer defined in this schema LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference defined in this schema LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional LogFileCountLimit LogFileCountLimit is optional type: integer LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel"
                                                           },
    "V1/Configuration/Schemas/OpcUa_DataSource_schema.html":  {
                                                                  "href":  "V1/Configuration/Schemas/OpcUa_DataSource_schema.html",
                                                                  "title":  "Sample OPC UA data source configuration",
                                                                  "keywords":  "Sample OPC UA data source configuration The OPC UA data source configuration schema specifies how to formally describe the data source parameters for OPC UA. { \"EndpointUrl\": \"opc.tcp://\u003cip \"opc.tcp:  \u003cip address\u003e:\u003cport - often 62541\u003e/\u003cserver 62541\u003e \u003cserver path\u003e\", \"UseSecureConnection\": false, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": \"OpcUa\" } OPC UA data source configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden OpcUa_DataSource_schema.json DataSourceConfiguration properties Property Type Required Nullable Defined by EndpointUrl string Optional Yes DataSourceConfiguration (this schema) IncomingTimestamp reference Optional No DataSourceConfiguration (this schema) Password string Optional Yes DataSourceConfiguration (this schema) RootNodeIds string Optional Yes DataSourceConfiguration (this schema) StreamIdPrefix string Optional Yes DataSourceConfiguration (this schema) UseSecureConnection boolean Optional No DataSourceConfiguration (this schema) UserName string Optional Yes DataSourceConfiguration (this schema) EndpointUrl EndpointUrl is optional type: string defined in this schema EndpointUrl type string , nullable IncomingTimestamp IncomingTimestamp is optional type: reference defined in this schema IncomingTimestamp type ??? #/definitions/IncomingTimestampType # definitions IncomingTimestampType Password Password is optional type: string defined in this schema Password type string , nullable RootNodeIds RootNodeIds is optional type: string defined in this schema RootNodeIds type string , nullable StreamIdPrefix StreamIdPrefix is optional type: string defined in this schema StreamIdPrefix type string , nullable UseSecureConnection UseSecureConnection is optional type: boolean defined in this schema UseSecureConnection type boolean UserName UserName is optional type: string defined in this schema UserName type string , nullable All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required EndpointUrl string Optional IncomingTimestamp Optional Password string Optional RootNodeIds string Optional StreamIdPrefix string Optional UseSecureConnection boolean Optional UserName string Optional EndpointUrl EndpointUrl is optional type: string EndpointUrl type string , nullable IncomingTimestamp IncomingTimestamp is optional type: reference IncomingTimestamp type ??? #/definitions/IncomingTimestampType # definitions IncomingTimestampType Password Password is optional type: string Password type string , nullable RootNodeIds RootNodeIds is optional type: string RootNodeIds type string , nullable StreamIdPrefix StreamIdPrefix is optional type: string StreamIdPrefix type string , nullable UseSecureConnection UseSecureConnection is optional type: boolean UseSecureConnection type boolean UserName UserName is optional type: string UserName type string , nullable"
                                                              },
    "V1/ReleaseNotes/ReleaseNotes.html":  {
                                              "href":  "V1/ReleaseNotes/ReleaseNotes.html",
                                              "title":  "Edge Data Store release notes",
                                              "keywords":  "Edge Data Store release notes Overview Edge Data Store is supported on a variety of platforms and processors. OSIsoft provides ready to use install kits for the following platforms: Windows 10 x64 - EdgeDataStore.msi (Intel/AMD (Intel AMD 64 bit processors) Debian 9 or later x64/AMD64 x64 AMD64 - EdgeDataStore_linux-x64.deb (Intel/AMD (Intel AMD 64 bit processors) Debian 9 or later ARM32 - EdgeDataStore_linux-arm.deb (Raspberry PI 2,3,4, BeagleBone devices, other ARM v7 and ARM v8 32 bit processors) In addition to ready to use install kits, OSIsoft also provides examples of how to create Docker containers in a separate file. tar.gz files are provided with binaries for customers who want to build their own custom installers or containers for Linux. Differences from Beta 2 General The \"OSIsoft Edge System\" product was renamed to \"OSIsoft Edge Data Store\". The edgecmd command line utility is now provided to allow access to and modification of Edge Data Store configuration. This utility supercedes the command line functionality that was previously available via OSIsoft.Data.System.Host. Improvements were made to ensure component health status updates may not be lost when the product is shutdown. When the reset functionality for the entire product or the storage component is invoked, the product now properly restarts. The OPCUA and Modbus adapters may now be enabled at install time of the product. The structure for health streams produced by the product has been updated. Adapter components may be added or removed at runtime and no longer require a restart of the product. Changes to the Health Endpoints configuration are now applied at runtime and no longer require a restart of the product. All endpoint configurations related to transfering data and configuration to PI Web Api or OSIsoft Cloud Services have the following new properties: ValidateEndpointCertificate - Enable/Disable Enable Disable validation of endpoint certificate. Any endpoint certificate is accepted if set to false. TokenEndpoint - For use with OSIsoft Cloud Services endpoints only. Allows for alternative endpoint for retrieval of an OCS access token. Modbus Adapter Support has been added for a user-defined optional Streamid prefix. Storage Significant improvements have been made in the reliability and performance of egressing configuration and data to PI Web Api or OSIsoft Cloud Services. Improvements were made to ensure that no data is lost when egressing data to an egress endpoint. The OEM configuration facet of the Storage component has been deprecated. The following configuration properties were relocated to the Storage Runtime configuration facet: CheckpointRateInSec TransactionLogLimitMB EnableTransactionLog The Id property of a PeriodicEgressEndpoint configuration has been changed to be optional. If one is not provided when the endpoint is configured, a unique value will be assigned to it. Improvements were made to improve resiliency of the product by ensuring data and configuration are properly checkpointed to storage. Improvements were made to handle a wider range of data corruptions encountered in power loss scenarios. In Beta 2, under certain data egress scenarios, the Storage component would attempt to retrieve all data destined to be egressed, and then egress the data to the destination endpoint. This could lead to high memory usage and potential stability issues. This behavior has been changed to stream the data in a more controlled manner, leading to less memory being demanded. Install Edge Data Store on a Device using an install kit To use any of the installers, first copy the appropriate file to the file system of the device. Windows (Windows 10 x64) Double click the EdgeDataStore.msi file in Windows Explorer or execute the file from a command prompt. You will be prompted for install location and default port, and when the install finishes, the EdgeDataStore will be installed and running on either the default port 5590 or the port you specified during the install. Debian 9 or later Linux (Ubuntu Raspberry PI, BeagleBone, other Debian based Linux distros) Complete the following: Open a terminal window and type: sudo apt install ./EdgeDataStore_linux_\u003ceither . EdgeDataStore_linux_\u003ceither x64 or arm depending upon processor\u003e.deb A check will be done for prerequisites. If the Linux operating system is up to date, the install will succeed. If the install fails, run the following commands from the terminal window and try the install again: sudo apt update sudo apt upgrade After the check for prerequisites succeeds, a prompt will display asking if you want to change the default port (5590). If you want to change the port, type in another port number in the acceptable range for the operating system you are using. If 5590 is acceptable, press Enter. The install will complete and EdgeDataStore will be running on your device. You can verify that EdgeDataStore is correctly installed by running the following script from the terminal window. Note: Depending on the processor, memory, and storage, it may take the system a few seconds to start up. curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If the installation was successful, you will get back a JSON copy of the default system configuration: { \"Storage\": { \"PeriodicEgressEndpoints\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableTransactionLog\": true }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"Modbus1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": {}, \"DataSelection\": [] }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ] }, \"OpcUa1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": {}, \"DataSelection\": [] } } If you get back an error, wait a few seconds and try it again. On a device with limited processor, memory, and slow storage, it may take some time before Edge Data Store is fully initialized and running for the first time."
                                          },
    "V1/Overview/VisualizationQuickStart.html":  {
                                                     "href":  "V1/Overview/VisualizationQuickStart.html",
                                                     "title":  "Edge Data Store visualization quick start",
                                                     "keywords":  "Edge Data Store visualization quick start This document is a quick tour of getting data from the Edge Data Store Storage component and displaying it on the screen of the device where the Edge Data Store is installed. This example is intended for simplicity rather than thoroughness, and should run on any device that is supported by Edge Data Store. This example will iterate through all streams in the default namespace on the Edge Data Store and continously display the latest values to the screen. This example assumes the Edge Data Store was installed with the default port (5590). using System; using System.Collections.Generic; using System.Net.Http; using Newtonsoft.Json; namespace EdgeDataScroller { class EdgeStream { public string Id { get; set; } } class DataScroller { static HttpClient _client = new HttpClient(); private static Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e GetDataForNamespace(string ns) { Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e outputs = new Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e(); string uri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/{0}/streams\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces {0} streams\", ns); string json = _client.GetStringAsync(uri).Result; List\u003cEdgeStream\u003e streams = JsonConvert.DeserializeObject\u003cList\u003cEdgeStream\u003e\u003e(json); foreach (var stream in streams) { string lastValueUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/{0}/streams/{1}/Data/Last\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces {0} streams {1} Data Last\", ns, stream.Id.Trim()); string lastValueJson = _client.GetStringAsync(lastValueUri).Result; Dictionary\u003cstring, object\u003e values = JsonConvert.DeserializeObject\u003cDictionary\u003cstring, object\u003e\u003e(lastValueJson); outputs.Add(stream.Id.Trim(), values); } return outputs; } static void DisplayData(List\u003cstring\u003e namespaces, TimeSpan interval) { Dictionary\u003cstring, Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e\u003e outputs = new Dictionary\u003cstring, Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e\u003e(); Console.WriteLine(\"Data Displayed at \" + DateTime.UtcNow.ToString(\"o\")); foreach (string ns in namespaces) { outputs.Add(ns, GetDataForNamespace(ns)); } foreach (string ns in outputs.Keys) { foreach (string stream in outputs[ns].Keys) { if (null == outputs[ns][stream]) { Console.WriteLine(\"No values for \" + stream); continue; } foreach (string field in outputs[ns][stream].Keys) { object obj = outputs[ns][stream][field]; string value = obj.ToString(); if (obj is DateTime) { value = ((DateTime)obj).ToString(\"o\"); } Console.WriteLine($\"{ns}.{stream}.{field} = {value}\"); } } Console.WriteLine(\"****\"); } Console.WriteLine(string.Empty); System.Threading.Thread.Sleep(interval); } static void Main(string[] args) { List\u003cstring\u003e namespaces = new List\u003cstring\u003e(); namespaces.Add(\"default\"); TimeSpan interval = TimeSpan.FromSeconds(5.0); if (null != args \u0026\u0026 args.Length \u003e 0) { string choice = args[0].Trim().ToLowerInvariant(); if (choice == \"all\") { namespaces.Add(\"diagnostics\"); } if (choice == \"diagnostics\") { namespaces.Clear(); namespaces.Add(\"diagnostics\"); } if (args.Length \u003e 1) { string newInterval = args[1].Trim(); double newValue = -1.0; if (double.TryParse(newInterval, out newValue)) { interval = TimeSpan.FromSeconds(newValue); } } } while (true) DisplayData(namespaces, interval); } } }"
                                                 },
    "V1/Overview/OpcUaQuickStart.html":  {
                                             "href":  "V1/Overview/OpcUaQuickStart.html",
                                             "title":  "Edge OPC UA quick start",
                                             "keywords":  "Edge OPC UA quick start This topic is a quick tour of setting up the Edge OPC UA component. It is possible to add a single EDS OPC UA adapter during Edge Data Store installation named OpcUa1. If multiple EDS OPC UA adapters are desired, please reference Edge Data Store Configuration on how to add a new component to Edge Data Store. The example below covers configuring the adapter added during installation. If another adapter has been installed, please substitute the name of the installed adapter in the below example for OpcUa1. Configure an OPC UA data source Create a file in JSON format describing the location of the data source. Modify the following values to match your environment. { \"EndpointUrl\": \"opc.tcp://\u003cip \"opc.tcp:  \u003cip address\u003e:\u003cport - often 62541\u003e/\u003cserver 62541\u003e \u003cserver path\u003e\", \"UseSecureConnection\": false, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": \"OpcUa\" } Enter the correct IP address and port for your OPC UA data source. Save the file with the name OpcUa1Datasource.json. Run the following curl script from the same directory where the file is located. You should run the script on the same computer where the Edge Data Store is installed: curl -i -d \"@OpcUa1Datasource.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Datasource http:  localhost:5590 api v1 configuration OpcUa1 Datasource When this command completes successfully (a 204 is returned by curl), your OPC UA data source has been created. If you get a 400 error, check your JSON file for errors. If you get a 404 or 500 error, check to make sure Edge Data Store is running on your computer. Configure OPC UA data selection Select the OPC UA data you want to store in Edge Data Store by configuring OPC UA data selection. The following is a sample JSON for five OPC UA values. Modify the values as appropriate for your environment. [{ \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"StreamId\": null } ] Save the JSON content above in a text file and name it OpcUa1Dataselection.json. Run the following curl script so the system will be configured to collect Opc Ua data values. curl -i -d \"@OpcUa1Dataselection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection To see the streams that have been created in Edge Storage to store the data you are writing, you can run the following curl script: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/ http:  localhost:5590 api v1 tenants default namespaces default streams  To view the data in the streams being written, you can refer to the SDS part of this documentation. To egress the data to OSIsoft Cloud Services or the PI System, see the egress documentation or quick starts."
                                         },
    "V1/Overview/OMFQuickStart.html":  {
                                           "href":  "V1/Overview/OMFQuickStart.html",
                                           "title":  "Edge Storage OMF Quick Start",
                                           "keywords":  "Edge Storage OMF Quick Start This document is a quick tour of getting data into the Edge Storage component using the OSisoft Message Format (OMF), and then retrieving the data using the Sequential Data Store (SDS) API. Both OMF data ingress and SDS data retrieval are accomplished using REST APIs. This tour assumes the Edge Data Store has been installed, and is accessible via a REST API using the default installed port (5590). This tour will use curl, a commonly available tool on both Windows and Linux, and use command line commands. The same operations can be used with any programming language or tool that supports making REST calls. In addition data retrieval steps (GET commands) can be accomplished using a browser if one is available on the device. Create an OMF Type The first step in OMF data ingress is to create an OMF type that describes the format of the data to be stored in a container. In our example the data to be written is a timestamp and a numeric value, so the OMF JSON describing the type is: [{ \"id\": \"MyCustomType\", \"classification\": \"dynamic\", \"type\": \"object\", \"properties\": { \"Timestamp\": { \"type\": \"string\", \"format\": \"date-time\", \"isindex\": true }, \"Value\": { \"type\": \"number\", \"format\": \"float32\" } } }] The value is indexed by a timestamp and the numeric value that will be stored is a 32 bit floating point value. In order to create the OMF type in the Edge Storage, the JSON should be stored as a file with the name OmfCreateType.json and run the following curl script: curl -i -d \"@OmfCreateType.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: type\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  When this command completes successfully, an SDS type with the same name will have been created on the server. Any number of containers can be created from the type, as long as they use a timestamp as an index and a 32 bit floating point value. Type creation only needs to be done the first time you send using a custom application, but it does not cause an error if you resend the same definition at a later time. Create an OMF Container The next step in writing OMF data is to create a container. As with an OMF Type, this only needs to be done once before sending data events, and resending the same definition repeatedly does not cause an error. [{ \"id\": \"MyCustomContainer\", \"typeid\": \"MyCustomType\" }] This container references the type that was created in the last step, and an error will occur if the type does not exist when the container is created. In order to create the OMF container in the Edge Storage, the JSON should be stored as a file with the name OmfCreateContainer.json and the following curl script run: curl -i -d \"@OmfCreateContainer.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: container\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  When this command completes successfully, an SDS stream will have been created to store data defined by the type. Write Data Events to the OMF Container Now that the type and container have been created, we can write data using OMF: [{ \"containerid\": \"MyCustomContainer\", \"values\": [{ \"Timestamp\": \"2019-07-16T15:18:24.9870136Z\", \"Value\": 12345.6789 }, { \"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789 } ] }] This example includes two data events that will be stored in the SDS Stream that was created in the previous steps. It is generally a best practice to batch OMF values when writing them for the best performance. In order to write the data in the Edge Storage, the JSON should be stored as a file with the name OmfCreateDataEvents.json and the following curl script run: curl -i -d \"@OmfCreateDataEvents.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: data\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  When this command completes successfully, two values will have been written to the SDS stream. Read Last Data written using SDS In order to read the data back from the server that has been written, you can use the SDS REST API. Here is an example curl script that reads back the last value entered: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data Last When run this GET command returns the last value written: {\"Time\":\"2017-11-23T18:00:00Z\",\"Measurement\":60.0} Read a range of data events written using SDS In order to read the data back from the server that has been written, you can use the SDS REST API. Here is an example curl script that reads back a time range of values that have been written: curl \"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" \"http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" This command will return up to 100 values after the startIndex specified: [{\"Time\":\"2017-11-23T17:00:00Z\",\"Measurement\":50.0},{\"Time\":\"2017-11-23T18:00:00Z\",\"Measurement\":60.0}] Both values that were entered were returned - up to 100 values after the specified timestamp would be returned. For more information on SDS APIs see the SDS section of this documentation."
                                       },
    "V1/Overview/OCSEgressQuickStart.html":  {
                                                 "href":  "V1/Overview/OCSEgressQuickStart.html",
                                                 "title":  "OSIsoft Cloud Services (OCS) egress quick start",
                                                 "keywords":  "OSIsoft Cloud Services (OCS) egress quick start This topic provides a quick start for getting data stored in the Edge Data Store into OCS. You can accomplish this by using the OCS OMF endpoint which is configured for OCS authentication. Create a periodic egress configuration Configure Edge Storage periodic egress for the Osisoft Cloud Services (OCS) endpoint and credentials: [{ \"Id\": \"OCS\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"https://\u003cyour \"https:  \u003cyour OCS OMF endpoint endpoint\u003e\", \"ClientId\": \"\u003cyour OCS ClientId\u003e\", \"ClientSecret\": \"\u003cyour OCS ClientSecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null, \"ValidateEndpointCertificate\": true, \"TokenEndpoint\": null }] Type the URL of your OCS OMF endpoint into the \"Endpoint\": value in the preceding JSON file. Type a ClientId and ClientSecret that can write data to your OCS tenant and namespace in the \"ClientId\": and \"ClientSecret\": values in the preceding JSON file. Note: If required, you can use the StreamPrefix and TypePrefix to ensure uniqueness on the destination system. If a StreamPrefix is specified, it will be used to create a unique stream id on OCS. This configuration is set up to send all stream data to OCS. If you want to only send specific streams, edit the EgressFilter value. Examples of more advanced scenarios are in the Egress section of this documentation. Save the JSON with the file name PeriodicEgressEndpoints.json. Run the following curl script to configure the Edge Storage component to send data to OCS. Run the script from the same directory where the file exists on the device where Edge Data Store is installed. You can run the file and curl script from any directory on the device as long as the file and the curl script are run from the same directory: curl -i -d \"@PeriodicEgressEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/storage/PeriodicEgressEndpoints/ http:  localhost:5590 api v1 configuration storage PeriodicEgressEndpoints  When this command completes successfully, data will start being egressed to OCS."
                                             },
    "V1/Overview/ModbusQuickStart.html":  {
                                              "href":  "V1/Overview/ModbusQuickStart.html",
                                              "title":  "Edge Modbus TCP quick start",
                                              "keywords":  "Edge Modbus TCP quick start This topic provides a quick start to setting up the Edge Modbus TCP component. It is possible to add a single EDS Modbus TCP adapter during installation named Modbus1. If multiple EDS Modbus TcP adapters are desired, please reference Edge Data Store Configuration on how to add a new component to Edge Data Store. The examples below will change if a different adapter is being configured - please replace Modbus1 with the name of the component you have added. Configure a Modbus TCP data source Create a file in JSON format describing the location of the Modbus data source. The timeouts are in milliseconds. { \"IpAddress\": \"\u003cModbus IP Address\u003e\", \"Port\": \u003cPort - usually 502\u003e, \"ConnectTimeout\": 15000, \"ReconnectInterval\": 5000, \"RequestTimeout\": 9000, \"DelayBetweenRequests\": 0, \"MaxResponseDataLength\": 250 } Enter the correct IP address and port for your Modbus data source. Save the file with the name Modbus1DataSource.json. Run the following curl script from the same directory where the file is located. Note: You should run the script on the same computer where the Edge Data Store is installed: curl -i -d \"@Modbus1Datasource.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/Modbus1/Datasource http:  localhost:5590 api v1 configuration Modbus1 Datasource When this command completes successfully (a 204 is returned by curl), your Modbus TCP data source has been created. If you get a 400 error, check your JSON file for errors. If you get a 404 or 500 error, check to make sure Edge Data Store is running on your computer. Configure Modbus data selection Select the Modbus TCP data you want to store in Edge Data Store by configuring Modbus data selection. The following is a sample JSON for 5 Modbus values. Modify the values as appropriate for your Modbus environment. [{ \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 2, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 3, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 4, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 5, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 } ] Save the JSON content above in a text file and name it Modbus1Dataselection.json. Note: When you run the following curl script, the system will be configured to collect Modbus data values. curl -i -d \"@Modbus1Dataselection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/Modbus1/Dataselection http:  localhost:5590 api v1 configuration Modbus1 Dataselection To see the streams that have been created in Edge Storage to store the data you are writing, you can run the following curl script: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/ http:  localhost:5590 api v1 tenants default namespaces default streams  To view the data in the streams being written by Modbus, you can refer to the SDS part of this documentation. To egress the data to OSIsoft Cloud Services or the PI System, see the egress documentation or quick starts."
                                          },
    "V1/Overview/CommandLineQuickStartWindows.html":  {
                                                          "href":  "V1/Overview/CommandLineQuickStartWindows.html",
                                                          "title":  "Edge Data Store command line quick start - Windows",
                                                          "keywords":  "Edge Data Store command line quick start - Windows One of the ways to configure Edge Data Store is with the edgecmd.exe command line tool. To access the tool on Windows, open a command prompt and navigate to the directory where Edge Data Store is installed. This is usually in the following location: C:\\Program Files\\OSIsoft\\EdgeDataStore\\ It is recommended that you do not copy or delete any files in that directory. To invoke the tool you can use the full path from a different directory: C:\\Users\\John\u003e\"C:\\Program Files\\OSIsoft\\EdgeDataStore\\edgecmd.exe\" Help ************************************************************************************************************************ Welcome to OSIsoft Edge Data Store configuration utility. Utility version: 1.0.0.148 ************************************************************************************************************************ --------------------------------------------------------------------------------------------------------- Command-line options =\u003e \u0027Configuration\u0027, \u0027Help\u0027 --------------------------------------------------------------------------------------------------------- Please enter ID of a component you would like to configure or to get component specific help output. Example: .\\edgecmd.exe Help ComponentId .\\edgecmd.exe Configuration ComponentId To get set of components registered to the Edge Data Store please run: .\\edgecmd.exe Configuration System Components To configure the system, please use \u0027System\u0027 as the ComponentId. Example of getting System help output: .\\edgecmd.exe Help System Example of configuring System Logging level: .\\edgecmd.exe Configuration System logging LogLevel=Warning C:\\Users\\John\u003e Most configuration options that can be done using REST can also be done using the edgecmd utility and command line arguments. Generally the configuration and administrative REST interfaces are exposed via the command line. Access to reading and writing data to the Edge Data Store Storage Component - OMF Ingress and SDS Read/Write Read Write capabilities are only available using the REST API."
                                                      },
    "V1/Overview/CommandLineLinuxQuickStart.html":  {
                                                        "href":  "V1/Overview/CommandLineLinuxQuickStart.html",
                                                        "title":  "Edge Data Store command line quick start - Linux",
                                                        "keywords":  "Edge Data Store command line quick start - Linux One of the ways to configure the Edge Data Store is with the edgecmd command line tool. To access the tool on Linux open a command prompt. The utility is available to use in any directory on Linux. debian@beaglebone:~$ edgecmd help ************************************************************************************************************************ Welcome to OSIsoft Edge Data Store configuration utility. Utility version: 1.0.0.148 ************************************************************************************************************************ --------------------------------------------------------------------------------------------------------- Command-line options =\u003e \u0027Configuration\u0027, \u0027Help\u0027 --------------------------------------------------------------------------------------------------------- Please enter ID of a component you would like to configure or to get component specific help output. Example: ./edgecmd . edgecmd Help ComponentId ./edgecmd . edgecmd Configuration ComponentId To get set of components registered to the Edge Data Store please run: ./edgecmd . edgecmd Configuration System Components To configure the system, please use \u0027System\u0027 as the ComponentId. Example of getting System help output: ./edgecmd . edgecmd Help System Example of configuring System Logging level: ./edgecmd . edgecmd Configuration System logging LogLevel=Warning debian@beaglebone:~$ Most configurations options that can be done using REST can also be done using the edgecmd utility and command line arguments. Generally the configuration and administrative REST interfaces are exposed via the command line. Access to reading and writing data to the Edge Data Store Storage Component - OMF Ingress and SDS Read/Write Read Write capabilities are only available using the REST API."
                                                    },
    "V1/Overview/AnalyticsQuickStart.html":  {
                                                 "href":  "V1/Overview/AnalyticsQuickStart.html",
                                                 "title":  "Edge Data Store analytics quick start",
                                                 "keywords":  "Edge Data Store analytics quick start This topic provides a quick start for a very simple analytic that can written using the Edge Data Store. The intended input device for this example is a Modbus TCP or other sensor that outputs 4 boolean values. The normal range of operation is that the values are neither all true or all false. If all values are true, the exception condition High is triggered. If all values are false, the exception condition Low is triggered. Any other combination of boolean values is Normal. Three analytic streams are created to track these changes. The ValueRangeHigh stream is 1 when High and 0 when anything else. The ValueRangeLow stream is -1 when Low and 0 when anything else. The ValueRangeOut stream is -1 when Low, 0 when Normal, and 1 when High. This example assumes the Edge Data Store was installed with the default port (5590). using System; using System.Collections.Generic; using System.Net.Http; using System.Text; using Newtonsoft.Json; using Newtonsoft.Json.Linq; namespace ExceptionReportingSample { class ModbusField { public string StreamId { get; set; } public int ScanRate { get; set; } } enum Alert { Normal, High, Low } class ExceptionReporting { static HttpClient _client = new HttpClient(); private static List\u003cstring\u003e StreamIds = new List\u003cstring\u003e(new string[] { \"SwitchState1\", \"SwitchState2\", \"SwitchState3\", \"SwitchState4\" }); private const string ValueRangeHigh = \"ValueRangeHigh\"; private const string ValueRangeLow = \"ValueRangeLow\"; private const string ValueRangeOut = \"ValueRangeOut\"; private const string TypeId = \"ValueRange\"; private const string ModbusComponentId = \"Modbus1\"; private const double lowValue = -1.0; private const double highValue = 1.0; private const double normalValue = 0.0; public static Alert _alert = Alert.Normal; static TimeSpan GetPollingIntervalFromModbus(string modbusComponentId, List\u003cstring\u003e StreamIds) { int pollingMilliseconds = 5000; string endpoint = $\"http://localhost:5590/api/v1/configuration/{modbusComponentId}/DataSelection\"; $\"http:  localhost:5590 api v1 configuration {modbusComponentId} DataSelection\"; string modbusConfig = _client.GetStringAsync(endpoint).Result; List\u003cModbusField\u003e values = JsonConvert.DeserializeObject\u003cList\u003cModbusField\u003e\u003e(modbusConfig); foreach (var value in values) { foreach (string StreamId in StreamIds) { if (StreamId == value.StreamId \u0026\u0026 value.ScanRate \u003c pollingMilliseconds) { pollingMilliseconds = value.ScanRate; } } } return TimeSpan.FromMilliseconds(pollingMilliseconds); } static bool GetStreamValue(string StreamId) { bool value = false; string lastValueUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/Data/Last\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} Data Last\", StreamId); string lastValueJson = _client.GetStringAsync(lastValueUri).Result; Dictionary\u003cstring, object\u003e values = JsonConvert.DeserializeObject\u003cDictionary\u003cstring, object\u003e\u003e(lastValueJson); object objValue = values[\"Value\"]; if (objValue is Boolean) value = (bool)objValue; return value; } static bool FindOrCreateType(string typeId) { string typeUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/types/{0}\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default types {0}\", typeId); HttpResponseMessage response = _client.GetAsync(typeUri).Result; if (response.IsSuccessStatusCode) return true; string typeJson = @\"{\"\"Id\"\": \"\"\" + typeId + @\"\"\",\"\"Name\"\": \"\"\" + typeId + @\"\"\",\"\"SdsTypeCode\"\": 1,\"\"Properties\"\": [{\"\"Id\"\": \"\"Time\"\",\"\"Name\"\": \"\"Time\"\",\"\"IsKey\"\": true,\"\"SdsType\"\": {\"\"SdsTypeCode\"\": 16}},{\"\"Id\"\": \"\"Measurement\"\",\"\"Name\"\": \"\"Measurement\"\",\"\"SdsType\"\": {\"\"SdsTypeCode\"\": 14}}]}\"; var content = new StringContent(typeJson, Encoding.UTF8, \"application/json\"); \"application json\"); response = _client.PostAsync(typeUri, content).Result; return response.IsSuccessStatusCode; } static bool FindOrCreateStream(string streamId, string typeId) { string streamUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} \", streamId); HttpResponseMessage response = _client.GetAsync(streamUri).Result; if (response.IsSuccessStatusCode) return true; string streamJson = @\"{\"\"Id\"\": \"\"\" + streamId + @\"\"\",\"\"Name\"\": \"\"\" + streamId + @\"\"\",\"\"TypeId\"\": \"\"\" + typeId + @\"\"\"}\"; var content = new StringContent(streamJson, Encoding.UTF8, \"application/json\"); \"application json\"); response = _client.PostAsync(streamUri, content).Result; return response.IsSuccessStatusCode; } static bool WriteStreamValue(string StreamId, double value, DateTime timestamp) { string dataUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/Data\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} Data\", StreamId); string dataJson = @\"[{\"\"Time\"\": \"\"\" + timestamp.ToString(\"o\") + @\"\"\",\"\"Measurement\"\":\" + value.ToString() + \"}]\"; var content = new StringContent(dataJson, Encoding.UTF8, \"application/json\"); \"application json\"); HttpResponseMessage response = _client.PostAsync(dataUri, content).Result; return response.IsSuccessStatusCode; } static Alert CheckStatus(int numberTrue) { if (numberTrue \u003e 3) return Alert.High; if (numberTrue \u003c 1) return Alert.Low; return Alert.Normal; } static bool ReportChange(Alert oldAlert, Alert newAlert) { bool success = true; DateTime now = DateTime.UtcNow; switch (oldAlert) { case Alert.Normal: if (Alert.Low == newAlert) { return WriteStreamValue(ValueRangeLow, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, lowValue, now); } if (Alert.High == newAlert) { return WriteStreamValue(ValueRangeHigh, highValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, highValue, now); } break; case Alert.High: if (Alert.Low == newAlert) { return WriteStreamValue(ValueRangeLow, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, normalValue, now); } if (Alert.Normal == newAlert) { return WriteStreamValue(ValueRangeOut, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, normalValue, now); } break; case Alert.Low: if (Alert.Normal == newAlert) { return WriteStreamValue(ValueRangeOut, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeLow, normalValue, now); } if (Alert.High == newAlert) { return WriteStreamValue(ValueRangeLow, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, highValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, highValue, now); } break; default: break; } return success; } static void Main(string[] args) { TimeSpan pollingInterval = GetPollingIntervalFromModbus(ModbusComponentId, StreamIds); FindOrCreateType(TypeId); FindOrCreateStream(ValueRangeHigh, TypeId); FindOrCreateStream(ValueRangeLow, TypeId); FindOrCreateStream(ValueRangeOut, TypeId); while (true) { int numberTrue = 0; foreach (string StreamId in StreamIds) { bool value = GetStreamValue(StreamId); if (value) numberTrue++; } Alert currentAlert = CheckStatus(numberTrue); if (currentAlert != _alert) { if (ReportChange(_alert, currentAlert)) { _alert = currentAlert; } } System.Threading.Thread.Sleep(pollingInterval); Console.WriteLine(\"ValueRange should be \" + _alert); } } } }"
                                             },
    "V1/OpcUa/SupportedFeaturesOPCUA.html":  {
                                                 "href":  "V1/OpcUa/SupportedFeaturesOPCUA.html",
                                                 "title":  "Supported features",
                                                 "keywords":  "Supported features Data Types The following table lists OPC UA variable types that the OPC UA EDS adapter supports data collection from and types of streams that are going to be created in Edge Data Store. OPC UA data type Stream data type Boolean Boolean Byte Int16 SByte Int16 Int16 Int16 UInt16 UInt16 Int32 Int32 UInt32 UInt32 Int64 Int64 UInt64 UInt64 Float Float32 Double Float64 DateTime DateTime String String Export operation The OPC UA EDS adapter is able to export available OPC UA dynamic variables by browsing the OPC UA hierarchies or sub-hierarchies. Browse can be limited by specifying comma separated collection of nodeIds in data source configuration (RootNodeIds) which are going to be treated as a root(s) from where the adapter starts the browse operation. The adapter triggers export operation after successful connection to the OPC UA server when the data selection file doesn\u0027t exist in configuration directory. The exported data selection JSON file can be copied from the directory or retrieved via REST API call. Data selection file can be also created manually in order to avoid potentially long and expensive browse operation and configured before configuring data source or pushed in one configuration call with data source configuration."
                                             },
    "V1/Modbus/ModbusTCPDataSourceConfiguration.html":  {
                                                            "href":  "V1/Modbus/ModbusTCPDataSourceConfiguration.html",
                                                            "title":  "Modbus TCP data source configuration",
                                                            "keywords":  "Modbus TCP data source configuration To use the Modbus TCP EDS adapter Adapter of Edge Data Store, you must configure it for the Modbus TCP data source from which it will be polling data. Configure Modbus TCP data source Note: You cannot modify Modbus TCP data source configurations manually. You must use the REST endpoints to add or edit the configuration. Complete the following to configure the Modbus TCP data source: Using any text editor, create a file that contains a Modbus TCP data source in JSON form. You can create or copy this file to any directory on a device with Edge Data Store installed. For content structure, see Modbus TCP data source example . For a table of all available parameters, see Parameters for Modbus TCP data source . Save the file as DataSource.config.json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cEDS http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSource/ adapterId\u003e DataSource  . If a Modbus TCP EDS adapter is added during installation, it will have an EDS adapterId of Modbus1, which is used in the following example. Note: During installation, it is possible to add a single Modbus TCP EDS adapter which is named Modbus1. The following example uses this component name. Example using cURL (run this command from the same directory where the file is located): curl -v -d \"@DataSource.config.json\" -H \"Content-Type: application/json\" application json\" -X POST \"http://localhost:5590/api/v1/configuration/Modbus1/DataSource\" \"http:  localhost:5590 api v1 configuration Modbus1 DataSource\" Parameters for Modbus TCP data source The following parameters are available for configuring a Modbus TCP data source. Parameter Required Type Description IpAddress Required string The IP address of the device from which the data is to be collected using the Modbus TCP protocol. Host name is not supported. Port Optional number The TCP port of the target device that listens for and responds to Modbus TCP requests. The value ranges from 0 to 65535. If not configured, the default TCP port is 502 (which is the default port for Modbus TCP protocol). StreamIdPrefix Optional number Parameter applied to all data items collected from the data source. If not configured, the default value is the ID of the Modbus TCP EDS adapter. The custom StreamIdPrefix has the highest priority. ApplyPrefixToStreamId Optional boolean Parameter applied to all data items collected from the data source that have custom stream ID configured. If configured, the adapter will apply the StreamIdPrefix property to all the streams with custom ID configured. The property does not affect any streams with default ID configured ConnectTimeout Optional number Parameter to specify the time (in milliseconds) to wait when Modbus TCP EDS adapter is trying to connect to the data source. The value ranges from 1000 ms to 30000 ms. The default value is 5000 ms. ReconnectInterval Optional number Parameter to specify the time (in milliseconds) to wait before retrying to connect to the data source when the data source is offline. The value ranges from 100 ms to 30000 ms. The default value is 1000 ms. RequestTimeout Optional number Parameter to specify the time (in milliseconds) that Modbus TCP EDS adapter waits for a pending request before marking it as timeout and dropping the request. The default value is 10000 ms. The value must be a positive integer, there is no value range. DelayBetweenRequests Optional number Parameter to specify the minimum time (in milliseconds) between two successive requests sent to the data source. The value ranges from 0 ms to 1000 ms. The default value is 0 ms. MaxResponseDataLength Optional number Parameter to limit the maximum length (in bytes) of data that can be read within one transaction. This feature is provided to support devices that limit the number of bytes that can be returned. If there is no device limitation, the request length should be the maximum length of 250 bytes. The value ranges from 2 to 250. The default value is 250 ms. Modbus TCP data source example The following is an example of valid Modbus TCP data source configuration: { \"IpAddress\": \"117.23.45.110\", \"Port\" : 502, \"ConnectTimeout\" : 10000, \"StreamIdPrefix\" : \"DataSource1\", }"
                                                        },
    "V1/Modbus/ModbusTCPDataSelectionConfiguration.html":  {
                                                               "href":  "V1/Modbus/ModbusTCPDataSelectionConfiguration.html",
                                                               "title":  "Modbus TCP data selection configuration",
                                                               "keywords":  "Modbus TCP data selection configuration Once a data source is configured for a Modbus TCP instance, you must configure which data is to be collected from the Modbus TCP slave device. Configure Modbus TCP data selection Note: You cannot modify Modbus TCP data selection configurations manually. You must use the REST endpoints to add or edit the configuration. Complete the following to configure Modbus TCP data selection: Using any text editor, create a file that contains a Modbus TCP data selection in JSON form. This file can be created or copied to any directory on a device with Edge Data Store installed. For content structure, see Modbus TCP data selection example . For a table of all available parameters, see Parameters for Modbus TCP data selection . Save the file as DataSelection.config.json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cEDS http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSelection/ adapterId\u003e DataSelection  . Example using cURL (run this command from the same directory where the file is located): curl -v -d \"@DataSelection.config.json\" -H \"Content-Type: application/json\" application json\" -X POST \"http://localhost:5590/api/v1/configuration/\u003cEDS \"http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSelection\" adapterId\u003e DataSelection\" Parameters for Modbus TCP data selection The following parameters are available for configuring Modbus TCP data selection. Parameter Required Type Description Id Optional String This field is used to update an existing measurement. The ID automatically updates when there are changes to the measurement and will follow the format of \u003cUnitId \u003e. \u003cRegisterType \u003e. \u003cRegisterOffset \u003e. Selected Optional bool This field is used to select or clear a measurement. To select an item, set to true. To remove an item, leave the field empty or set to false. If not configured, the default value is true. Name Optional string The optional friendly name of the data item collected from the data source. If not configured, the default value will be the stream ID. UnitId Required number Modbus TCP slave device unit ID. This must be a value between 0 and 247, inclusively. RegisterType Required number or string Modbus TCP register type. Supported types are Coil, Discrete, Input16, Input32, Holding16 and Holding32. Input16 and Holding16 are used to read registers that have a size of 16 bits. For registers that have a size of 32 bits, use the Input32 and Holding32 register types. To represent the types, you can type in the register type ID or the exact name: 1 or Coil (Read Coil Status) 2 or Discrete (Read Discrete Input Status) 3 or Holding16 (Read 16-bit Holding Registers) 4 or Holding32 (Read 32-bit Holding Registers) 6 or Input16 (Read 16-bit Input Registers) 7 or Input32 (Read 32-bit Input Registers) RegisterOffset Required number The 0 relative offset to the starting register for this measurement. For example, if your Holding registers start at base register 40001, the offset to this register is 0. For 40002, the offset to this register is 1. DataTypeCode Required number An integer representing the data type that Modbus TCP EDS adapter will read starting at the register specified by the offset. Supported data types are: 1 = Boolean 10 = Int16 20 = UInt16 30 = Int32 31 = Int32ByteSwap 100 = Float32 101 = Float32ByteSwap 110 = Float64 111 = Float64ByteSwap 1001 - 1250 = String 2001 - 2250 = StringByteSwap ScanRate Required number How often this measurement should be read from the device in milliseconds. Acceptable values are from 0 to 86400000. If 0 ms is specified, Modbus TCP EDS adapter will scan for data as fast as possible. BitMap Required string The bitmap is used to extract and reorder bits from a word register. The format of the bitmap is uuvvwwxxyyzz, where uu, vv, ww, yy, and zz each refer to a single bit. A leading zero is required if the referenced bit is less than 10. The low-order bit is 01 and high-order bit is either 16 or 32. Up to 16 bits can be referenced for a 16-bit word (data types 10 and 20) and up to 32 bits can be referenced for a 32-bit word (data type 30 and 31). The bitmap 0307120802 will map the second bit of the original word to the first bit of the new word, the eighth bit to the second bit, the twelfth bit to the third bit, and so on. The high-order bits of the new word are padded with zeros if they are not specified. ConversionFactor Required number This numerical value can be used to scale the raw response received from the Modbus TCP device. If this is specified, regardless of the specified data type, the value will be promoted to a float32 (single) when stored. [Result = (Value /   Conversion Factor)] ConversionOffset Required number This numerical value can be used to apply an offset to the response received from the Modbus TCP device. If this is specified, regardless of the specified data type, the value will be promoted to a float32 (single) when stored. [Result = (Value - Conversion Offset)] StreamID Required string The custom stream ID that will be used to create the streams. If not specified, the Modbus TCP EDS adapter will generate a default stream ID based on the measurement configuration. A properly configured custom stream ID follows these rules: Is not case-sensitive. Can contain spaces. Cannot start with two underscores (\"__\"). Can contain a maximum of 260 characters. Cannot use the following characters: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % \u003c \u003e | Cannot start or end with a period. Cannot contain consecutive periods. Cannot consist of only periods. Each JSON object in the file represents a measurement. You can modify the fields in each object to configure the measurement parameters. To add more measurements, you need to create more JSON objects with properly completed fields. Modbus TCP data selection example [ { \"Selected\": true, \"Name\": \"Measurement1\", \"UnitId\": 0, \"RegisterType\": 1, \"RegisterOffset\": 0, \"DataTypeCode\": 1, \"BitMap\": \"\", \"ConversionFactor\": null, \"ConversionOffset\": null, \"StreamId\": \"SampleStreamID1\", \"ScanRate\": 0 }, { \"Selected\": true, \"Name\": \"Measurement2\", \"UnitId\": 247, \"RegisterType\": 2, \"RegisterOffset\": 65535, \"DataTypeCode\": 10, \"BitMap\": \"\", \"ConversionFactor\": 1, \"ConversionOffset\": 0, \"StreamId\": \"\", \"ScanRate\": 86400000 }, { \"Selected\": true, \"Name\": \"Measurement3\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"StreamId\": \"Sample Stream ID 2\", \"ScanRate\": 1000 }, { \"Selected\": true, \"Name\": \"Measurement4\", \"UnitId\": 1, \"RegisterType\": 4, \"RegisterOffset\": 1, \"DataTypeCode\": 30, \"BitMap\": \"30293231\", \"ConversionFactor\": 1, \"ConversionOffset\": 2, \"StreamId\": \"Sample_Stream_ID_3\", \"ScanRate\": 1000 } ]"
                                                           },
    "V1/Modbus/ModbusTCPAdapterSecurity.html":  {
                                                    "href":  "V1/Modbus/ModbusTCPAdapterSecurity.html",
                                                    "title":  "Modbus TCP EDS adapter security",
                                                    "keywords":  "Modbus TCP EDS adapter security Configure Modbus TCP EDS adapter security Data source configuration Adapter certificate store"
                                                },
    "V1/Modbus/ModbusOverview.html":  {
                                          "href":  "V1/Modbus/ModbusOverview.html",
                                          "title":  "Modbus TCP EDS adapter",
                                          "keywords":  "Modbus TCP EDS adapter Overview Modbus TCP is a commonly available communication protocol used for connecting and transmitting information between industrial electronic devices. The Modbus TCP EDS adapter polls Modbus TCP slave devices, and transfers time series data from the data source devices into Edge Data Store. Polling is based on the measurement configuration provided, and models the register measurements in a Modbus TCP data source. The Modbus TCP EDS adapter communicates with any device conforming to the Modbus TCP/IP TCP IP protocol through a gateway or router. The Modbus TCP slave devices and routers do not need to be on the same subnet as Edge Data Store. You can add a single Modbus TCP EDS adapter during installation. If you want multiple Modbus TCP EDS adapters, see Edge Data Store configuration on how to add a new component to Edge Data Store."
                                      },
    "V1/LinuxWindows/LinuxWindows.html":  {
                                              "href":  "V1/LinuxWindows/LinuxWindows.html",
                                              "title":  "Linux and Windows platform differences",
                                              "keywords":  "Linux and Windows platform differences When developing applications to work with the Edge Data Store, there is no difference between Linux and Windows installations in expected behavior. Edge Data Store installation best practices differ between Linux and Windows, as described in the following sections: File locations Windows Program binaries are placed in the C:\\Program Files\\OSIsoft\\EdgeDataStore directory by default. For information about changing this location, see the installation documentation. Configuration, log, and data files are placed under C:\\ProgramData\\OSIsoft\\EdgeDataStore This is not configurable. This folder structure will not be automatically removed during uninstallation. For information about clearing these files, see the installation documentation. Key material for configuration files\u0027 encrypted secrets is stored using the Windows DPAPI in a secure Windows store. This is not configurable. Linux Program binaries are placed in the /opt/EdgeDataStore  opt EdgeDataStore directory. Configuration, log, and data files are placed under /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore . This folder structure will not be automatically removed during uninstallation. For information about clearing these files, see the installation documentation. Key material for configuration files\u0027 encrypted secrets are stored using limited access files under /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore . You cannot configure file locations for Linux. When the Debian installer is used, Edge Data Store is installed using the service identity osisoft.edgedatastore.service. If you need to restart the service from the Linux command line, you can use the following command: sudo systemctl restart osisoft.edgedatastore.service File descriptors (handles) Linux operating systems impose a limit on the number of file descriptors used in a process. The number of open file descriptors is directly related to the number of streams used in EDS (e.g. data ingress). Overall, every stream utilizes two file descriptors. EDS will no longer function properly when it reaches the limit of available file descriptors. To prevent this, it is necessary to either limit the number of streams used in EDS or increase the maximum allowed file descriptors per process. The following figures were identified on a Raspberry Pi 3 Model B+ using a Raspbian operating system. An installation of EDS with no user-defined streams had an average of 424 open file descriptors. The same installation with 250 streams had an average of 932 open file descriptors. The file descriptor limit per process for the operating system used was 1024. Note Figures will differ on other Linux operating systems and devices, and may differ slightly from execution to execution, so it\u0027s important to understand the system you are using. Windows has an object called a handle that is used in much the same way that Linux uses file descriptors. However, Windows does not have the same sort of limitation just described."
                                          },
    "V1/Configuration/Schemas/System_schema.html":  {
                                                        "href":  "V1/Configuration/Schemas/System_schema.html",
                                                        "title":  "SystemConfiguration schema",
                                                        "keywords":  "SystemConfiguration schema \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"Components\": [{ \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ], \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 } } Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_Logging_schema.json Properties Property Type Required Nullable Defined by Logging SystemLoggingConfiguration Optional Yes EdgeLoggerConfiguration Components [SystemComponentsConfiguration] Optional Yes ComponentsConfiguration HealthEndpoints [SystemHealthEndpointsConfiguration] Optional Yes HealthEndpointsConfiguration Port SystemPortConfiguration Optional Yes PortConfiguration Logging is optional type: SystemLoggingConfiguration Components is optional type: [SystemComponentsConfiguration] HealthEndpoints is optional type: [SystemHealthEndpointsConfiguration] Port is optional type: SystemPortConfiguration"
                                                    },
    "V1/Configuration/Schemas/System_Port_schema.html":  {
                                                             "href":  "V1/Configuration/Schemas/System_Port_schema.html",
                                                             "title":  "Sample system port configuration file",
                                                             "keywords":  "Sample system port configuration file { \"Port\": 5590 } System port schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden System_Port_schema.json PortConfiguration properties Property Type Required Nullable Defined by Port integer Optional No PortConfiguration (this schema) Port Port is optional type: integer defined in this schema Port type integer minimum value: 1024 maximum value: 65535 All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Port integer Optional Port Port is optional type: integer Port type integer minimum value: 1024 maximum value: 65535"
                                                         },
    "V1/Configuration/Schemas/System_Logging_schema.html":  {
                                                                "href":  "V1/Configuration/Schemas/System_Logging_schema.html",
                                                                "title":  "System logging configuration schema",
                                                                "keywords":  "System logging configuration schema The System logging configuration schema specifies how to formally describe the logging parameters for the System. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden System_Logging_schema.json EdgeLoggerConfiguration properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) LogFileCountLimit LogFileCountLimit is optional type: integer defined in this schema LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer defined in this schema LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference defined in this schema LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional LogFileCountLimit LogFileCountLimit is optional type: integer LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel"
                                                            },
    "V1/Configuration/Schemas/System_HealthEndpoints_schema.html":  {
                                                                        "href":  "V1/Configuration/Schemas/System_HealthEndpoints_schema.html",
                                                                        "title":  "OMF health endpoint configuration schema",
                                                                        "keywords":  "OMF health endpoint configuration schema The OMF health endpoint configuration schema specifies how to formally describe the OMF health endpoint parameters. [{ \"endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"buffering\": 0, \"maxBufferSizeMB\": 0 }, { \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"buffering\": 0, \"maxBufferSizeMB\": 0 } ] Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden System_HealthEndpoints_schema.json OmfHealthEndpointConfiguration properties Property Type Required Nullable Defined by Buffering reference Optional No OmfHealthEndpointConfiguration (this schema) ClientId string Optional Yes OmfHealthEndpointConfiguration (this schema) ClientSecret string Optional Yes OmfHealthEndpointConfiguration (this schema) Endpoint string Optional Yes OmfHealthEndpointConfiguration (this schema) Id string Optional Yes OmfHealthEndpointConfiguration (this schema) MaxBufferSizeMB integer Optional No OmfHealthEndpointConfiguration (this schema) Password string Optional Yes OmfHealthEndpointConfiguration (this schema) UserName string Optional Yes OmfHealthEndpointConfiguration (this schema) ValidateEndpointCertificate boolean Optional No OmfHealthEndpointConfiguration (this schema) Buffering Buffering is optional type: reference defined in this schema Buffering type ??? #/definitions/BufferType # definitions BufferType ClientId ClientId is optional type: string defined in this schema ClientId type string , nullable ClientSecret ClientSecret is optional type: string defined in this schema ClientSecret type string , nullable Endpoint Endpoint is optional type: string defined in this schema Endpoint type string , nullable Id Id is optional type: string defined in this schema Id type string , nullable MaxBufferSizeMB MaxBufferSizeMB is optional type: integer defined in this schema MaxBufferSizeMB type integer Password Password is optional type: string defined in this schema Password type string , nullable UserName UserName is optional type: string defined in this schema UserName type string , nullable ValidateEndpointCertificate ValidateEndpointCertificate is optional type: boolean defined in this schema ValidateEndpointCertificate type boolean All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Buffering Optional ClientId string Optional ClientSecret string Optional Endpoint string Optional Id string Optional MaxBufferSizeMB integer Optional Password string Optional UserName string Optional ValidateEndpointCertificate boolean Optional Buffering Buffering is optional type: reference Buffering type ??? #/definitions/BufferType # definitions BufferType ClientId ClientId is optional type: string ClientId type string , nullable ClientSecret ClientSecret is optional type: string ClientSecret type string , nullable Endpoint Endpoint is optional type: string Endpoint type string , nullable Id Id is optional type: string Id type string , nullable MaxBufferSizeMB MaxBufferSizeMB is optional type: integer MaxBufferSizeMB type integer Password Password is optional type: string Password type string , nullable UserName UserName is optional type: string UserName type string , nullable ValidateEndpointCertificate ValidateEndpointCertificate is optional type: boolean ValidateEndpointCertificate type boolean"
                                                                    },
    "V1/Configuration/Schemas/OpcUa_DataSelection_schema.html":  {
                                                                     "href":  "V1/Configuration/Schemas/OpcUa_DataSelection_schema.html",
                                                                     "title":  "Sample Opc UA data selection configuration",
                                                                     "keywords":  "Sample Opc UA data selection configuration The OPC UA data selection configuration schema specifies how to formally describe the data selection parameters for OPC UA. [{ \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"StreamId\": null } ] OPC UA data collection item schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden OpcUa_DataSelection_schema.json DataCollectionItem properties Property Type Required Nullable Defined by Name string Optional Yes DataCollectionItem (this schema) NodeId string Optional Yes DataCollectionItem (this schema) Selected boolean Optional No DataCollectionItem (this schema) StreamId string Optional Yes DataCollectionItem (this schema) Name Name is optional type: string defined in this schema Name type string , nullable NodeId NodeId is optional type: string defined in this schema NodeId type string , nullable Selected Selected is optional type: boolean defined in this schema Selected type boolean StreamId StreamId is optional type: string defined in this schema StreamId type string , nullable All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Name string Optional NodeId string Optional Selected boolean Optional StreamId string Optional Name Name is optional type: string Name type string , nullable NodeId NodeId is optional type: string NodeId type string , nullable Selected Selected is optional type: boolean Selected type boolean StreamId StreamId is optional type: string StreamId type string , nullable"
                                                                 },
    "V1/Configuration/Schemas/Modbus_schema.html":  {
                                                        "href":  "V1/Configuration/Schemas/Modbus_schema.html",
                                                        "title":  "ModbusConfiguration Schema",
                                                        "keywords":  "ModbusConfiguration Schema Abstract Extensible Status Identifiable Custom Properties Additional Properties Defined In Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_Logging_schema.json Properties Property Type Required Nullable Defined by Logging ModbusLoggingConfiguration Optional Yes EdgeLoggerConfiguration DataSource DataSourceConfiguration Optional Yes ComponentsConfiguration DataSelection [ModbusDataSelectionConfiguration] Optional Yes DataSelectionConfiguration Logging is optional type: SystemLoggingConfiguration DataSource is optional type: DataSourceConfiguration DataSelection is optional type: [ModbusDataSelectionConfiguration]"
                                                    },
    "V1/Configuration/Schemas/Modbus_Logging_schema.html":  {
                                                                "href":  "V1/Configuration/Schemas/Modbus_Logging_schema.html",
                                                                "title":  "ModbusLoggerConfiguration Schema",
                                                                "keywords":  "ModbusLoggerConfiguration Schema The Modbus TCP logger configuration schema specifies how to formally describe the Modbus TCP logging parameters. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden ModbusLoggerConfiguration Properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) LogFileCountLimit LogFileCountLimit is optional type: integer defined in this schema LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer defined in this schema LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference defined in this schema LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional LogFileCountLimit LogFileCountLimit is optional type: integer LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel"
                                                            },
    "V1/Configuration/Schemas/Modbus_DataSource_schema.html":  {
                                                                   "href":  "V1/Configuration/Schemas/Modbus_DataSource_schema.html",
                                                                   "title":  "Sample Modbus TCP data source configuration",
                                                                   "keywords":  "Sample Modbus TCP data source configuration { \"IpAddress\": \"\u003cModbus IP Address\u003e\", \"Port\": \u003cPort - usually 502\u003e, \"ConnectTimeout\": 15000, \"ReconnectInterval\": 5000, \"RequestTimeout\": 9000, \"DelayBetweenRequests\": 0, \"MaxResponseDataLength\": 250 } Modbus TCP data source configuration schema The Modbus TCP data source configuration schema specifies how to formally describe the Modbus TCP data source parameters. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_DataSource_schema.json DataSourceConfiguration properties Property Type Required Nullable Defined by ConnectTimeout integer Optional No DataSourceConfiguration (this schema) DelayBetweenRequests integer Optional No DataSourceConfiguration (this schema) IpAddress string Optional Yes DataSourceConfiguration (this schema) MaxResponseDataLength integer Optional No DataSourceConfiguration (this schema) Port integer Optional No DataSourceConfiguration (this schema) ReconnectInterval integer Optional No DataSourceConfiguration (this schema) RequestTimeout integer Optional No DataSourceConfiguration (this schema) StreamIdPrefix string Optional Yes DataSourceConfiguration (this schema) ConnectTimeout ConnectTimeout is optional type: integer defined in this schema ConnectTimeout type integer DelayBetweenRequests DelayBetweenRequests is optional type: integer defined in this schema DelayBetweenRequests type integer IpAddress IpAddress is optional type: string defined in this schema IpAddress type string , nullable MaxResponseDataLength MaxResponseDataLength is optional type: integer defined in this schema MaxResponseDataLength type integer Port Port is optional type: integer defined in this schema Port type integer ReconnectInterval ReconnectInterval is optional type: integer defined in this schema ReconnectInterval type integer RequestTimeout RequestTimeout is optional type: integer defined in this schema RequestTimeout type integer StreamIdPrefix StreamIdPrefix is optional type: string defined in this schema StreamIdPrefix type string , nullable All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required ConnectTimeout integer Optional DelayBetweenRequests integer Optional IpAddress string Optional MaxResponseDataLength integer Optional Port integer Optional ReconnectInterval integer Optional RequestTimeout integer Optional StreamIdPrefix string Optional ConnectTimeout ConnectTimeout is optional type: integer ConnectTimeout type integer DelayBetweenRequests DelayBetweenRequests is optional type: integer DelayBetweenRequests type integer IpAddress IpAddress is optional type: string IpAddress type string , nullable MaxResponseDataLength MaxResponseDataLength is optional type: integer MaxResponseDataLength type integer Port Port is optional type: integer Port type integer ReconnectInterval ReconnectInterval is optional type: integer ReconnectInterval type integer RequestTimeout RequestTimeout is optional type: integer RequestTimeout type integer StreamIdPrefix StreamIdPrefix is optional type: string StreamIdPrefix type string , nullable"
                                                               },
    "V1/Administration/Administration.html":  {
                                                  "href":  "V1/Administration/Administration.html",
                                                  "title":  "Edge Data Store administration",
                                                  "keywords":  "Edge Data Store administration Edge Data Store provides a number of administration level functions. Reset Edge Data Store Edge Data Store provides a method of performing a complete reset of the product. When you perform a reset, all event data and Edge Data Store configuration is deleted, and the product is restarted. Note: All configuration and stored data will be lost as a result of performing this action. To reset Edge Data Store, use any REST client and make a request using the following: Method: POST Endpoint: http://localhost:5590/api/v1/administration/System/Reset http:  localhost:5590 api v1 administration System Reset Header: Content-Type application/json application json Example using cURL: curl -v -d \"\" -X POST http://localhost:5590/api/v1/Administration/System/Reset http:  localhost:5590 api v1 Administration System Reset An HTTP status 204 message indicates success. Reset the Edge Storage component Edge Data Store provides a method by which you can delete and reset all event and configuration data related to the Edge Data Store component, after which the product will be restarted. To reset the Storage component, use any REST client and make a request using the following: Method: POST Endpoint: http://localhost:5590/api/v1/administration/Storage/Reset http:  localhost:5590 api v1 administration Storage Reset Header: Content-Type application/json application json Example using cURL: curl -v -d \"\" -X POST http://localhost:5590/api/v1/Administration/Storage/Reset http:  localhost:5590 api v1 Administration Storage Reset An HTTP status 204 message indicates success. Stop and start an Edge Data Store adapter Edge Data Store provides the ability to stop and start EDS adapters. By default, when Edge Data Store starts, all currently configured EDS adapters are started and remain running until the product shuts down. Stop an EDS adapter To stop an individual EDS adapter, use any REST client and make a request using the following: Method: POST Endpoint: http://localhost:5590/api/v1/administration/EDS http:  localhost:5590 api v1 administration EDS adapterId/Stop adapterId Stop Header: Content-Type application/json application json Example using cURL: curl -v -d \"\" -X POST http://localhost:5590/api/v1/Administration/EDS http:  localhost:5590 api v1 Administration EDS adapterId/Stop adapterId Stop Note: Replace EDS adapterId with the id of the EDS adapter you want to stop. An HTTP status 204 message indicates success. Start an EDS adapter To start an individual EDS adapter, use any REST client and make a request using the following: Method: POST Endpoint: http://localhost:5590/api/v1/administration/EDS http:  localhost:5590 api v1 administration EDS adapterId/Start adapterId Start Header: Content-Type application/json application json Example using cURL: curl -v -d \"\" -X POST http://localhost:5590/api/v1/Administration/EDS http:  localhost:5590 api v1 Administration EDS adapterId/Start adapterId Start Note: Replace EDS adapterId with the id of the EDS adapter you want to start. An HTTP status 204 message indicates success."
                                              },
    "docfx.console.2.43.2/content/index.html":  {
                                                    "href":  "docfx.console.2.43.2/content/index.html",
                                                    "title":  "This is the HOMEPAGE.",
                                                    "keywords":  "This is the HOMEPAGE . Refer to Markdown for how to write markdown files. Quick Start Notes: Add images to images folder if the file is referencing an image."
                                                },
    "V1/Administration/LoggingConfiguration.html":  {
                                                        "href":  "V1/Administration/LoggingConfiguration.html",
                                                        "title":  "Message logging configuration",
                                                        "keywords":  "Message logging configuration Edge Data Store writes daily log messages to flat text files in the following locations: ??? Windows: %ProgramData%/OSIsoft/EdgeDataStore/Logs %ProgramData% OSIsoft EdgeDataStore Logs ??? Linux: /usr/share/OSIsoft/EdgeDataStore/Logs  usr share OSIsoft EdgeDataStore Logs Each message in the log displays the message severity level, timestamp, and the message itself. Default logging configuration and schema By default, logging captures Information, Warning, Error, and Critical messages in the message logs. The default logging configuration for a component on install is the following { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } The schema file specifies how to formally describe the configuration parameters for message logging. It is located in: ??? Windows: %ProgramFiles%/OSIsoft/EdgeDataStore/Schema %ProgramFiles% OSIsoft EdgeDataStore Schema ??? Linux: /opt/EdgeDataStore/Schema  opt EdgeDataStore Schema Log levels Changing the logLevel leads to captures of logs in severity including and above the specified log level. The log levels in their increasing order of severity are as follows: Trace, Debug, Information, Warning, Error, Critical. Table: General guidelines for setting the log level. Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values. Generally, these messages shouldn???t be enabled in production environment. Debug Logs that can be used to troubleshoot data flow issues by logging metrics and detailed flow related information. Information Logs that track the general flow of the application. Any non-repetitive general information (like version information relating to the software at startup, what external services are being used, data source connection string, number of measurements, egress URL, change of state ???Starting???, ???Stopping??? or configuration) can be useful for diagnosing potential application errors. Warning Logs that highlight an abnormal or unexpected event in the application flow, but do not otherwise cause the application execution to stop. This can be used to react on unconfigured data source state, message that communication with backup failover instance has been lost, use of insecure communication channel, or any other event that could require attention, but isn???t directly impacting the flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure. This can be used to react on invalid configuration, unavailable external endpoint, internal flow error, etc. Critical Logs that describe an unrecoverable application or system crash, or a catastrophic failure that requires immediate attention. This can be used to react on application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (Data Protection key file for example). Log file count limit The logFileCountLimit controls the number of days the log files are persisted before they are deleted for creation of new log files. Changing logging configuration To change the logging configuration you can Save the new configuration information in a JSON file format. Run the following script (or make an equivalent REST API call): For example, Component_Logging.json: { \"logLevel\": \"Warning\", \"logFileSizeLimitBytes\": 16777216, \"logFileCountLimit\": 30 } curl -i -d \"@Component_Logging.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging \u003cComponentId\u003e is the ComponentId of the adapter or Storage. On successful execution, the log level change takes effect immediately during runtime. The other configurations (log file size and file count) get updated after Edge Data Store is restarted. Note: If you do not specify all the parameters while changing the configuration, the specified parameters will be updated while the unspecified parameters will revert to the default schema values."
                                                    },
    "V1/Configuration/Schemas/Modbus_DataSelection_schema.html":  {
                                                                      "href":  "V1/Configuration/Schemas/Modbus_DataSelection_schema.html",
                                                                      "title":  "Sample Modbus TCP data selection configuration",
                                                                      "keywords":  "Sample Modbus TCP data selection configuration [{ \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 2, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 3, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 4, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 5, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 } ] Modbus TCP data selection configuration schema The Modbus TCP data selection configuration schema specifies how to formally describe the data selection parameters for Modbus TCP. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_DataSelection_schema.json Modbus TCP data selection configuration properties Property Type Required Nullable Defined by BitMap string Optional Yes DataSelectionConfiguration (this schema) ConversionFactor number Optional Yes DataSelectionConfiguration (this schema) ConversionOffset number Optional Yes DataSelectionConfiguration (this schema) DataTypeCode integer Optional No DataSelectionConfiguration (this schema) Name string Optional Yes DataSelectionConfiguration (this schema) RegisterOffset integer Optional No DataSelectionConfiguration (this schema) RegisterType reference Optional No DataSelectionConfiguration (this schema) ScanRate integer Optional No DataSelectionConfiguration (this schema) Selected boolean Optional No DataSelectionConfiguration (this schema) StreamId string Optional Yes DataSelectionConfiguration (this schema) UnitId integer Optional No DataSelectionConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required BitMap string Optional ConversionFactor number Optional ConversionOffset number Optional DataTypeCode integer Optional Name string Optional RegisterOffset integer Optional RegisterType Optional ScanRate integer Optional Selected boolean Optional StreamId string Optional UnitId integer Optional"
                                                                  },
    "V1/Configuration/Schemas/EdgeSystem_schema.html":  {
                                                            "href":  "V1/Configuration/Schemas/EdgeSystem_schema.html",
                                                            "title":  "Edge Data Store configuration schema",
                                                            "keywords":  "Edge Data Store configuration schema The Edge Data Store configuration schema specifies how to formally describe the system parameters (logging, components, health endpoints, port). \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"Components\": [{ \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ], \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 } } Properties Property Type Required Nullable Defined by Storage StorageConfiguration Optional Yes StorageConfiguration System SystemConfiguration Optional Yes SystemConfiguration {ComponentName} {ComponentConfiguration} Optional Yes {ComponentConfiguration} Storage is optional type: StorageConfiguration System is optional type: SystemConfiguration {ComponentName} EdgeDataStoreConfiguration Schema Edge Data Store Configuration Schema Properties Storage System {ComponentName}"
                                                        },
    "V1/Configuration/Schemas/ConfigurationSchemaList.html":  {
                                                                  "href":  "V1/Configuration/Schemas/ConfigurationSchemaList.html",
                                                                  "title":  "Configuration schemas",
                                                                  "keywords":  "Configuration schemas Edge Data Store is configured through a series of JSON files. The schemas for these files are provided in the installation directory and are documented following. Edge Data Store configuration schemas Use the following schemas to configure Edge Data Store: EdgeLoggerConfiguration PortConfiguration OmfHealthEndpointConfiguration EdgeDataStoreConfig EDS adapters configuration schemas Use the following schemas to configure EDS adapters: OPC UA DataSourceConfiguration DataCollectionItem EdgeLoggerConfiguration Modbus TCP DataSourceConfiguration DataSelectionConfiguration EdgeLoggerConfiguration Storage configuration schemas Use the following schemas to configure the Storage component: EdgeLoggerConfiguration StorageRuntimeConfiguration OEMConfiguration Periodic Egress Endpoints"
                                                              },
    "V1/Configuration/System.html":  {
                                         "href":  "V1/Configuration/System.html",
                                         "title":  "Edge Data Store system component",
                                         "keywords":  "Edge Data Store system component The EDS system component hosts all the other components running in Edge Data Store. It is how the port to access EDS is configured, and where adding and removing components is configured. You can configure the following facets in the system component: Logging Port Components Health Endpoints"
                                     },
    "V1/Configuration/Storage.html":  {
                                          "href":  "V1/Configuration/Storage.html",
                                          "title":  "Edge Data Store storage component",
                                          "keywords":  "Edge Data Store storage component Storage Configuration Overview The EDS storage component has three configurable facets: Storage Runtime Storage Periodic Egress Endpoints Logging"
                                      },
    "V1/Configuration/EDSOPCUAAdapter.html":  {
                                                  "href":  "V1/Configuration/EDSOPCUAAdapter.html",
                                                  "title":  "Edge Data Store OPC UA adapter",
                                                  "keywords":  "Edge Data Store OPC UA adapter EDS OPC UA adapter has three configurable facets: Data source Data selection Logging"
                                              },
    "V1/Configuration/EDSModbusTCPAdapter.html":  {
                                                      "href":  "V1/Configuration/EDSModbusTCPAdapter.html",
                                                      "title":  "Edge Data Store Modbus TCP adapter",
                                                      "keywords":  "Edge Data Store Modbus TCP adapter EDS Modbus TCP adapter has three configurable facets: Data source Data selection Logging"
                                                  },
    "V1/Configuration/EdgeSystemConfiguration.html":  {
                                                          "href":  "V1/Configuration/EdgeSystemConfiguration.html",
                                                          "title":  "Edge Data Store configuration",
                                                          "keywords":  "Edge Data Store configuration Edge Data Store uses JSON configuration files in a protected directory on Windows and Linux to store configuration that is read on startup. While the files are accessible to view, OSIsoft recommends that you use REST or the edgecmd command line tool for any changes you make to the files. As part of making Edge Data Store as secure as possible, any passwords or secrets that you configure are stored in encrypted form (with cryptographic key material stored separately in a secure location.) If you edit the files directly, the system may not work as expected. Note: You can edit any single component or facet of the system using REST, but also configure the system as a whole with a single REST call. Edge Data Store configuration Edge Data Store hosts other components. While the initial release of the Edge Data Store includes Modbus TCP, OPC UA, and Storage components, they are only active if you configure the system to use them. The system itself has a relatively small configuration surface area - the list of components and the HTTP Port used for REST calls. Configure Edge Data Store port System_Port.json specifies the port on which the System is listening for REST API calls. The same port is used for configuration and for writing data to OMF and SDS. The default configuration port is 5590. The default System_Port.json file installed is: { \"Port\": 5590 } Allowable ports are in the range of 1024-65535. Before you change the default, ensure that no other service or application on the computer running the EdgeDataStore is using that port - only one application or service can use a port. If you change the port number through the REST API, you must restart Edge Data Store. Save the JSON containing the new port number in the JSON format above to a file named EdgePort.json and run the following script: curl -i -d \"@EdgePort.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/system/port http:  localhost:5590 api v1 configuration system port After the REST command completes, restart Edge Data Store for the change to take effect. Configure Edge Data Store components The default System_Components.json file for the System component is the following. The Storage component is required for this initial release for Edge Data Store to run. With later releases of Edge Data Store, the storage component may not be required. [ { \"ComponentId\": \"Storage\", \"ComponentType\": \"EDS.Component\" } ] You can add additional Modbus TCP and OPC UA components if you want, but only a single Storage component is supported. To add a new component, in this example a Modbus TCP EDS adapter, create the following JSON. Note: A unique ComponentId is necessary for each component in the system. This example uses the ComponentId Modbus1 since it is the first Modbus TCP EDS adapter: { \"ComponentId\": \"Modbus1\", \"ComponentType\": \"Modbus\" } Save the JSON in a file named AddComponent.json . From the same directory where the file exists, run the following curl script: curl -i -d \"@AddComponent.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components After the curl command completes successfully, you can configure or use the new component. Entire Edge Data Store configuration Configure minimum Edge Data Store The following JSON file represents minimal configuration of an Edge Data Store. There are no Modbus TCP or OPC UA components, and the Storage component configurations are set to the default. If you configure a system with this JSON file, any existing Modbus TCP or OPC UA components will be disabled and removed. No storage data will be deleted or modified, and OMF and SDS data access will not be impacted. {{ \"Storage\": { \"PeriodicEgressEndpoints\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableTransactionLog\": true }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ] } } Save or copy the JSON in a file named EdgeMinimumConfiguration.json in any directory on a device with Edge Data Store installed. When you run the following curl command from the directory where the file exists, this will be set as the configuration of a running Edge Data Store (run the command from the directory where the file is located): curl -i -d \"@EdgeMinimumConfiguration.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration The configuration takes effect immediately after the command completes. The above example results in a minimal configuration of Edge Data Store. It only supports OMF and SDS operations using REST. No egress is configured, so no data will be forwarded to either OCS or PI Web API . Configure maximum Edge Data Store The following JSON file represents maximal configuration of an Edge Data Store. There are Modbus TCP and OPC UA components, and egress is configured to send to both PI Web API and OCS from both the default (operational data) and diagnostics (diagnostic data) namespace. { \"Modbus1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"IpAddress\": \"\u003cModbus IP address\u003e\", \"Port\": 502, \"ConnectTimeout\": 15000, \"ReconnectInterval\": 5000, \"RequestTimeout\": 9000, \"DelayBetweenRequests\": 0, \"MaxResponseDataLength\": 250 }, \"DataSelection\": [{ \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 2, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 3, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 4, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 5, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 } ] }, \"Storage\": { \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\" }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"PeriodicEgressEndpoints\": [{ \"Id\": \"OCS\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"\u003cOCS OMF URL for your tenant and namespace\u003e\", \"ClientId\": \"\u003cOCS ClientId\u003e\", \"ClientSecret\": \"\u003cOCS ClientSecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null }, { \"Id\": \"PWA\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"https://\u003cyour \"https:  \u003cyour PI Web API server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null }, { \"Id\": \"OCSDiag\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"\u003cOCS OMF URL for your tenant and namespace\u003e\", \"ClientId\": \"\u003cOCS ClientId\u003e\", \"ClientSecret\": \"\u003cOCS ClientSecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null }, { \"Id\": \"PWADiag\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"https://\u003cyour \"https:  \u003cyour PI Web API server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null } ] }, \"OpcUa1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"EndpointUrl\": \"opc.tcp://\u003cOPC \"opc.tcp:  \u003cOPC UA server IP and port\u003e/OSIsoftTestServer\", port\u003e OSIsoftTestServer\", \"UseSecureConnection\": false, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": \"OpcUa\" }, \"DataSelection\": [{ \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Power\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_001.Power\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Efficiency\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_001.Efficiency\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Flowrate\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_001.Flowrate\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Power\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_002.Power\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Efficiency\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_002.Efficiency\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Flowrate\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_002.Flowrate\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Level\", \"NodeId\": \"ns=2;s=Line1.Tank1.Level\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Mass\", \"NodeId\": \"ns=2;s=Line1.Tank1.Mass\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Volume\", \"NodeId\": \"ns=2;s=Line1.Tank1.Volume\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Level\", \"NodeId\": \"ns=2;s=Line1.Tank2.Level\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Mass\", \"NodeId\": \"ns=2;s=Line1.Tank2.Mass\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Volume\", \"NodeId\": \"ns=2;s=Line1.Tank2.Volume\", \"StreamId\": null } ] }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"Components\": [{ \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ], \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 } } } Fill in any credentials or IP addresses with appropriate values for your environment. Save the edited version of the previous JSON in a file named EdgeMaximumConfiguration.json in any directory. If you run the following curl command, this will be set as the configuration of a running Edge Data Store (You should run the command from the same directory where the file is located): curl -i -d \"@EdgeMaximumConfiguration.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration The configuration takes effect immediately after the command completes. Full JSON definition of configuration parameters: Edge Data Store Configuration"
                                                      },
    "V1/CommandLine/CommandLine.html":  {
                                            "href":  "V1/CommandLine/CommandLine.html",
                                            "title":  "Command line configuration of Edge Data Store",
                                            "keywords":  "Command line configuration of Edge Data Store You can configure and administer Edge Data Store on Linux and Windows by using the edgecmd command line utility. This document will be referred to as edgecmd, even though it is named edgecmd.exe on Windows and edgecmd on Linux. Location on Windows: C:\\Program Files\\OSIsoft\\EdgeDataStore\\edgecmd.exe Note: Specify the full path when you use it on Windows. Location on Linux: /opt/OSIsoft/EdgeDataStore/edgecmd  opt OSIsoft EdgeDataStore edgecmd Note: You can access it without using the full path on Linux. Most options that you can configure using REST, you can also configure using the edgecmd utility and command line arguments. Generally, the configuration and administrative REST interfaces are exposed through the command line. Access to reading and writing data to the Edge Data Store Storage Component, OMF Ingress, and SDS Read/Write Read Write capabilities are only available through using the REST API. Example: View system configuration using edgecmd: edgecmd Configuration System { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ] } Get help The Edgecmd application provides a \u0027Help\u0027 utility. For general instructions on how to use the Edgecmd application, type: edgecmd Help You can also use the utility to get help for any registered component in Edge Data Store. If you add a specific component ID to the end of the previous command, you receive help output for every configuration facet that the component supports, along with examples of commands that you can run to configure the component. Example: View help for the \u0027System\u0027 component: edgecmd Help System --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027Logging\u0027 --------------------------------------------------------------------------------------------------------- LogLevel [Required] Desired log level settings. Options: Verbose, Information, Warning, Error, Fatal. LogFileSizeLimitBytes [Required] Maximum size in bytes of log files that the service will create for this component. Must be no less than 1000. LogFileCountLimit [Required] Maximum number of log files that the service will create for this component. Must be a positive integer. Example: ./edgecmd . edgecmd Configuration System Logging LogLevel=Warning Example: ./edgecmd . edgecmd Configuration System Logging LogFileSizeLimitBytes=32768 Example: ./edgecmd . edgecmd Configuration System Logging LogFileCountLimit=5 --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027HealthEndpoints\u0027 --------------------------------------------------------------------------------------------------------- Id [Optional] Id of existing configuration to be edited of removed. Endpoint [Required] URL of OMF destination UserName [Required group 1] User name used for authentication to PI Web API OMF endpoint. Password [Required group 1] Password used for authentication to PI Web API OMF endpoint. ClientId [Required group 2] Client ID used for authentication to OSIsoft Cloud Services. ClientSecret [Required group 2] Client Secret used for authentication to OSIsoft Cloud Services. Buffering [Optional] Set the buffering type for messages to this endpoint. Options are \u0027memory\u0027, \u0027disk\u0027 or \u0027none\u0027. Defaults to \u0027none\u0027. MaxBufferSizeMB [Optional] If an integer \u003e0, this is the limit on the maximum megabytes of data to buffer for messages to this endpoint. Useful for limiting memory or disk usage growth in the event of disconnection to the endpoint. If the buffer is full, old messages will be discarded for new messages. Defaults to 0. ValidateEndpointCertificate [Optional] If true, endpoint certificate will be validated (recommended). If false, any endpoint certificate will be accepted. OSIsoft strongly recommends using disabled endpoint certificate validation for testing purposes only. Note: Only one Required group must be specified. Group 1 for PI Web API or Group 2 for OCS. Example: ./edgecmd . edgecmd Configuration System HealthEndpoints Endpoint=endpointURL UserName=UserName Password=Password --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027Port\u0027 --------------------------------------------------------------------------------------------------------- Port [Required] The tcp port to bind this application host to (Range [1024,65535]) Example: ./edgecmd . edgecmd Configuration System Port Port=5590 --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027Components\u0027 --------------------------------------------------------------------------------------------------------- ComponentId [Required] ID of the hosted component. ComponentType [Required] Type of the hosted component. Example: ./edgecmd . edgecmd Configuration System Components ComponentId=Modus1 ComponentType=Modbus For help regarding a specific facet within a component, add the facet name after the component ID. Example: Help for the \u0027Port\u0027 facet within the \u0027System\u0027 component: edgecmd Help System Port --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027Port\u0027 --------------------------------------------------------------------------------------------------------- Port [Required] The tcp port to bind this application host to (Range [1024,65535]) Example: ./edgecmd . edgecmd Configuration System Port Port=5590 Edge Data Store components Add components With the following command, you can view which components are currently configured on Edge Data Store: edgecmd Configuration System Components To register a new component, use the following command: edgecmd Configuration System Components componentId=\u003ccomponentId\u003e componentType=\u003ccomponentType\u003e Valid component types are \"Modbus\" and \"OpcUa\". If you are trying to register a Modbus EDS adapter, use \"Modbus\" and if you are trying to register an OPC UA adapter, use \"OpcUa\". Example: Modbus adapter component registration: edgecmd Configuration System Components componentId=Modbus1 componentType=Modbus Configure components The EDS Modbus adapter and OPC UA adapter each have three configurable facets: data source, data selection, and logging. You can configure these with edgecmd by specifying a component ID and facet name. Example: Configuration of the data source facet of a Modbus adapter: edgecmd Configuration Modbus1 DataSource IpAddress=117.23.45.110 port=502 ConnectTimeout=15000 StreamIdPrefix=\"DataSource1\" For detailed information on how to configure each adapter, see the Modbus and OPC UA schemas. Delete components You can delete components from the Edge Data Store by using the following command: edgecmd Configuration System Components id=\u003ccomponentId\u003e delete Note: You can\u0027t delete the \"Storage\" component because it is required for Edge Data Store to operate. Retrieve existing Edge Data Store configurations You can use the edgecmd utility to view the configuration for each part of Edge Data Store. To view the entire configuration for every Edge Data Store component, run the following command: edgecmd Configuration To retrieve component specific configuration: edgecmd Configuration componentId To retrieve facet specific configuration of an Edge Data Store component: edgecmd Configuration componentId facetName For facets that contain multiple entries, you can retrieve the configuration for a specific entry by its Id: edgecmd Configuration componentId facetName id=IndexToRetrieve Examples View the configuration of the \u0027System\u0027 component edgecmd Configuration System { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ] } View the configuration for the \u0027Logging\u0027 facet within the \u0027Storage\u0027 component edgecmd Configuration Storage Logging { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } View the configuration of a specific entry in the \u0027PeriodicEgressEndpoint\u0027 facet within the \u0027Storage\u0027 component edgecmd Configuration Storage PeriodicEgressEndpoints id=Endpoint_1 { \"id\": \"Endpoint_1\", \"executionPeriod\": \"2.00:00:00\", \"name\": null, \"description\": null, \"enabled\": true, \"endpoint\": \"http://localhost:5590\", \"http:  localhost:5590\", \"clientId\": null, \"clientSecret\": null, \"userName\": \"user_54\", \"password\": \"***************\", \"validateEndpointCertificate\": true, \"tokenEndpoint\": null, \"debugExpiration\": null, \"namespaceId\": \"default\", \"backfill\": false, \"egressFilter\": null, \"streamPrefix\": null, \"typePrefix\": null } Configure Edge Data Store To create a configuration, you must enter the component and facet where the configuration payload should go, followed by key=value pairs to specify which values are to be changed. Example: Change all values in the \u0027Logging\u0027 facet: edgecmd Configuration Storage Logging LogLevel=Warning LogFileSizeLimitBytes=32768 LogFileCountLimit=5 You can use this to configure any number of valid key=value pairs in a facet. Example: Change a single value in the \u0027Logging\u0027 facet: edgecmd Configuration Storage Logging LogFileCountLimit=5 You can also use it to add an entry to a collection configuration, for example, the \u0027Health Endpoints\u0027 facet in the \u0027System\u0027 component: edgecmd Configuration System HealthEndpoints Id=endpoint_1 Endpoint=endpointURL UserName=UserName Password=Password Note: If an entry with the specified id already exists, it will be updated based on the new key=value pairs. Configure with JSON Files You can also configure Edge Data Store by a JSON file input into the edgecmd application. File imports will completely replace the existing configurations that you are attempting to change. Therefore, it cannot be used to change individual values in a facet without modifying others. To import a bulk configuration: edgecmd Configuration file=PathToJsonFile To import a facet specific configuration file for a component: edgecmd Configuration componentId facetName file=PathToJsonFile To import a file with configuration for individual facets, you can use a bulk file import operation. The file must contain just payload for the given component ID. Example command: edgecmd Configuration file=\"~/Bulk_Storage_Runtime.json\" file=\"~ Bulk_Storage_Runtime.json\" The file \u0027Bulk_Storage_Runtime.json\u0027 contains: { \"Storage\": { \"Runtime\": { \"StreamStorageLimitMb\": 66, \"StreamStorageTargetMb\": 33, \"IngressDebugExpiration\": \"2020-07-08T01:00:00\", \"CheckpointRateInSec\": 6, \"TransactionLogLimitMB\": 350, \"EnableTransactionLog\": true } } } The command will only affect the \u0027Runtime\u0027 facet in the \u0027Storage\u0027 component, it will not change any other components or facets. However, if you import a file containing the following, the \u0027StreamStorageLimitMb\u0027 and \u0027StreamStorageTargetMb\u0027 values would be modified, resetting the remaining values in the facet (IngressDebugExpiration, CheckpointRateInSec, TransactionLogLimitMB, and EnableTransactionLog) to their default values: { \"Storage\": { \"Runtime\": { \"StreamStorageLimitMb\": 66, \"StreamStorageTargetMb\": 33, } } } Delete configuration data You can use the edgecmd application to delete configuration data from Edge Data Store. To delete a configuration entry from a collection configuration (for example, \u0027HealthEndpoints\u0027 facet within the \u0027System\u0027 component), you must specify the component ID, facet, and ID of the entry to remove followed by the \u0027delete\u0027 keyword. Example: edgecmd Configuration System HealthEndpoints Id=endpoint_1 delete To delete an entire configuration file, you must specify the component ID and facet followed by the \u0027delete\u0027 keyword. Example: edgecmd Configuration System HealthEndpoints delete"
                                        },
    "V1/Administration/ManagementTools.html":  {
                                                   "href":  "V1/Administration/ManagementTools.html",
                                                   "title":  "Edge Data Store management tools",
                                                   "keywords":  "Edge Data Store management tools Command Line configuration You can use the edgecmd utility on both Linux and Windows to configure and manage Edge Data Store. REST tools The following tools are available to facilitate the execution of REST calls. cURL Edge Data Store documentation displays cURL commands for configuration and management examples. cURL is a command line tool supported on Windows and Linux, used to make HTTP calls. cURL has a large range of capabilitie. You can accomplish any Edge Data Store administrative or programming task with cURL. cURL is also easily scripted, using Bash or Powershell on either Linux or Windows. OSIsoft recommends this tool for managing Edge Data Store. Any system that can run Edge Data Store supports cURL. Postman Postman is an effective REST tool for systems with GUI components. Edge Data Store is supported on platforms that lack this capability. It is particularly useful for learning more about Edge Data Store REST APIs. C#, Python, Go You can use any modern programming language to make REST calls to administer and write programs for Edge Data Store. Since the administrative and programming interfaces are unified in using REST, you can write applications that both manage Edge Data Store and read and write data. If you want you can, for example, access the Diagnostics namespace locally to monitor and act upon the local system state. System Tools Many OSIsoft customers use Windows computers, even though they may deploy Linux devices to host Edge Storage. You can install Edge Data Store on Windows 10, and the same custom applications developed on Windows should work on Linux, as long as the application development environment is supported on Linux. Edge Data Store has been designed to use platform independent programming. To facilitate working with Linux devices, Windows tools like PuTTY and WinSCP are very useful for copying files and remotely accessing Linux command lines."
                                               },
    "README.html":  {
                        "href":  "README.html",
                        "title":  "Edge-Data-Store-Docs",
                        "keywords":  "Edge-Data-Store-Docs"
                    },
    "docfx.console.2.43.2/content/articles/intro.html":  {
                                                             "href":  "docfx.console.2.43.2/content/articles/intro.html",
                                                             "title":  "Add your introductions here!",
                                                             "keywords":  "Add your introductions here!"
                                                         },
    "docfx.console.2.43.2/content/api/index.html":  {
                                                        "href":  "docfx.console.2.43.2/content/api/index.html",
                                                        "title":  "PLACEHOLDER",
                                                        "keywords":  "PLACEHOLDER TODO: Add .NET projects to src folder and run docfx to generate a REAL API Documentation !"
                                                    },
    "V1/Troubleshooting/Troubleshooting.html":  {
                                                    "href":  "V1/Troubleshooting/Troubleshooting.html",
                                                    "title":  "Edge Data Store troubleshooting",
                                                    "keywords":  "Edge Data Store troubleshooting If you encounter errors while using or developing against Edge Data Store, you have both local and remote means of diagnosing issues. Edge Data Store supports a diagnostics namespace that is used to store streams containing diagnostic information from Edge Data Store itself. As with any other stream data stored in the Edge Storage component, you can egress this to either PI Web API or OSIsoft Cloud Services to monitor the state of a system remotely. In addition, all components in Edge Data Store support OMF Health messages that can be configured using the Health Endpoint configuration so that OMF Health messages are sent to remote PI Web API or OSIsoft Cloud Service endpoints to support remote monitoring of devices. OMF Ingress These steps are useful when a custom application is not writing stream data to EDS. The first step in any OMF troubleshooting process is to make sure the OMF messages are sent in the correct order (OMF type, OMF container, OMF data). No data will be present until messages are received and processed in that order. Logging of warnings, errors, and message can help with resolving these issues. OMF Ingress logging You can find log messages related to egress in the Storage logs. If you set the the log level temporarily to Trace , most information for troubleshooting will be output. OMF Ingress message debugging In order to troubleshoot problems between an OMF application and Edge Storage, enable debugging. If you set an appropriate value for the IngressDebugExpiration property in a storage runtime configuration , debugging will be enabled for all incoming OMF messages, and HTTP request and response content will be stored to disk for review. The property represents the date and time when debugging should no longer be enabled. You can also disable debugging if you set the value to null . Examples of valid strings representing date and time: Utc: ???yyyy-mm-ddThh:mm:ssZ??? Local: ???mm-dd-yyyy hh:mm:ss??? The content of the OMF message, including the headers will be written to the Logs Directory. For an active application this can become quite large. As a result, debug information is stored to disk in a separate format that usual log messages. A single file will be written to the usual Logs directory for every incoming OMF Type, Container, and Data message. Periodic Egress EDS Periodic Egress extracts data from SDS streams and will send the appropriate sequences of type, container, and data OMF messages on startup. If you see unexpected data in an OCS or PI System one thing to check for is if multiple devices may be writing to the same stream. Careful use of stream prefixes in the periodic egress endpoint configuration will ensure output data streams will be logically separated in the systems of record. Type prefixes may be helpful if you have a case where you have changed a stream type definitions on EDS. OMF types on both OCS and the PI System are immutable once created - if the type of the data stream changes, it is best to either delete the old type definition (if nothing is still using it) or add a type prefix to create a new unique type that will be used by new streams egressing from EDS to the systems of record. Periodic Egress logging You can find log messages related to egress in the Storage logs. If you set the the log level temporarily to Trace , most information for troubleshooting will be output. Periodic Egress debugging In order to troubleshoot problems between Edge Data Store and the destination, enable debugging. If you set an appropriate value for the DebugExpiration property in an egress configuration, debugging for that destination will be enabled, and HTTP request and response content will be stored to disk for review. The property represents the date and time when debugging should no longer be enabled. You can also disable debugging if you set the value to null . Examples of valid strings representing date and time: Utc: ???yyyy-mm-ddThh:mm:ssZ??? Local: ???mm-dd-yyyy hh:mm:ss??? The content length of each request/response request response and the overall number of requests/responses, requests responses, can be quite large. As a result, debug information is stored to disk in a separate location than the typical log messages. Debug folders/files folders files will be created under the Edge Data Store data folder. The debug folder/file folder file structure is: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\Storage\\egressdump\\{tenantId}\\{namespaceId}\\{egressId}\\{omfType}\\{Ticks}-{Guid}-{Req/Res}.txt %programdata%\\OSIsoft\\EdgeDataStore\\Storage\\egressdump\\{tenantId}\\{namespaceId}\\{egressId}\\{omfType}\\{Ticks}-{Guid}-{Req Res}.txt Linux: /usr/share/OSIsoft/EdgeDataStore/Storage/egressdump/{tenantId}/{namespaceId}/{egressId}/{omfType}/{Ticks}-{Guid}-{Req/Res}.txt  usr share OSIsoft EdgeDataStore Storage egressdump {tenantId} {namespaceId} {egressId} {omfType} {Ticks}-{Guid}-{Req Res}.txt omfType represents Type/Container/Data; Type Container Data; Ticks represents the tick count for the Utc DateTime when it was decided that the message would be written to disk; Guid represents a unique Guid for each request/response request response pair; Req/Res Req Res whether it was the HTTP request or response."
                                                },
    "V1/Security/SecurityOverview.html":  {
                                              "href":  "V1/Security/SecurityOverview.html",
                                              "title":  "Edge Data Store security overview",
                                              "keywords":  "Edge Data Store security overview EDS adapter REST APIs Edge Data Store supports REST APIs for configuration, data reading (through SDS), and data writing (through OMF and SDS). Edge Data Store provides only localhost access to REST APIs. Any code that reads or writes to the REST APIs must reside on the computer or device on which Edge Data Store is running. REST access is through HTTP. The default port is 5590. The port number can be changed during installation, or during configuration after installation. URLs must be of the form \"http://localhost:{port}/\" \"http:  localhost:{port} \" or \"http://127.0.0.1:{port}/\". \"http:  127.0.0.1:{port} \". Note: Do not use the host\u0027s name or IP Address in the URL. Note: Docker users must use the \"host\" networking mode for the container. For an example of running a container using this mode, see the section pertaining to Docker containers . Data egress Writing data to OSIsoft Cloud Services or OSIsoft PI Web API is not limited to the local machine. You can write data to either of these destinations using HTTPS. Adapters Modbus and Opc Ua are not limited to the local machine. Both are enabled to access remote data sources through binary protocols. Secure storage Sensitive information, such as passwords and client secrets, are saved in configuration files in an encrypted form. Only the Edge Data Store runtime can properly store and retrieve these protected data items. Note: Do not manually edit configuration files. Altering encrypted values will cause failures. The only time unencrypted values for sensitive information are available is when you provide them to the system through the REST API, such as with initial configuration or update. From that point forward, the unencrypted values are not available, neither in the configuration files nor through the REST API. The REST API will only return a placeholder string for such values. You should use caution when you submit sensitive data items. For example, you should remove from the system any temporary file containing unencrypted credentials used to submit configuration to the REST API. Service and file system security The installer creates a specific user account that the Edge Data Store service runs under. You can only use this account for running the service. For example, you cannot use it for interactive sessions. You cannot typically configure this service account. Modifying the service configuration in this respect could cause system failure. The Edge Data Store binary files, configuration files, and data files are configured by the installer and runtime to allow appropriate access by the service account. You do not normally need to modify the permission and ownership assignments for these files, and should not as failures could occur."
                                          },
    "V1/SDS/Writing_Data_API.html":  {
                                         "href":  "V1/SDS/Writing_Data_API.html",
                                         "title":  "API calls for writing data",
                                         "keywords":  "API calls for writing data Example type, stream, and data Many of the following API methods descriptions following contain example requests and responses in JSON to highlight usage and specific behaviors. The following type, stream, and data are used in the examples: Example type SimpleType is an SdsType with a single index and two additional properties. This type is defined in Python and Javascript: Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class SimpleType(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2, } var SimpleType = function () { this.Time = null; this.State = null; this.Value = null; } Example stream Simple is an SdsStream of type SimpleType . Example data Simple has stored values as follows: 11/23/2017 11 23 2017 12:00:00 PM: Ok 0 11/23/2017 11 23 2017 1:00:00 PM: Ok 10 11/23/2017 11 23 2017 2:00:00 PM: Ok 20 11/23/2017 11 23 2017 3:00:00 PM: Ok 30 11/23/2017 11 23 2017 4:00:00 PM: Ok 40 All times are represented at offset 0, GMT. Insert Values Inserts data into the specified stream. Returns an error if data is already present at the index of any event. Request POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Parameters string namespaceId default or diagnostics string streamId The stream identifier Request body A serialized list of one or more events of the stream type Response The response includes a status code Note: This request will return an error if an event already exists for any index in the request. If any individual index encounters a problem, the entire operation is rolled back and no insertions are made. The streamId and index that caused the issue are included in the error response. Example The following request is used to insert events into stream Simple of SimpleType , POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data The request body specifies the values to insert: [ { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T18:00:00Z\", \"State\": 0, \"Measurement\": 60 } ] Patch Values Modifies the specified stream event(s). Patching affects only the data item parameters that are included in the call. Request PATCH api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?select={selectExpression} Parameters string namespaceId default or diagnostics string streamId The stream identifier string selectExpression Comma-separated list of strings that indicates the event fields to be changed in stream events Request body A serialized collection of one or more patch property events Response The response includes a status code Consider you have a stream Simple of SimpleType , to change one property, Measurement , for one event specify the following request PATCH api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?select=measurement With the following request body, [ { \"Time\":\"2017-11-23T12:00:00Z\", \"Measurement\":500.0 } ] This request will only change the Measurement value at the specified event index. Note: Patching is used to patch the events of the selected fields for one or more events in the stream. Only the fields indicated in selectExpression are modified. The events to be modified are indicated by the index value of each entry in the collection. If there is a problem patching any individual event, the entire operation is rolled back and the error will indicate the streamId and index of the problem. Remove Values There are two options for specifying which events to remove from a stream: Index Collection : One or more indexes can be specified in the request. Window : A window can be specified with a start index and end index. Index Collection Removes the event at each index from the specified stream. Different overloads are available to make it easier to indicate the index where you want to remove a data event. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?index={index}[\u0026index={index}???] Parameters string namespaceId default or diagnostics string streamId The stream identifier string index One or more indexes of events to remove Response The response includes a status code Note: If any individual event fails to be removed, the entire operation is rolled back and no events are removed. The streamId and index that caused the issue are included in the error response. If you attempt to remove events at indexes that have no events, an error is returned. If this occurs, you can use Window request format to remove any events from a specified ???window??? of indexes, which will not return an error if no data is found. Window Removes events at and between the start index and end index. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026endIndex={endIndex} Parameters string namespaceId default or diagnostics string streamId The stream identifier string startIndex The index defining the beginning of the window string endIndex The index defining the end of the window Response The response includes a status code Note: If any individual event fails to be removed, the entire operation is rolled back and no removes are done. Replace Values Writes one or more events over existing events in the specified stream. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?allowCreate=false Parameters string namespaceId default or diagnostics string streamId The stream identifier Request body A serialized list of one or more events of the stream type Response The response includes a status code Note: This request returns an error if the stream does not have an event to be replaced at the specified index. If any individual event fails to be replaced, the entire operation is rolled back and no replaces are performed. The index that caused the issue and the streamId are included in the error response. Update Values Writes one or more events to the specified stream. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Parameters string namespaceId default or diagnostics string streamId The stream identifier Request body A serialized list of one or more events of the stream type Response The response includes a status code Note: This request performs an insert or a replace depending on whether an event already exists at the event indexes. If any item fails to write, the entire operation is rolled back and no events are written to the stream. The index that caused the issue is included in the error response."
                                     },
    "V1/SDS/Writing_Data.html":  {
                                     "href":  "V1/SDS/Writing_Data.html",
                                     "title":  "Write data",
                                     "keywords":  "Write data The SDS REST APIs provide programmatic access to read and write SDS data. This topic describes the APIs used to write SdsStream data. All writes rely on a stream???s key or primary index. The primary index determines the order of events in the stream. Secondary indexes are updated, but they do not contribute to the request. All references to indexes are to the primary index. Single stream writes The following support writing multiple values: Insert Values inserts a collection of events. Patch Values updates specific fields for a collection of events. Replace Values replaces a collection of events. Remove Values deletes the events based on the request parameters. Update Values add or replaces a collection of events. The base URI for writing SDS data to a single stream is: api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Parameters string namespaceId default or diagnostics string streamId The stream identifier Request body format With the exception of Remove Values, all single stream write calls require a request body containing the events to insert or modify. The events must be formatted as a serialized JSON array of the stream\u0027s type. JSON arrays are comma-delimited lists of a type enclosed within square brackets. The following code shows a list of three WaveData events that are properly formatted for insertion. For the complete example, see the OCS-Samples . [ { \"Order\":2, \"Tau\":0.25722883666666846, \"Radians\":1.6162164471269089, \"Sin\":1.9979373673043652, \"Cos\":-0.090809010174665111, \"Tan\":-44.003064529862513, \"Sinh\":4.8353589272389, \"Cosh\":5.2326566823391856, \"Tanh\":1.8481468289554672 }, { \"Order\":4, \"Tau\":0.25724560000002383, \"Radians\":1.6163217742567466, \"Sin\":1.9979277915696148, \"Cos\":-0.091019446679060964, \"Tan\":-43.901119254534827, \"Sinh\":4.8359100947709592, \"Cosh\":5.233166005842703, \"Tanh\":1.8481776000882766 }, { \"Order\":6, \"Tau\":0.25724560000002383, \"Radians\":1.6163217742567466, \"Sin\":1.9979277915696148, \"Cos\":-0.091019446679060964, \"Tan\":-43.901119254534827, \"Sinh\":4.8359100947709592, \"Cosh\":5.233166005842703, \"Tanh\":1.8481776000882766 } ] You can serialize your data using one of many available JSON serializers available at Introducing JSON . Response format Supported response formats include JSON, verbose JSON, and SDS. The default response format for SDS is JSON, which is used in all examples in this documentation. Default JSON responses do not include any values that are equal to the default value for their type. Verbose JSON responses include all values in the returned JSON payload, including defaults. To specify verbose JSON return, add the header Accept-Verbosity with a value of verbose to the request. Verbose has no impact on writes; writes return only error messages. To specify SDS format, set the Accept header in the request to application/sds application sds . Indexes SDS writes rely on the primary index for positioning within streams and locating existing events. Most writes use the index as specified by the value. Deletes are the exception to this rule. When deleting, indexes are specified as strings in the URI. For more details about working with indexes, see the Indexes page. To specify compound indexes in the URI, specify each field that composes the index, in the specified order, separated by the pipe character, ???|???."
                                 },
    "V1/SDS/Units_of_Measure.html":  {
                                         "href":  "V1/SDS/Units_of_Measure.html",
                                         "title":  "Units of measure",
                                         "keywords":  "Units of measure The Sequential Data Store (SDS) provides a collection of built-in units of measure (Uom). These units of measure can be associated with SdsStreams and SdsTypes in order to provide unit information for stream data that model measurable quantities. If data has unit information associated with it, SDS is able to support unit conversions when retrieving data. For more information, see Reading data . Since a unit of measurement (that is meter) defines the magnitude of a quantity (that is Length), SDS represents this through two objects: SdsUom and SdsUomQuantity. SdsUom A SdsUom represents a single unit of measure, such as \u0027meter\u0027. The following table shows the required and optional SdsUom fields. Property Type Optionality Description Example Id String Required Unique identifier for the unit of measure meters per second Abbreviation String Optional Abbreviation for the unit of measure m/s m s Name String Optional Full name for the unit of measure Meters per second DisplayName String Optional Friendly display name for the unit of measure meters per second QuantityId String Required The identifier associated with the quantity that this unit is a measure of Velocity ConversionFactor Double Required Used for unit conversions. When a value of this unit is multiplied by the ConversionFactor and then incremented by the ConversionOffset, the value in terms of the base unit of the corresponding quantity is returned. 1.0 ConversionOffset Double Required Used for unit conversions. See details for ConversionFactor 0.0 SdsUomQuantity Represents a single measurable quantity (i.e. Length) The following table shows the required and optional SdsUomQuantity fields. Property Type Optionality Description Example Id String Required Unique identifier for the quantity Velocity Name String Optional Full name for the quantity Velocity BaseUom SdsUom Required The base unit of measure for this quantity. All other Uom\u0027s measuring this quantity will have ConversionFactor\u0027s and ConversionOffsets relative to the BaseUom SdsUom representing \"meters per second\" Dimensions short[] Optional Reserved for internal use. Represents the seven base SI dimensions: Length, Mass, Time, Electric Current, Thermodynamic Temperature, Amount of Substance, and Luminous Density. [1,0,-1,0,0,0,0] Supported quantities A list of the supported quantities and their base unit of measures follows. Supported quantities are read-only. Quantity Id Base Uom Id Angular Velocity radian per second Area square meter Computer Storage byte Density kilogram per cubic meter Dynamic Viscosity pascal second Electric Charge coulomb Electric Current ampere Electric Potential volt Electric Resistance ohm Energy joule Entropy and Heat Capacity joule per kelvin Force newton Frequency hertz Length meter Luminous Intensity candela Mass kilogram Mass Flow Rate kilogram per second Molar Flow Rate mole per second Molecular Weight kilogram per mole Amount of Substance mole Plane Angle radian Power watt Pressure pascal Quantity count Ratio percent Specific Energy joule per kilogram Specific Entropy and Specific Heat Capacity joule per kilogram kelvin Specific Volume cubic meter per kilogram Speed meter per second Temperature kelvin Temperature (Delta) delta kelvin Time second Volume cubic meter Volume Flow Rate cubic meter per second Supported units of measure A list of the supported units of measure follows. Supported units of measure are read-only. Uom Id Abbreviation Quantity Id Conversion Factor Conversion Offset count count Quantity 1 0 Ampere hour Ah Electric Charge 3600 0 coulomb C Electric Charge 1 0 kilogram per second kg/s kg s Mass Flow Rate 1 0 long ton per day lton/d lton d Mass Flow Rate 0.011759802 0 million pound per day MMlb/d MMlb d Mass Flow Rate 5.24991169 0 short ton per day ston/d ston d Mass Flow Rate 0.010499823 0 thousand pound per day klb/d klb d Mass Flow Rate 0.005249912 0 gram per second g/s g s Mass Flow Rate 0.001 0 pound per second lb/s lb s Mass Flow Rate 0.45359237 0 tonne per day t/d t d Mass Flow Rate 0.011574074 0 long ton lton Mass 1016.046909 0 million pound MM lb Mass 453592.37 0 ounce oz Mass 0.028349523 0 short ton ston Mass 907.18474 0 thousand pound klb Mass 453.59237 0 ton ton Mass 907.18474 0 gram g Mass 0.001 0 milligram mg Mass 1.00E-06 0 pound lb Mass 0.45359237 0 tonne t Mass 1000 0 kilogram kg Mass 1 0 second s Time 1 0 hour h Time 3600 0 day d Time 86400 0 month month Time 2628000 0 week week Time 604800 0 year yr Time 31536000 0 minute min Time 60 0 dyne dyne Force 1.00E-05 0 kilogram-force kgf Force 9.80665 0 pound-force lbf Force 4.448221615 0 newton N Force 1 0 watt W Power 1 0 million British thermal unit per day MM Btu/d Btu d Power 12211.29459 0 million British thermal unit per hour MM Btu/h Btu h Power 293071.0702 0 gigawatt GW Power 1000000000 0 megawatt MW Power 1000000 0 British thermal unit per hour Btu/h Btu h Power 0.29307107 0 calorie per second cal/s cal s Power 4.1868 0 horsepower hp Power 745.6998716 0 joule per second J/s J s Power 1 0 kilowatt kW Power 1000 0 megajoule per hour MJ/h MJ h Power 277.7777778 0 million calorie per hour MMcal/h MMcal h Power 1163 0 mole per second mol/s mol s Molar Flow Rate 1 0 gram mole per second gmol/s gmol s Molar Flow Rate 1 0 kilogram mole per second kmol/s kmol s Molar Flow Rate 1000 0 pound mole per second lbmol/s lbmol s Molar Flow Rate 453.59237 0 meter m Length 1 0 centimeter cm Length 0.01 0 inch in Length 0.0254 0 International nautical mile nmi Length 1852 0 kilometer km Length 1000 0 millimeter mm Length 0.001 0 foot ft Length 0.3048 0 mile mi Length 1609.344 0 sixteenth of an inch sxi Length 0.0015875 0 yard yd Length 0.9144 0 candela cd Luminous Intensity 1 0 meter per second m/s m s Speed 1 0 centimeter per second cm/s cm s Speed 0.01 0 foot per second ft/s ft s Speed 0.3048 0 International nautical mile per hour nmi/h nmi h Speed 0.514444444 0 kilometer per hour km/h km h Speed 0.277777778 0 mile per hour mi/h mi h Speed 0.44704 0 revolution per minute rpm Angular Velocity 0.104719755 0 radian per second rad/s rad s Angular Velocity 1 0 barrel per day bbl/d bbl d Volume Flow Rate 1.84E-06 0 cubic centimeter per second cm3/s cm3 s Volume Flow Rate 1.00E-06 0 cubic foot per second ft3/s ft3 s Volume Flow Rate 0.028316847 0 cubic meter per hour m3/h m3 h Volume Flow Rate 0.000277778 0 Imperial gallon per minute Imp gal/min gal min Volume Flow Rate 7.58E-05 0 liter per second L/s L s Volume Flow Rate 0.001 0 US gallon per minute US gal/min gal min Volume Flow Rate 6.31E-05 0 cubic meter per second m3/s m3 s Volume Flow Rate 1 0 pascal Pa Pressure 1 0 atmosphere atm Pressure 101325 0 bar bar Pressure 100000 0 inches of mercury inHg Pressure 3386.388158 0 kilogram-force per square centimeter kgf/cm2 kgf cm2 Pressure 98066.5 0 kilogram-force per square meter kgf/m2 kgf m2 Pressure 9.80665 0 kilopascal kPa Pressure 1000 0 millimeter of mercury mmHg Pressure 133.3223684 0 newton per square meter N/m2 N m2 Pressure 1 0 pound-force per square inch psi Pressure 6894.757293 0 pound-force per square inch (customary) psia Pressure 6894.757293 0 torr torr Pressure 133.3223684 0 square meter m2 Area 1 0 square foot ft2 Area 0.09290304 0 acre acre Area 4046.856422 0 square mile mi2 Area 2589988.11 0 square yard yd2 Area 0.83612736 0 hectare ha Area 10000 0 square centimeter cm2 Area 0.0001 0 square inch in2 Area 0.00064516 0 square kilometer km2 Area 1000000 0 square millimeter mm2 Area 1.00E-06 0 yobibyte YiB Computer Storage 1.21E+24 0 zebibyte ZiB Computer Storage 1.18E+21 0 exbibyte EiB Computer Storage 1.15E+18 0 pebibyte PiB Computer Storage 1.13E+15 0 tebibyte TiB Computer Storage 1.10E+12 0 gibibyte GiB Computer Storage 1073741824 0 mebibyte MiB Computer Storage 1048576 0 kibibyte KiB Computer Storage 1024 0 yottabyte YB Computer Storage 1.00E+24 0 zettabyte ZB Computer Storage 1.00E+21 0 exabyte EB Computer Storage 1.00E+18 0 petabyte PB Computer Storage 1.00E+15 0 terabyte TB Computer Storage 1.00E+12 0 gigabyte GB Computer Storage 1000000000 0 megabyte MB Computer Storage 1000000 0 kilobyte kB Computer Storage 1000 0 byte B Computer Storage 1 0 kelvin K Temperature 1 0 degree Celsius ??C Temperature 1 273.15 degree Rankine ??R Temperature 0.555555556 -2.56E-13 degree Fahrenheit ??F Temperature 0.555555556 255.3722222 milliampere mA Electric Current 0.001 0 ampere A Electric Current 1 0 joule per gram J/g J g Specific Energy 1000 0 joule per kilogram J/kg J kg Specific Energy 1 0 British thermal unit per pound Btu/lb Btu lb Specific Energy 2326 0 kilocalorie per kilogram kcal/kg kcal kg Specific Energy 4186.8 0 kilojoule per kilogram kJ/kg kJ kg Specific Energy 1000 0 kilojoule per pound kJ/lb kJ lb Specific Energy 2204.622622 0 British thermal unit per degree Rankine Btu/??R Btu ??R Entropy and Heat Capacity 1899.100535 0 British thermal unit per degree Fahrenheit Btu/??F Btu ??F Entropy and Heat Capacity 1899.100535 0 kilojoule per kelvin kJ/K kJ K Entropy and Heat Capacity 1000 0 joule per kelvin J/K J K Entropy and Heat Capacity 1 0 cubic foot per pound ft3/lb ft3 lb Specific Volume 0.062427961 0 cubic centimeter per gram cm3/g cm3 g Specific Volume 0.001 0 cubic meter per kilogram m3/kg m3 kg Specific Volume 1 0 hertz Hz Frequency 1 0 mole mol Amount of Substance 1 0 gram mole gmol Amount of Substance 1 0 kilogram mole kmol Amount of Substance 1000 0 pound mole lbmol Amount of Substance 453.59237 0 percent % Ratio 1 0 parts per billion ppb Ratio 1.00E-07 0 parts per million ppm Ratio 0.0001 0 ohm ?? Electric Resistance 1 0 gram per gram mole g/gmol g gmol Molecular Weight 0.001 0 pound per pound mole lb/lbmol lb lbmol Molecular Weight 0.001 0 kilogram per mole kg/mol kg mol Molecular Weight 1 0 kilogram per kilogram mole kg/kmol kg kmol Molecular Weight 0.001 0 British thermal unit per pound degree Rankine Btu/(lb Btu (lb ??R) Specific Entropy and Specific Heat Capacity 4186.8 0 British thermal unit per pound degree Fahrenheit Btu/(lb Btu (lb ??F) Specific Entropy and Specific Heat Capacity 4186.8 0 joule per gram kelvin J/(g J (g K) Specific Entropy and Specific Heat Capacity 1000 0 kilojoule per kilogram kelvin kJ/(kg kJ (kg K) Specific Entropy and Specific Heat Capacity 1000 0 joule per kilogram kelvin J/(kg J (kg K) Specific Entropy and Specific Heat Capacity 1 0 kilovolt kV Electric Potential 1000 0 millivolt mV Electric Potential 0.001 0 megavolt MV Electric Potential 1000000 0 volt V Electric Potential 1 0 joule J Energy 1 0 gigawatt hour GWh Energy 3.60E+12 0 megawatt hour MWh Energy 3600000000 0 watt hour Wh Energy 3600 0 British thermal unit Btu Energy 1055.055853 0 calorie cal Energy 4.1868 0 gigajoule GJ Energy 1000000000 0 kilojoule kJ Energy 1000 0 kilowatt hour kWh Energy 3600000 0 megajoule MJ Energy 1000000 0 watt second Ws Energy 1 0 kilocalorie kcal Energy 4186.8 0 million calorie MMcal Energy 4186800 0 million British thermal unit MM Btu Energy 1055055853 0 acre foot acre ft Volume 1233.481838 0 million imperial gallon Imp Mgal Volume 4546.09 0 thousand imperial gallon Imp kgal Volume 4.54609 0 barrel bbl Volume 0.158987295 0 Imperial gallon Imp gal Volume 0.00454609 0 million US gallon US Mgal Volume 3785.411784 0 thousand US gallon US kgal Volume 3.785411784 0 cubic centimeter cm3 Volume 1.00E-06 0 cubic foot ft3 Volume 0.028316847 0 kiloliter kL Volume 1 0 liter L Volume 0.001 0 megaliter M L Volume 1000 0 milliliter mL Volume 1.00E-06 0 thousand cubic meter k m3 Volume 1000 0 US gallon US gal Volume 0.003785412 0 million barrel MMbbl Volume 158987.2949 0 thousand barrel kbbl Volume 158.9872949 0 cubic meter m3 Volume 1 0 kilogram per cubic meter kg/m3 kg m3 Density 1 0 gram per liter g/L g L Density 1 0 kilogram per liter kg/L kg L Density 1000 0 pound per barrel lb/bbl lb bbl Density 2.853010174 0 pound per cubic foot lb/ft3 lb ft3 Density 16.01846337 0 pound per US gallon lb/US lb US gal Density 119.8264273 0 tonne per cubic meter t/m3 t m3 Density 1000 0 radian rad Plane Angle 1 0 degree ?? Plane Angle 0.017453293 0 revolution r Plane Angle 6.283185307 0 pascal second Pa*s Dynamic Viscosity 1 0 poise P Dynamic Viscosity 0.1 0 delta degree Fahrenheit delta ??F Temperature (Delta) 0.555555556 0 delta degree Rankine delta ??R Temperature (Delta) 0.555555556 0 delta kelvin delta K Temperature (Delta) 1 0 delta degree Celsius delta ??C Temperature (Delta) 1 0 SdsUomQuantity API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsUomQuantitys. For general SdsUomQuantity information, see Units of Measure . Get Quantity Returns the quantity corresponding to the specified quantity Id within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Quantities/{quantityId} api v1 Tenants default Namespaces {namespaceId} Quantities {quantityId} Parameters string namespaceId default or diagnostics string quantityId The quantity identifier Response The response includes a status code and a response body. Response body The requested SdsUomQuantity. Example response body for quantityId = \"Length\": HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"Length\", \"Name\": \"Length\", \"BaseUom\": { \"Id\": \"meter\", \"Abbreviation\": \"m\", \"Name\": \"meter\", \"DisplayName\": \"meter\", \"QuantityId\": \"Length\", \"ConversionFactor\": 1 }, \"Dimensions\": [ 1, 0, 0, 0, 0, 0, 0 ] } Get Quantities Returns a list of all quantities available within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Quantities?skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} Quantities?skip={skip}\u0026count={count} Parameters string namespaceId default or diagnostics int skip An optional parameter representing the zero-based offset of the first SdsUomQuantity to retrieve. If not specified, a default value of 0 is used. int count An optional parameter representing the maximum number of SdsUomQuantity to retrieve. If not specified, a default value of 100 is used. Response The response includes a status code and a response body. Response body A list of SdsUomQuantity objects Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"Angular Velocity\", \"Name\": \"Angular Velocity\", \"BaseUom\": { \"Id\": \"radian per second\", \"Abbreviation\": \"rad/s\", \"rad s\", \"Name\": \"radian per second\", \"DisplayName\": \"radian per second\", \"QuantityId\": \"Angular Velocity\", \"ConversionFactor\": 1 }, \"Dimensions\": [ 0, 0, -1, 0, 0, 0, 0 ] }, { \"Id\": \"Area\", \"Name\": \"Area\", \"BaseUom\": { \"Id\": \"square meter\", \"Abbreviation\": \"m2\", \"Name\": \"square meter\", \"DisplayName\": \"square meter\", \"QuantityId\": \"Area\", \"ConversionFactor\": 1 }, \"Dimensions\": [ 2, 0, 0, 0, 0, 0, 0 ] }, ] Get Quantity Uom Returns the unit of measure associated with the specified uomId belonging to the quantity with the specified quantityId. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Quantities/{quantityId}/Units/{uomId} api v1 Tenants default Namespaces {namespaceId} Quantities {quantityId} Units {uomId} Parameters string namespaceId default or diagnostics string quantityId The quantity identifier string uomId The unit of measure identifier Response The response includes a status code and a response body. Response body The requested SdsUom Example response for quantityId = \"Length\" and uomId =\"mile\": HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"mile\", \"Abbreviation\": \"mi\", \"Name\": \"mile\", \"DisplayName\": \"mile\", \"QuantityId\": \"Length\", \"ConversionFactor\": 1609.344 } Get Quantity Uoms Returns the list of units of measure that belongs to the quantity with the specified quantityId. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Quantities/{quantityId}/Units api v1 Tenants default Namespaces {namespaceId} Quantities {quantityId} Units Parameters string namespaceId default or diagnostics string quantityId The quantity identifier Response The response includes a status code and a response body. Response body A collection of SdsUom objects for the specified quantity Example response for quantityId = \"Electric Current\": HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"milliampere\", \"Abbreviation\": \"mA\", \"Name\": \"milliampere\", \"DisplayName\": \"milliampere\", \"QuantityId\": \"Electric Current\", \"ConversionFactor\": 0.001 }, { \"Id\": \"ampere\", \"Abbreviation\": \"A\", \"Name\": \"ampere\", \"DisplayName\": \"ampere\", \"QuantityId\": \"Electric Current\", \"ConversionFactor\": 1 } ] SdsUom API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsUoms. For general SdsUom information, see Units of Measure . Get Uom Returns the unit of measure corresponding to the specified uomId within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Units/{uomId} api v1 Tenants default Namespaces {namespaceId} Units {uomId} Parameters string namespaceId default or diagnostics string uomId The unit of measure identifier Response The response includes a status code and a response body. Response body The requested SdsUom Example response body for uomId = \"ounce\": HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"ounce\", \"Abbreviation\": \"oz\", \"Name\": \"ounce\", \"DisplayName\": \"ounce\", \"QuantityId\": \"Mass\", \"ConversionFactor\": 0.028349523 } Get Uoms Returns a list of all available units of measure in the system. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Units?skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} Units?skip={skip}\u0026count={count} Parameters string namespaceId default or diagnostics int skip An optional parameter representing the zero-based offset of the first SdsUomQuantity to retrieve. If not specified, a default value of 0 is used. int count An optional parameter representing the maximum number of SdsUomQuantity to retrieve. If not specified, a default value of 100 is used. Response The response includes a status code and a response body. Response body A list of SdsUom objects Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"count\", \"Abbreviation\": \"count\", \"Name\": \"count\", \"DisplayName\": \"count\", \"QuantityId\": \"Quantity\", \"ConversionFactor\": 1 }, { \"Id\": \"Ampere hour\", \"Abbreviation\": \"Ah\", \"Name\": \"Ampere hour\", \"DisplayName\": \"Ampere hour\", \"QuantityId\": \"Electric Charge\", \"ConversionFactor\": 3600 }, { \"Id\": \"coulomb\", \"Abbreviation\": \"C\", \"Name\": \"coulomb\", \"DisplayName\": \"coulomb\", \"QuantityId\": \"Electric Charge\", \"ConversionFactor\": 1 } ] Associate a unit of measure with an SdsType At SdsType creation, you can associate an SdsUom with a SdsTypeProperty . Associate a unit of measure with an SdsStream At SdsStream creation, you can override any unit of measure associated with an SdsTypeProperty belonging to the SdsType of the stream. This enables the reuse of an SdsType that may have default unit information associated with it already."
                                     },
    "V1/SDS/table_format.html":  {
                                     "href":  "V1/SDS/table_format.html",
                                     "title":  "Table format",
                                     "keywords":  "Table format A table is a convenient structure for analytics and display. The REST APIs for retrieving multiple events from the data store supports returning results in a table. You can set the form variable to specify a table or a table with headers. You can apply table format to any read that returns multiple values and summaries. The following is a request to retrieve values using the window parameters: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z The following response would be returned from the above code: Content-Type: application/json application json [ { \"Time\":\"2017-04-01T07:00:00Z\", \"State\":1 }, { \"Time\":\"2017-04-01T07:01:00Z\", \"State\":1, \"Measurement\":1.0 }, { \"Time\":\"2017-04-01T07:02:00Z\", \"State\":1, \"Measurement\":2.0 }, { \"Time\":\"2017-04-01T07:03:00Z\", \"State\":1, \"Measurement\":3.0 }, { \"Time\":\"2017-04-01T07:04:00Z\", \"State\":1, \"Measurement\":4.0 }, { \"Time\":\"2017-04-01T07:05:00Z\", \"State\":1, \"Measurement\":5.0 }, { \"Time\":\"2017-04-01T07:06:00Z\", \"State\":1, \"Measurement\":6.0 }, { \"Time\":\"2017-04-01T07:07:00Z\", \"State\":1, \"Measurement\":7.0 }, { \"Time\":\"2017-04-01T07:08:00Z\", \"State\":1, \"Measurement\":8.0 }, { \"Time\":\"2017-04-01T07:09:00Z\", \"State\":1, \"Measurement\":9.0 } ] To retrieve the results in table format, add the form variable and specify table . GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z \u0026form=table Response Content-Type: application/json application json { \"Name\":\"Simple\", \"Columns\":[ { \"Name\":\"Time\", \"Type\":\"DateTime\" }, { \"Name\":\"State\", \"Type\":\"Int32Enum\" }, { \"Name\":\"Measurement\", \"Type\":\"Double\" } ], \"Rows\":[ [ \"2017-04-01T07:00:00Z\", 1, 0.0 ], [ \"2017-04-01T07:01:00Z\", 1, 1.0 ], [ \"2017-04-01T07:02:00Z\", 1, 2.0 ], [ \"2017-04-01T07:03:00Z\", 1, 3.0 ], [ \"2017-04-01T07:04:00Z\", 1, 4.0 ], [ \"2017-04-01T07:05:00Z\", 1, 5.0 ], [ \"2017-04-01T07:06:00Z\", 1, 6.0 ], [ \"2017-04-01T07:07:00Z\", 1, 7.0 ], [ \"2017-04-01T07:08:00Z\", 1, 8.0 ], [ \"2017-04-01T07:09:00Z\", 1, 9.0 ] ] } To retrieve the results in table format with column headers, add the form variable and specify tableh . GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z \u0026form=tableh Response Content-Type: application/json application json { \"Name\":\"Simple\", \"Columns\":[ { \"Name\":\"Time\", \"Type\":\"DateTime\" }, { \"Name\":\"State\", \"Type\":\"Int32Enum\" }, { \"Name\":\"Measurement\", \"Type\":\"Double\" } ], \"Rows\":[ [ \"Time\", \"State\", \"Measurement\" ], [ \"2017-04-01T07:00:00Z\", 1, 0.0 ], [ \"2017-04-01T07:01:00Z\", 1, 1.0 ], [ \"2017-04-01T07:02:00Z\", 1, 2.0 ], [ \"2017-04-01T07:03:00Z\", 1, 3.0 ], [ \"2017-04-01T07:04:00Z\", 1, 4.0 ], [ \"2017-04-01T07:05:00Z\", 1, 5.0 ], [ \"2017-04-01T07:06:00Z\", 1, 6.0 ], [ \"2017-04-01T07:07:00Z\", 1, 7.0 ], [ \"2017-04-01T07:08:00Z\", 1, 8.0 ], [ \"2017-04-01T07:09:00Z\", 1, 9.0 ] ] }"
                                 },
    "V1/SDS/SequentialDataStore.html":  {
                                            "href":  "V1/SDS/SequentialDataStore.html",
                                            "title":  "OSIsoft Sequential Data Store (SDS)",
                                            "keywords":  "OSIsoft Sequential Data Store (SDS) The Edge Data Store includes the Sequential Data Store (SDS) REST APIs for reading and writing data stored locally on the device where the Edge Data Store is running. SDS is the same technology that is used in OCS for storing data, so the usage of the REST APIs is very similar to OCS for reading and writing data. All data from all sources on the Edge Data Store (Modbus, OPC UA, OMF, SDS) can be read using the SDS REST APIs on the local device, in the default tenant and the default namespace. In addition, the default tenant has a diagnostics namespace where diagnostic data are written by the Edge Data Store and installed components that can be read to monitor the health of a running system using the SDS REST APIs. The SDS instance running in EDS is an advanced storage engine that is also used in OCS. While it works very well for storing OMF compatible data in EDS, it can also be used for advanced scenarios where data stored in SDS cannot be converted to OMF. All data egress from EDS to both OCS and the PI System uses OMF in this release of EDS, so for streams that will be egressed to the PI System or OCS, it is recommended that they have only a single time based index. Multiple values are supported in a single stream, but for successful egress there is a limitation in this release of a single time based index only. SDS Reading Data APIs SDS Reading Data SDS Writing Data SDS Writing Data APIs Compression SDS Filter Expressions SDS Streams SDS Types SDS Views SDS Stream Metadata SDS Table Format SDS Searching SDS Units of Measure"
                                        },
    "V1/SDS/Searching.html":  {
                                  "href":  "V1/SDS/Searching.html",
                                  "title":  "Searching",
                                  "keywords":  "Searching Search in SDS provides a way to search text, fields, and so on across the Sequential Data Store. This topic covers the searching for SdsStreams, SdsTypes, and SdsStreamViews. Searching for streams The search functionality for streams is exposed through the REST API. The searchable properties are below. Property Searchable Id Yes TypeId Yes Name Yes Description Yes Indexes No InterpolationMode No ExtrapolationMode No PropertyOverrides No Tags * Yes Metadata * Yes Searching for streams is possible using the REST API and specifying the optional query parameter, as shown here: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} Streams?query={query}\u0026skip={skip}\u0026count={count} The Stream fields valid for search are identified in the fields table located on the Streams page. Note: Stream Metadata has unique syntax rules. For more information, see How Searching Works: Stream Metadata . Searching for types Similarly, the search functionality for types is also exposed through REST API. The query syntax and the request parameters are the same. The only difference is the resource you\u0027re searching on. You can search on different properties for types than for streams. The searchable properties are below. For more information, see Types . Property Searchable Id Yes Name Yes Description Yes SdsTypeCode No InterpolationMode No ExtrapolationMode No Properties Yes, with limitations Searching for types is also possible using the REST API and specifying the optional query parameter, as shown here: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Types?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} Types?query={query}\u0026skip={skip}\u0026count={count} The Type fields valid for search are identified in the fields table located on the Types page. The Properties field is identified as being searchable but with limitations: Each SdsTypeProperty of a given SdsType has its name and Id included in the Properties field. This includes nested SdsTypes of the given SdsType. Therefore, the searching of properties will distinguish SdsTypes by their respective lists of relevant SdsTypeProperty Ids and names. Searching for stream views Similarly, the search functionality for stream views is also exposed through REST API. The query syntax and the request parameters are the same. The only difference is the resource you are searching on. You can match on different properties for stream views than for streams and types. The searchable properties are below. For more information, see Stream Views . Property Searchable Id Yes Name Yes Description Yes SourceTypeId Yes TargetTypeId Yes Properties Yes, with limitations As previously mentioned, searching for stream views is also possible using the REST API and specifying the optional query parameter, as shown here: GET api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} StreamViews?query={query}\u0026skip={skip}\u0026count={count} The Stream View fields valid for search are identified in the fields table located on the Stream Views page. The Properties field is identified as being searchable with limitations because SdsStreamViewProperty objects are not searchable. Only the SdsStreamViewProperty\u0027s SdsStreamView is searchable by its Id, SourceTypeId, and TargetTypeId, which are used to return the top level SdsStreamView object when searching. This includes nested SdsStreamViewProperties. How searching works The query parameter will be applied across all searchable fields of objects that are searched on by default. For example, you can assume that a namespace contains the following streams: streamId Name Description stream1 tempA The temperature from DeviceA stream2 pressureA The pressure from DeviceA stream3 calcA calculation from DeviceA values Using the stream data above, the following table shows the results of a call to get streams with different Query values: QueryString Streams returned temperature Only stream1 returned. calc* Only stream3 returned. DeviceA* All three streams returned. humidity* No streams returned. The skip and count parameters determine which items are returned when a large number of them match the query criteria. count indicates the maximum number of items returned. The maximum value of the count parameter is 1000. skip indicates the number of matched items to skip over before returning matching items. You use the skip parameter when more items match the search criteria than can be returned in a single call. The orderby parameter is supported for searching both the streams and types. The basic functionality of it is to search the items and then return the result in sorted order. The default value for orderby parameter is ascending order. You can change it to descending order by specifying desc alongside the orderby field value. It can be used in conjunction with query , skip , and count parameters. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=id asc GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name desc GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name desc\u0026skip=10\u0026count=20 Search operators You can specify search operators in the query string to return more specific search results. Operators Description AND AND operator. For example, cat AND dog searches for streams containing both \"cat\" and \"dog\". AND must be in all caps. OR OR operator. For example, cat OR dog searches for streams containing either \"cat\" or \"dog\" or both. OR must be in all caps. NOT NOT operator. For example, cat NOT dog searches for streams that have the \"cat\" term or do not have \"dog\". NOT must be in all caps. * Wildcard operator. For example, cat* searches for streams that have a term that starts with \"cat\", ignoring case. : Field-scoped query. For example, id:stream* will search for streams where the id field starts with \"stream\", but will not search on other fields like name or description . Note: Field names are camel case and are case sensitive. ( ) Precedence operator. For example, motel AND (wifi OR luxury) searches for streams containing the motel term and either wifi or luxury (or both). Notes regarding wildcard operator * : You can use the wildcard * only once for each search term, except for the case of a Contains type query clause. In that case, two wildcards are allowed: one as prefix and one as suffix for example, *Tank* is valid but *Ta*nk , Ta*nk* , and *Ta*nk* are currently not supported. The wildcard * only works when specifying a single search term. For example, you can search for Tank* , *Tank , Ta*nk but not Tank Meter* . : Operator You can also determine which fields are searched by using the following syntax: fieldname:fieldvalue Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure * Operator You can use the \u0027*\u0027 character as a wildcard to specify an incomplete string. Query string Matches field value Does not match field value log* log logger analog *log analog alog logg *log* analog alogger lop l*g log logg lop Supported Not Supported * *log l*g log* *log* *l*g* *l*g l*g* Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=log* api v1 Tenants default Namespaces {namespaceId} Streams?query=log* Other operators examples Query string Matches field value Does not match field value mud AND log log mud mud log mud log mud OR log log mud mud log mud AND (NOT log) mud mud log mud AND (log OR pump*) mud log mud pumps mud bath name:stream* AND (description:pressure OR description:pump) The name starts with \"stream\" and the description includes either term \"pressure\" or term \"pump\" How Searching Works: Stream Metadata Stream Metadata modifies the aforementioned search syntax rules and each operator\u0027s behavior is described below. For example, assume that a namespace contains the following streams and the respective metadata key-value pair(s) for each stream. streamId Metadata stream1 { manufacturer, company } { serial, abc } stream2 { serial, a1 } stream3 { status, active } { second key, second value } : Operator A stream metadata key is only searchable in association with a stream metadata value. This pairing is defined using the same field scoping \u0027:\u0027 operator, like myStreamMetadataKey:streamMetadataValue If the \u0027:\u0027 operator is not used within an individual search clause, metadata keys are not searched against.Instead, metadata values are searched against (along with the other searchable Stream fields). QueryString Streams returned manufacturer:company Only stream1 returned. company Only stream1 returned. a* All three streams returned. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=manufacturer:company api v1 Tenants default Namespaces {namespaceId} Streams?query=manufacturer:company * Operator For searching on metadata values, the \u0027*\u0027 character is again used as a wildcard to specify an incomplete string. Additionally, you can use this wildcard character with the metadata key as well. This is not supported for any other \"fields\". If you include a wildcard in a field (defined as a value to the immediate left of a \u0027:\u0027 operator), the query will only be valid against stream metadata. QueryString Streams returned manufa*turer:compan* Only stream1 returned. ser*al:a* Stream1 and stream2 are returned. s*:a* All three streams returned. Id:stream* All three streams returned. Id*:stream* Nothing returned. Note: In the final example nothing matches on a stream\u0027s Id value because including \u0027*\u0027 in a search clause\u0027s field prevents non-stream metadata fields from being searched. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=manufa*turer:compan* api v1 Tenants default Namespaces {namespaceId} Streams?query=manufa*turer:compan*"
                              },
    "V1/SDS/SDS_Views.html":  {
                                  "href":  "V1/SDS/SDS_Views.html",
                                  "title":  "Stream views",
                                  "keywords":  "Stream views An SdsStreamView provides a way to map stream data requests from one data type to another. You can apply a stream View to any read or GET operation. SdsStreamView is used to specify the mapping between source and target types. SDS attempts to determine how to map Properties from the source to the destination. When the mapping is straightforward, such as when the properties are in the same position and of the same data type, or when the properties have the same name, SDS will map the properties automatically. When SDS is unable to determine how to map a source property, the property is removed. If SDS encounters a target property that it cannot map to, the property is added and configured with a default value. To map a property that is beyond the ability of SDS to map on its own, you should define an SdsStreamViewProperty and add it to the SdsStreamView???s Properties collection. The following table shows the required and optional SdsStreamView fields. Fields that are not included are reserved for internal SDS use. For more information on search limitations, see Searching . Property Type Optionality Searchability Details Id String Required Yes Identifier for referencing the stream view Name String Optional Yes Friendly name Description String Optional Yes Description text SourceTypeId String Required Yes Identifier of the SdsType of the SdsStream TargetTypeId String Required Yes Identifier of the SdsType to convert events to Properties IList\u003cSdsStreamViewProperty\u003e Optional Yes, with limitations Property level mapping Rules for the stream view identifier (SdsStreamView.Id) Is not case sensitive Can contain spaces Cannot contain forward slash (\"/\") (\" \") Can contain a maximum of 100 characters Properties /   SdsStreamViewProperty The SdsStreamView Properties collection provides detailed instructions for specifying the mapping of event properties. Each SdsStreamViewProperty in the properties collection defines the mapping of an event???s property. SdsStreamView properties are required only when property mapping is not straightforward. Additionally, if you do not want a type property mapped, it is not necessary to create an SdsStreamView property for it. The following table shows the required and optional SdsStreamViewProperty fields. Property Type Optionality Details SourceId String Required Identifier of the SdsTypeProperty from the source SdsType Properties list TargetId String Required Identifier of the SdsTypeProperty from the target SdsType Properties list SdsStreamView SdsStreamView Optional Additional mapping instructions for derived types The SdsStreamView field supports nested properties. SdsStreamViewMap When an SdsStreamView is added, SDS defines a plan mapping. Plan details are retrieved as an SdsStreamViewMap. The SdsStreamViewMap provides a detailed property-by-property definition of the mapping. The following table shows the SdsStreamViewMap fields. The SdsStreamViewMap cannot be written to SDS, so required and optional have no meaning. Property Type Optionality Details SourceTypeId String Required Identifier of the SdsType of the SdsStream TargetTypeId String Required Identifier of the SdsType to convert events to Properties IList\u003cSdsStreamViewMapProperty\u003e Optional Property level mapping Properties /   SdsStreamViewMapProperty The SdsStreamViewMapProperty is similar an SdsStreamViewProperty but adds a mode detailing one or more actions taken on the property. The following table shows the SdsStreamViewMapProperty fields. The SdsStreamViewMap cannot be written; it can only be retrieved from SDS, so required and optional have no meaning. Property Type Details SourceTypeId String Identifier of the SdsType of the SdsStream TargetTypeId String Identifier of the SdsType to convert events to Mode SdsStreamViewMode Aggregate of actions applied to the properties. SdsStreamViewModes are combined via binary arithmetic SdsStreamViewMap SdsStreamViewMap Mapping for derived types The available SdsStreamViewModes are shown in the following table. Name Value Description None 0x0000 No action FieldAdd 0x0001 Add a property matching the specified SdsTypeProperty FieldRemove 0x0002 Remove the property matching the specified SdsTypeProperty FieldRename 0x0004 Rename the property matching the source SdsTypeProperty to the target SdsTypeProperty FieldMove 0x0008 Move the property from the location in the source to the location in the target FieldConversion 0x0016 Converts the source property to the target type InvalidFieldConversion 0x0032 Cannot perform the specified mapping Changing stream type Stream Views can be used to change the type defining a stream. You cannot modify the SdsType; types are immutable. But you can map a stream from its current type to a new type. To update a stream\u0027s type, define an SdsStreamView and PUT the stream view to the following: PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Type?streamViewId={streamViewId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Type?streamViewId={streamViewId} For details, see Update Stream Type . Working with SdsStreamViews When working with Stream Views either invoke HTTP directly or use some of the sample code. Both Python and JavaScript samples have SdsStreamView definitions. The JSON for a simple mapping between a source type with identifier Sample and a target type with identifier Sample1 would appear as follows. { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple1\" } The SdsStreamViewMap would appear as follows. { \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple1\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\", \"Mode\":4 } ] } SdsStreamView API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsStreamViews. See Stream Views for general SdsStreamView information. Get Stream View Returns the stream view corresponding to the specified streamViewId within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId} api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Parameters string namespaceId default or diagnostics string streamViewId The stream view identifier Response The response includes a status code and a response body. Response body The requested SdsStreamView. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\" } ] } Get Stream View Map Returns the stream view map corresponding to the specified streamViewId within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId}/Map api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Map Parameters string namespaceId default or diagnostics string streamViewId The stream view identifier Response The response includes a status code and a response body. Response body The requested SdsStreamView. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\", \"Mode\":20 }, { \"SourceId\":\"State\", \"Mode\":2 }, { \"TargetId\":\"State\", \"Mode\":1 } ] } Get Stream Views Returns a list of stream views within a given namespace. If specifying the optional search query parameter, the list of stream views returned will match the search criteria. If the search query parameter is not specified, the list will include all stream views in the Namespace. See Searching for information about specifying those respective parameters. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} api v1 Tenants default Namespaces {namespaceId} StreamViews?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string namespaceId default or diagnostics string query An optional parameter representing a string search. For information about specifying the search parameter, see Searching . int skip An optional parameter representing the zero-based offset of the first SdsStreamView to retrieve. If not specified, a default value of 0 is used. int count An optional parameter representing the maximum number of SdsStreamViews to retrieve. If not specified, a default value of 100 is used. string orderby An optional parameter representing sorted order which SdsStreamViews will be returned. A field name is required. The sorting is based on the stored values for the given field (of type string). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc would sort the returned results by the name values, descending. If no value is specified, there is no sorting of results. Response The response includes a status code and a response body. Response body A collection of zero or more SdsStreamViews. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\" }, { \"Id\":\"StreamViewWithProperties\", \"Name\":\"StreamViewWithProperties\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\" } ] } ] Get or Create Stream View If a stream view with a matching identifier already exists, the stream view passed in is compared with the existing stream view. If the stream views are identical, a Found (302) status is returned and the stream view. If the stream views are different, the Conflict (409) error is returned. If no matching identifier is found, the specified stream view is created. Request POST api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId} api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Parameters string namespaceId default or diagnostics string streamViewId The stream view identifier. The identifier must match the SdsStreamView.Id field. Request body The request content is the serialized SdsStreamView. Response The response includes a status code and a response body. Response body The newly created or matching SdsStreamView. Create or Update Stream View Creates or updates the definition of a stream view. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId} api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Parameters string namespaceId default or diagnostics string streamViewId The stream view identifier Request body The request content is the serialized SdsStreamView. Response The response includes a status code and a response body. Response body The newly created or updated SdsStreamView. Delete Stream View Deletes a stream view from the specified tenant and namespace. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId} api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Parameters string namespaceId default or diagnostics string streamViewId The stream view identifier Response The response includes a status code."
                              },
    "V1/SDS/SDS_Types.html":  {
                                  "href":  "V1/SDS/SDS_Types.html",
                                  "title":  "Types",
                                  "keywords":  "Types The Sequential Data Store (SDS) stores streams of events and provides convenient ways to find and associate events. Events are stored in streams, called SdsStreams. An SdsType defines the shape or structure of the event and how to associate events within the SdsStream. SdsTypes can define simple atomic types, such as integers, floats, strings, arrays, and dictionaries. They can also define complex types using SdsTypes. You can define complex, nested types using the Properties collection of an SdsType. An SdsType used to define an SdsStream must have a key. A key is a property, or a combination of properties that constitute an ordered, unique identity. The key is ordered, so it functions as an index. It is known as the primary index. While a timestamp (DateTime) is a very common type of key, any type that can be ordered is permitted. Other indexes (secondary indexes), are defined in the SdsStream. For more details on indexes, see Indexes . When you define a type, consider how the events will be represented in a stream. The SdsType defines each event in the stream. An event is a single unit whose properties have values that relate to the index; that is, each property of an SdsType event is related to the event???s index. Each event is a single unit. An SdsType is referenced by its identifier or Id field. SdsType identifiers must be unique within a Namespace. An SdsType can also refer other SdsTypes by using their identifiers. This enables type re-usability. Nested types and base types are automatically created as separate types. For further information, see Type Reusability SdsTypes define how events are associated and read within a collection of events, or SdsStream. The read characteristics when attempting to read non-existent indexes, indexes that fall between, before or after existing indexes, are determined by the interpolation and extrapolation settings of the SdsType. For more information about read characteristics, see Interpolation and Extrapolation . SdsTypes are immutable. After you create an SdsType, you cannot change its definition. If the definition of an SdsType is incorrect, you must delete and recreate it. In addition, the SdsType may be deleted only if no streams, stream views, or types reference it. Only SdsTypes used to define SdsStreams or SdsStreamViews are required to be added to the Sequential data store. SdsTypes that define properties or base types are contained within the parent SdsType and are not required to be added to the Data Store independently. The following table shows the required and optional SdsType fields. Fields that are not included are reserved for internal SDS use. For search limitations, see the Searching . Property Type Optionality Searchable Details Id String Required Yes Identifier for referencing the type Name String Optional Yes Friendly name Description String Optional Yes Description text SdsTypeCode SdsTypeCode Required No Numeric code identifying the base SdsType InterpolationMode SdsInterpolationMode Optional No Interpolation setting of the type. Default is Continuous. ExtrapolationMode SdsExtrapolationMode Optional No Extrapolation setting of the type. Default is All. Properties IList\u003cSdsTypeProperty\u003e Required Yes, with limitations List of SdsTypeProperty items Rules for the type identifier (SdsType.Id) Is not case sensitive Can contain spaces Cannot contain forward slash (\"/\") (\" \") Can contain a maximum of 100 characters SdsTypeCode The SdsTypeCode is a numeric identifier used by the Data Store to identify SdsTypes. A SdsTypeCode exists for every supported type. Atomic types, such as strings, floats and arrays, are defined entirely by the SdsTypeCode. Atomic types do not need fields to define the type. Types requiring additional definition, such as enums and objects, are identified using a generic SdsTypeCode, such as ByteEnum, Int32Enum, NullableInt32Enum, or Object, plus additional SdsProperty fields. Supported Types The following types are supported and defined by the SdsTypeCode: Type SdsTypeCode Array 400 Boolean 3 BooleanArray 203 Byte 6 ByteArray 206 ByteEnum 606 Char 4 CharArray 204 DateTime 16 DateTimeArray 216 DateTimeOffset 20 DateTimeOffsetArray 220 DBNull 2 Decimal 15 DecimalArray 215 Double 14 DoubleArray 214 Empty 0 Guid 19 GuidArray 219 IDictionary 402 IEnumerable 403 IList 401 Int16 7 Int16Array 207 Int16Enum 607 Int32 9 Int32Array 209 Int32Enum 609 Int64 11 Int64Array 211 Int64Enum 611 NullableBoolean 103 NullableByte 106 NullableByteEnum 706 NullableChar 104 NullableDateTime 116 NullableDateTimeOffset 120 NullableDecimal 115 NullableDouble 114 NullableGuid 119 NullableInt16 107 NullableInt16Enum 707 NullableInt32 109 NullableInt32Enum 709 NullableInt64 111 NullableInt64Enum 711 NullableSByte 105 NullableSByteEnum 705 NullableSingle 113 NullableTimeSpan 121 NullableUInt16 108 NullableUInt16Enum 708 NullableUInt32 110 NullableUInt32Enum 710 NullableUInt64 112 NullableUInt64Enum 712 Object 1 SdsColumn 510 SdsObject 512 SdsStream 507 SdsStreamIndex 508 SdsTable 509 SdsType 501 SdsTypeProperty 502 SdsValues 511 SdsStreamView 503 SdsStreamViewMap 505 SdsStreamViewMapProperty 506 SdsStreamViewProperty 504 SByte 5 SByteArray 205 SByteEnum 605 Single 13 SingleArray 213 String 18 StringArray 218 TimeSpan 21 TimeSpanArray 221 UInt16 8 UInt16Array 208 UInt16Enum 608 UInt32 10 UInt32Array 210 UInt32Enum 610 UInt64 12 UInt64Array 212 UInt64Enum 612 Version 22 VersionArray 222 Interpolation Interpolation determines how a stream behaves when asked to return an event at an index between two existing events. InterpolationMode determines how the returned event is constructed. The table below lists InterpolationModes: Mode Enumeration value Operation Default 0 The default InterpolationMode is Continuous Continuous 0 Interpolates the data using previous and next index values StepwiseContinuousLeading 1 Returns the data from the previous index StepwiseContinuousTrailing 2 Returns the data from the next index Discrete 3 Returns ???null??? Note that Continuous cannot return events for values that cannot be interpolated, such as when the type is not numeric. The table below describes how the Continuous InterpolationMode affects indexes that occur between data in a stream: InterpolationMode = Continuous or Default Type Result for an index between data in a stream Comment Numeric Types Interpolated* Rounding is done as needed for integer types Time related Types Interpolated DateTime, DateTimeOffset, TimeSpan Nullable Types Interpolated** Limited support for nullable numeric types Array and List Types No event is returned String Type No event is returned Boolean Type Returns value of nearest index Enumeration Types Returns Enum value at 0 This may have a value for the enumeration GUID No event is returned Version No event is returned IDictionary or IEnumerable No event is returned Dictionary, Array, List, and so on. *When extreme values are involved in an interpolation (for example Decimal.MaxValue) the call might result in a BadRequest exception. **Nullable types are interpolated in the same manner as their non-nulllable equivalents as long as the values surrounding the desired interpolation index are non-null. If either of the values are null, the interpolated value will be null. If the InterpolationMode is not assigned, the events are interpolated in the default manner, unless the interpolation mode is overridden in the SdsTypeProperty or the SdsStream. For more information on overriding the interpolation mode on a specific type property, see SdsTypeProperty . For more information on overriding the interpolation mode for a specific stream, see Sds Streams . Extrapolation Extrapolation defines how a stream responds to requests with indexes that precede or follow all data in the steam. ExtrapolationMode acts as a master switch to determine whether extrapolation occurs and at which end of the data. ExtrapolationMode works with the InterpolationMode to determine how a stream responds. The following tables show how ExtrapolationMode affects returned values for each InterpolationMode value: ExtrapolationMode with InterpolationMode = Default or Continuous ExtrapolationMode Enumeration value Index before data Index after data All 0 Returns first data value Returns last data value None 1 No event is returned No event is returned Forward 2 No event is returned Returns last data value Backward 3 Returns first data value No event is returned ExtrapolationMode with InterpolationMode = Discrete ExtrapolationMode Enumeration value Index before data Index after data All 0 No event is returned No event is returned None 1 No event is returned No event is returned Forward 2 No event is returned No event is returned Backward 3 No event is returned No event is returned ExtrapolationMode with InterpolationMode = StepwiseContinuousLeading ExtrapolationMode Enumeration value Index before data Index after data All 0 Returns first data value Returns last data value None 1 No event is returned No event is returned Forward 2 No event is returned Returns last data value Backward 3 Returns first data value No event is returned ExtrapolationMode with InterpolationMode = StepwiseContinuousTrailing ExtrapolationMode Enumeration value Index before data Index after data All 0 Returns first data value Returns last data value None 1 No event is returned No event is returned Forward 2 No event is returned Returns last data value Backward 3 Returns first data value No event is returned If the ExtrapolationMode is not assigned, the events are extrapolated in the default manner, unless the extrapolation mode is overridden on the SdsStream. For more information on overriding the extrapolation mode on a specific stream, see Sds Streams . For additional information about the effect of read characteristics, see the documentation on the read method you are using. SdsTypeProperty The properties collection defines the fields in an SdsType. The following table shows the required and optional SdsTypeProperty fields. Fields that are not included are reserved for internal SDS use. Property Type Optionality Details Id String Required Identifier for referencing the type Name String Optional Friendly name Description String Optional Description text SdsType SdsType Required Field defining the property\u0027s Type IsKey Boolean Required Identifies the property as the Key (Primary Index) Value Object Optional Value of the property Order Int Optional Order of comparison within a compound index InterpolationMode SdsInterpolationMode Optional Interpolation setting of the property. Default is null. Uom String Optional Unit of Measure of the property The SdsTypeProperty???s identifier follows the same rules as the SdsType???s identifier. IsKey is a Boolean value used to identify the SdsType???s Key. A key defined by more than one property is called a compound key. The maximum number of properties that can define a compound key is three. In a compound key, each property that is included in the key is specified as IsKey. The Order field defines the precedence of fields applied to the index. The Value field is used for properties that represent a value. An example of a property with a value is an enum???s named constant. When representing an enum in a SdsType, the SdsType???s properties collection defines the enum???s constant list. The SdsTypeProperty???s Identifier represents the constant???s name and the SdsTypeProperty???s value represents the constant???s value (see the enum State definitions below). InterpolationMode is assigned when the property of the event should be interpolated in a specific way that differs from the InterpolationMode of the SdsType. InterpolationMode is only applied to a property that is not part of the Index. If the InterpolationMode is not set, the property is are interpolated in the manner defined by the SdsType???s IntepolationMode. An SdsType with the InterpolationMode set to Discrete cannot have a property with an InteroplationMode. For more information on interpolation of events, see Interpolation . Uom is the unit of measure for the property. The Uom of a property may be specified by the name or the abbreviation. The names and abbreviations of Uoms are case sensitive. The InterpolationMode and Uom of a property can be overridden on the stream. For more information, see Streams . Supported units of measure For a list of units of measures that are supported for an SdsTypeProperty, see Units of Measure . Working with SdsTypes The following discussion refers to the following types and are defined in Python and JavaScript samples. In the sample code, SdsType , SdsTypeProperty , and SdsTypeCode are defined as in the code snippets shown here: Python class SdsTypeCode(Enum): Empty = 0 Object = 1 DBNull = 2 Boolean = 3 Char = 4 ... class SdsTypeProperty(object): \"\"\"SDS type property definition\"\"\" def __init__(self): self.__isKey = False @property def Id(self): return self.__id @Id.setter def Id(self, id): self.__id = id ... @property def IsKey(self): return self.__isKey @IsKey.setter def IsKey(self, iskey): self.__isKey = iskey @property def SdsType(self): return self.__SdsType @SdsType.setter def SdsType(self, SdsType): self.__SdsType=SdsType ... class SdsType(object): \"\"\"SDS type definitions\"\"\" def __init__(self): self.SdsTypeCode = SdsTypeCode.Object @property def Id(self): return self.__id @Id.setter def Id(self, id): self.__id = id ... @property def BaseType(self): return self.__baseType @BaseType.setter def BaseType(self, baseType): self.__baseType = baseType @property def SdsTypeCode(self): return self.__typeCode @SdsTypeCode.setter def SdsTypeCode(self, typeCode): self.__typeCode = typeCode @property def Properties(self): return self.__properties @Properties.setter def Properties(self, properties): self.__properties = properties JavaScript SdsTypeCodeMap: { Empty: 0, \"Object\": 1, DBNull: 2, \"Boolean\": 3, Char: 4, ... SdsTypeProperty: function (SdsTypeProperty) { if (SdsTypeProperty.Id) { this.Id = SdsTypeProperty.Id; } if (SdsTypeProperty.Name) { this.Name = SdsTypeProperty.Name; } if (SdsTypeProperty.Description) { this.Description = SdsTypeProperty.Description; } if (SdsTypeProperty.SdsType) { this.SdsType = SdsTypeProperty.SdsType; } if (SdsTypeProperty.IsKey) { this.IsKey = SdsTypeProperty.IsKey; } }, SdsType: function (SdsType) { if (SdsType.Id) { this.Id = SdsType.Id } if (SdsType.Name) { this.Name = SdsType.Name; } if (SdsType.Description) { this.Description = SdsType.Description; } if (SdsType.SdsTypeCode) { this.SdsTypeCode = SdsType.SdsTypeCode; } if (SdsType.Properties) { this.Properties = SdsType.Properties; } }, Working with the following types (both Python and JavaScript classes are shown): Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getMeasurement, setMeasurement) def getMeasurement(self): return self.__measurement def setMeasurement(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2, } var Simple = function () { this.Time = null; this.State = null; this.Measurement = null; } Define the SdsType as follows: Python # Create the properties # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime # State is not a pre-defined type. A SdsType must be defined to represent the enum stateTypePropertyOk = SdsTypeProperty() stateTypePropertyOk.Id = \"Ok\" stateTypePropertyOk.Value = State.Ok stateTypePropertyWarning = SdsTypeProperty() stateTypePropertyWarning.Id = \"Warning\" stateTypePropertyWarning.Value = State.Warning stateTypePropertyAlarm = SdsTypeProperty() stateTypePropertyAlarm.Id = \"Alarm\" stateTypePropertyAlarm.Value = State.Alarm stateType = SdsType() stateType.Id = \"State\" stateType.Name = \"State\" stateType.Properties = [ stateTypePropertyOk, stateTypePropertyWarning, \\ stateTypePropertyAlarm ] state = SdsTypeProperty() state.Id = \"State\" state.Name = \"State\" state.SdsType = stateType # Value property is a simple non-indexed, pre-defined type value = SdsTypeProperty() value.Id = \"Measurement\" value.Name = \"Measurement\" value.SdsType = SdsType() value.SdsType.Id = \"Double\" value.SdsType.Name = \"Double\" # Create the Simple SdsType simpleType = SdsType() simpleType.Id = \"Simple\" simpleType.Name = \"Simple\" simpleType.Description = \"Basic sample type\" simpleType.SdsTypeCode = SdsTypeCode.Object simpleType.Properties = [ time ] JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    State is not a pre-defined type. An SdsType must be defined to represent the enum var stateTypePropertyOk = new SdsObjects.SdsTypeProperty({ \"Id\": \"Ok\", \"Value\": State.Ok }); var stateTypePropertyWarning = new SdsObjects.SdsTypeProperty({ \"Id\": \"Warning\", \"Value\": State.Warning }); var stateTypePropertyAlarm = new SdsObjects.SdsTypeProperty({ \"Id\": \"Alarm\", \"Value\": State.Alarm }); var stateType = new SdsObjects.SdsType({ \"Id\": \"State\", \"Name\": \"State\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Int32Enum, \"Properties\": [stateTypePropertyOk, stateTypePropertyWarning, stateTypePropertyAlarm, stateTypePropertyRed] }); //    Measurement property is a simple non-indexed, pre-defined type var measurementProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"doubleType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Double }) }); //    Create the Simple SdsType var simpleType = new SdsObjects.SdsType({ \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": \"This is a simple SDS type \", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [timeProperty, stateProperty, measurementProperty] }); Working with a derived class is easy. For the following derived class: class Derived(Simple): @property def Observation(self): return self.__observation @Observation.setter def Observation(self, observation): self.__observation = observation Extend the SdsType as follows: Python # Observation property is a simple non-indexed, standard data type observation = SdsTypeProperty() observation.Id = \"Observation\" observation.Name = \"Observation\" observation.SdsType = SdsType() observation.SdsType.Id = \"String\" observation.SdsType.Name = \"String\" observation.SdsType.SdsTypeCode = SdsTypeCode.String # Create the Derived SdsType derived = SdsType() derived.Id = \"Derived\" derived.Name = \"Derived\" derived.Description = \"Derived sample type\" derived.BaseType = simpleType # Set the base type to the derived type derived.SdsTypeCode = SdsTypeCode.Object derived.Properties = [ observation ] JavaScript var observationProprety = new SdsObjects.SdsTypeProperty({ \"Id\": \"Observation\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"strType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.String }) }); var derivedType = new SdsObjects.SdsType({ \"Id\": \"Derived\", \"Name\": \"Derived\", \"Description\": \" Derived sample type\", \"BaseType\": simpleType, \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [ observationProprety ] }); Type reusability An SdsType can also refer other SdsTypes by using their identifiers. This enables type re-usability. For example, if there is a common index and value property for a group of types that may have additional properties, a base type can be created with those properties. { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } If a new type should be created with properties additional to the ones above, you can add a reference to the base type by simply specifying the base type\u0027s Id. { \"Id\": \"Complex\", \"Name\": \"Complex\", \"SdsTypeCode\": 1, \"BaseType\":{ \"Id\":\"Simple\" }, \"Properties\": [ { \"Id\": \"Depth\", \"Name\": \"Depth\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } The new type may also include the full type definition of the reference type instead of specifying only the Id. For example: { \"Id\": \"Complex\", \"Name\": \"Complex\", \"SdsTypeCode\": 1, \"BaseType\":{ \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] }, \"Properties\": [ { \"Id\": \"Depth\", \"Name\": \"Depth\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } If the full definition is sent, the referenced types (base type \"Simple\" in the above example) should match the actual type initially created. If the full definition is sent and the referenced types do not exist, they will be created automatically by SDS. Further type creations can reference them as demonstrated above. Note: When trying to get types back from SDS, the results will also include types that were automatically created by SDS. Base types and properties of type Object, Enum, and user-defined collections such as Array, List and Dictionary will be treated as referenced types. Note that streams cannot be created using these referenced types. If a stream of particular type is to be created, the type should contain at least one property with a valid index type as described in Indexes . The index property may also be in the base type as shown in the example above. SdsType API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsTypes. See Types for general SdsType information. Get Type Returns the type corresponding to the specified typeId within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Types/{typeId} api v1 Tenants default Namespaces {namespaceId} Types {typeId} Parameters string namespaceId default or diagnostics string typeId The type identifier Response The response includes a status code and a response body. Response body The requested SdsType Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"SdsTypeCode\": 16 } }, { \"Id\": \"State\", \"Name\": \"State\", \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"SdsTypeCode\": 609, \"Properties\": [ { \"Id\": \"Ok\", \"Value\": 0 }, { \"Id\": \"Warning\", \"Value\": 1 }, { \"Id\": \"Alarm\", \"Value\": 2 } ] } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"SdsTypeCode\": 14 } } ] } Get Type Reference Count Returns a dictionary mapping the object name to the number of references held by streams, stream views and parent types for the specified type. For more information on the use of types to define streams and stream views, see Streams and Steam Views . For further details about type referencing, see: Type Reusability . Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Types/{typeId}/ReferenceCount api v1 Tenants default Namespaces {namespaceId} Types {typeId} ReferenceCount Parameters string namespaceId default or diagnostics string typeId The type identifier Response The response includes a status code and a response body. Response body A dictionary mapping object name to number of references. Example response body: { \"SdsStream\": 3, \"SdsStreamView\": 2, \"SdsType\": 1 } Get Types Returns a list of types within a given namespace. If specifying the optional search query parameter, the list of types returned will match the search criteria. If the search query parameter is not specified, the list will include all types in the namespace. For information about specifying those respective parameters, see Searching . Note: The results will also include types that were automatically created by SDS as a result of type referencing. For further details about type referencing, see: Type Reusability . Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Types?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} api v1 Tenants default Namespaces {namespaceId} Types?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string namespaceId default or diagnostics string query An optional query string to match which SdsTypes will be returned. For information about specifying the query parameter, see the Searching topic. int skip An optional value representing the zero-based offset of the first SdsType to retrieve. If not specified, a default value of 0 is used. int count An optional value representing the maximum number of SdsTypes to retrieve. If not specified, a default value of 100 is used. string orderby An optional parameter representing sorted order which SdsTypes will be returned. A field name is required. The sorting is based on the stored values for the given field (of type string). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc would sort the returned results by the name values, descending. If no value is specified, there is no sorting of results. Response The response includes a status code and a response body. Response body A collection of zero or more SdsTypes Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"SdsTypeCode\": 16 } }, { \"Id\": \"State\", \"Name\": \"State\", \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"SdsTypeCode\": 609, \"Properties\": [ { \"Id\": \"Ok\", \"Value\": 0 }, { \"Id\": \"Warning\", \"Value\": 1 }, { \"Id\": \"Alarm\", \"Value\": 2 } ] } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"SdsTypeCode\": 14 } } ] }, ] Get or Create Type Creates the specified type. If a type with a matching identifier already exists, SDS compares the existing type with the type that was sent. If the types are identical, a Found (302) error is returned with the Location header set to the URI where the type may be retrieved using a Get function. If the types do not match, a Conflict (409) error is returned. Note: A Conflict (409) error will also be returned if the type contains reference to any existing type, but the referenced type definition in the body does not match the existing type. You may reference an existing type without including the reference type definition in the body by using only the Ids. For further details about type referencing, see: Type Reusability . For a matching type ( Found ), clients that are capable of performing a redirect that includes the authorization header can automatically redirect to retrieve the type. However, most clients, including the .NET HttpClient, consider redirecting with the authorization token to be a security vulnerability. When a client performs a redirect and strips the authorization header, SDS cannot authorize the request and returns Unauthorized (401). For this reason, OSIsoft recommends that when using clients that do not redirect with the authorization header, you should disable automatic redirect and perform the redirect manually. Request POST api/v1/Tenants/default/Namespaces/{namespaceId}/Types/{typeId} api v1 Tenants default Namespaces {namespaceId} Types {typeId} Parameters string namespaceId default or diagnostics string typeId The type identifier. The identifier must match the SdsType.Id field in the request body. Request body The request content is the serialized SdsType. Example SdsType content: { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"SdsTypeCode\": 16 } }, { \"Id\": \"State\", \"Name\": \"State\", \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"SdsTypeCode\": 609, \"Properties\": [ { \"Id\": \"Ok\", \"Value\": 0 }, { \"Id\": \"Warning\", \"Value\": 1 }, { \"Id\": \"Alarm\", \"Value\": 2 } ] } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"SdsTypeCode\": 14 } } ] } Response The response includes a status code and a response body. Response body The request content is the serialized SdsType. OSIsoft recommends that you use JSON. Example Response body: HTTP/1.1 HTTP 1.1 201 Content-Type: application/json application json { \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": null, \"SdsTypeCode\": 1, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"Description\": null, \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"Description\": null, \"SdsTypeCode\": 16, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"State\", \"Name\": \"State\", \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"Description\": null, \"SdsTypeCode\": 609, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": [ { \"Id\": \"Ok\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 0, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Warning\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 1, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Alarm\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 2, \"Uom\": null, \"InterpolationMode\": null } ], \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"Description\": null, \"SdsTypeCode\": 14, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null } ], \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } Delete Type Deletes a type from the specified tenant and namespace. Note that a type cannot be deleted if any streams, stream views, or other types reference it. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Types/{typeId} api v1 Tenants default Namespaces {namespaceId} Types {typeId} Parameters string namespaceId default or diagnostics string typeId The type identifier Response The response includes a status code."
                              },
    "V1/SDS/SDS_Streams.html":  {
                                    "href":  "V1/SDS/SDS_Streams.html",
                                    "title":  "Streams",
                                    "keywords":  "Streams SDS stores collections of events and provides convenient ways to find and associating events. Events of consistent structure are stored in streams, called SdsStreams. An SdsType defines the structure of events in an SdsStream. SdsStreams are referenced by their identifier or Id field. SdsStream identifiers must be unique within a namespace. An SdsStream must include a TypeId that references the identifier of an existing SdsType. When an SdsStream contains data, you must use a stream view to update the stream type. The following table shows the required and optional SdsStream fields. Fields not listed are reserved for internal SDS use. Property Type Optionality Searchability Details Id String Required Yes An identifier for referencing the stream TypeId String Required Yes The SdsType identifier of the type to be used for this stream Name String Optional Yes Friendly name Description String Optional Yes Description text Indexes IList\u003cSdsStreamIndex\u003e Optional No Used to define secondary indexes for stream InterpolationMode SdsInterpolationMode Optional No Interpolation setting of the stream. Default is null. ExtrapolationMode SdsExtrapolationMode Optional No Extrapolation setting of the stream. Default is null. PropertyOverrides IList\u003cSdsStreamPropertyOverride\u003e Optional No Used to define unit of measure and interpolation mode overrides for a stream. Tags * IList\u003cString\u003e Optional Yes A list of tags denoting special attributes or categories. Metadata * IDictionary\u003cString, String\u003e Optional Yes A dictionary of string keys and associated string values. Note: Stream tags and metadata are accessed via the Tags API And Metadata API, respectively. However, they are associated with SdsStream objects and can be used as search criteria. Rules for the stream identifier (SdsStream.Id) Is not case sensitive Can contain spaces Cannot contain forward slash (\"/\") (\" \") Can contain a maximum of 100 characters Indexes The Key or Primary Index is defined at the SdsType. Secondary Indexes are defined at the SdsStream. Secondary indexes are applied to a single property; there are no compound secondary indexes. Only SdsTypeCodes that can be ordered are supported for use in a secondary index. Indexes are discussed in greater detail here: Indexes Interpolation and extrapolation The InterpolationMode, ExtrapolationMode, and PropertyOverrides can be used to determine how a specific stream reads data. These read characteristics are inherited from the type if they are not defined at the stream level. For more information about type read characteristics and how these characteristics dictate how events are read see Types . PropertyOverrides PropertyOverrides provide a way to override interpolation behavior and unit of measure for individual SdsType properties for a specific stream. The SdsStreamPropertyOverride object has the following structure: Property Type Optionality Details SdsTypePropertyId String Required SdsTypeProperty identifier InterpolationMode SdsInterpolationMode Optional Interpolation setting. Default is null Uom String Optional Unit of measure The unit of measure can be overridden for any type property defined by the stream type, including primary keys and secondary indexes. For more information about type property units of measure see Types . Read characteristics of the stream are determined by the type and the PropertyOverrides of the stream. The interpolation mode for non-index properties can be defined and overridden at the stream level. For more information about type read characteristics see Types . When specifying property interpolation overrides, if the SdsType InterpolationMode is Discrete , it cannot be overridden at any level. When InterpolationMode is set to Discrete and an event it not defined for that index, a null value is returned for the entire event. SdsStream API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsStreams. See Streams for general SdsStream information. Get Stream Returns the specified stream. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code and a response body. Response body The requested SdsStream. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\":\"Simple\", \"Name\":\"Simple\", \"TypeId\":\"Simple\", } Get Streams Returns a list of streams. If the optional search query parameter is specified, the list of streams returned will match the search criteria. If the search query parameter is not specified, the list will include all streams in the namespace. See Searching for information about specifying those respective parameters. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} api v1 Tenants default Namespaces {namespaceId} Streams?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string namespaceId default or diagnostics string query An optional parameter representing a string search. For information about specifying the search parameter, see Searching . int skip An optional parameter representing the zero-based offset of the first SdsStream to retrieve. If not specified, a default value of 0 is used. int count An optional parameter representing the maximum number of SdsStreams to retrieve. If not specified, a default value of 100 is used. string orderby An optional parameter representing sorted order which SdsStreams will be returned. A field name is required. The sorting is based on the stored values for the given field (of type string). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc would sort the returned results by the name values, descending. If no value is specified, there is no sorting of results. Response The response includes a status code and a response body. Response body A collection of zero or more SdsStreams. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\":\"Simple\", \"TypeId\":\"Simple\" }, { \"Id\":\"Simple with Secondary\", \"TypeId\":\"Simple\", \"Indexes\":[ { \"SdsTypePropertyId\":\"Measurement\" } ] }, { \"Id\":\"Compound\", \"TypeId\":\"Compound\" }, ] Get Stream Type Returns the type definition that is associated with a given stream. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Type api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Type Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code and a response body. Response body The requested SdsType. Get or Create Stream Creates the specified stream. If a stream with a matching identifier already exists, SDS compares the existing stream with the stream that was sent. If the streams are identical, a Found (302) error is returned with the Location header set to the URI where the stream may be retrieved using a Get function. If the streams do not match, a Conflict (409) error is returned. For a matching stream (Found), clients that are capable of performing a redirect that includes the authorization header can automatically redirect to retrieve the stream. However, most clients, including the .NET HttpClient, consider redirecting with the authorization token to be a security vulnerability. When a client performs a redirect and strips the authorization header, SDS cannot authorize the request and returns Unauthorized (401). For this reason, it is recommended that when using clients that do not redirect with the authorization header, you should disable automatic redirect. Request POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Parameters string namespaceId default or diagnostics string streamId The stream identifier. The stream identifier must match the identifier in content. Request body The request content is the serialized SdsStream. Response The response includes a status code and a response body. Response body The newly created SdsStream. Create or Update Stream Creates the specified stream. If a stream with the same Id already exists, the definition of the stream is updated. The following changes are permitted: Name Description Indexes InterpolationMode ExtrapolationMode PropertyOverrides Note that modifying Indexes will result in re-indexing all of the stream\u0027s data for each additional secondary index. For more information on secondary indexes, see Indexes . Unpermitted changes result in an error. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Parameters string namespaceId default or diagnostics string streamId The stream identifier Request body The request content is the serialized SdsStream. Response The response includes a status code. Update Stream Type Updates a stream???s type. The type is modified to match the specified stream view. Defined Indexes and PropertyOverrides are removed when updating a stream type. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Type?streamViewId={streamViewId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Type?streamViewId={streamViewId} Parameters string namespaceId default or diagnostics string streamId The stream identifier string streamViewId The stream view identifier Request body The request content is the serialized SdsStream. Response The response includes a status code. Response body On failure, the content contains a message describing the issue. Delete Stream Deletes a stream. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Parameters string namespaceId default or diagnostics string streamId The stream identifier Response The response includes a status code."
                                },
    "V1/Overview/SDSQuickStart.html":  {
                                           "href":  "V1/Overview/SDSQuickStart.html",
                                           "title":  "Edge Storage SDS quick start",
                                           "keywords":  "Edge Storage SDS quick start This quick start gives an overview on how to get data into the Edge Storage component using the Sequential Data Store (SDS) REST API. Prerequisites: Edge Data Store installed Edge Data Store accessible through a REST API using the default installed port (5590) The examples here use curl, a commonly available tool on both Windows and Linux, and command line commands. You can use the same operations with any programming language or tool that supports making REST calls. You can also accomplish data retrieval steps (GET commands) using a browser, if available on your device. Create an SDS type Complete the following to create an SDS type that describes the format of the data to be stored in a container: Create a JSON file using the example below: { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } Note: The data to be written is a timestamp and numeric value. It is indexed by a timestamp, and the numeric value that will be stored is a 64 bit floating point value. Save the JSON as a file with the name SDSCreateType.json. Run the following curl script: curl -i -d \"@SDSCreateType.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/types/Simple http:  localhost:5590 api v1 tenants default namespaces default types Simple When the previous command completes successfully, an SDS type with the same name is created on the server. You can create any number of containers from a single type, as long as they use a timestamp as an index and a 32 bit floating point value. You only need to create a type the first time you send data with a custom application. It does not cause an error to resend the same definition at a later time. Create an SDS stream Complete the following to create a stream: Create a JSON file using the example below: { \"Id\": \"Simple\", \"Name\": \"Simple\", \"TypeId\": \"Simple\" } Note: This stream references the type you created in the previous step. An error will occur if the type does not exist when the stream is created. As with an SDS type, you only need to create a stream once before sending data events. Resending the same definition repeatedly does not cause an error. Save the JSON as a file with the name SDSCreateStream.json. Run the following curl script: curl -i -d \"@SDSCreateStream.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple http:  localhost:5590 api v1 tenants default namespaces default streams Simple When the previous command completes successfully, an SDS stream is created on the server to store data defined by the specified type. Write data events to the SDS stream After you have created a type and container, you can write data using SDS. Complete the following to write data to a stream: Create a JSON file using the example below: [{ \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 50.0 }, { \"Time\": \"2017-11-23T18:00:00Z\", \"Measurement\": 60.0 }] Note: This example includes two data events that will be stored in the SDS Stream you created in the previous steps. As a best practice, you should batch SDS values when writing them for optimal performance. Save the JSON as a file with the name SDSWriteData.json. Run the following curl script: curl -i -d \"@SDSWriteData.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data When this command completes successfully, two values are written to the SDS stream. Read last data written using SDS You use the SDS REST API to read back data which has been written to the server. The following is an example curl script that reads back the last value entered: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data Last You can also use the following GET command to return the last value written: {\"Time\":\"2017-11-23T18:00:00Z\",\"Measurement\":60.0} Read a range of data events written using SDS You use the SDS REST API to read back data which has been written to the server. The following is an example curl script that reads back a time range of values entered: curl \"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" \"http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" [{\"Time\":\"2017-11-23T17:00:00Z\",\"Measurement\":50.0},{\"Time\":\"2017-11-23T18:00:00Z\",\"Measurement\":60.0}] Both values that were entered were returned. A maximum of 100 values after the specified timestamp will be returned. For more information on SDS APIs, see Sequential Data Store ."
                                       },
    "V1/Overview/PIEgressQuickStart.html":  {
                                                "href":  "V1/Overview/PIEgressQuickStart.html",
                                                "title":  "PI System (PI Web API) egress quick start",
                                                "keywords":  "PI System (PI Web API) egress quick start This document is a quick tour of getting data stored in the Edge Data Store into a remote PI System. This is accomplished using PI Web API which is configured for Basic authentication. Create a periodic egress configuration Complete the following to configure Edge Storage periodic egress for the PI Web API endpoint and credentials: Use the following example to create a JSON file. [{ \"Id\": \"PWA\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cunique stream prefix\u003e\", \"TypePrefix\": \"\u003cunique type prefix\u003e\", \"Endpoint\": \"https://\u003cyour \"https:  \u003cyour PI Web API Server\u003e/piwebapi/omf/\", Server\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null, \"ValidateEndpointCertificate\": true, \"TokenEndpoint\": null }] Add the server name, username and password of your PI Web API server into the \"Endpoint\" definition. You must specify a valid user account that can write data via PI Web API using Basic authentication. Note: StreamPrefix and TypePrefix values can be used to ensure uniqueness on the destination system, if required. The StreamPrefix value will create unique PI Points on the PI System. If you wish to only send specific streams, edit the \"EgressFilter\" value. Examples of more advanced scenarios are in the Egress section of this documentation. Save the JSON with the file name PeriodicEgressEndpoints.json. Run the following curl script, from the same directory where you saved the JSON file, to configure Edge Storage to send data to the PI System. Note: The file and curl script can be run from any directory on the device as long as the file and the curl script are run from the same directory: curl -i -d \"@PeriodicEgressEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/storage/PeriodicEgressEndpoints/ http:  localhost:5590 api v1 configuration storage PeriodicEgressEndpoints  When the command completes successfully, data will start being sent to the PI System."
                                            }
}
