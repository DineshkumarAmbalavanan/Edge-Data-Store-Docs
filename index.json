{
    "V1/OpcUa/PrinciplesOfOperationOPCUA.html":  {
                                                     "href":  "V1/OpcUa/PrinciplesOfOperationOPCUA.html",
                                                     "title":  "Principles of operation",
                                                     "keywords":  "Principles of operation This topic provides an operational overview of the OPC UA EDS adapter, focusing on streams creation and error handling. Adapter configuration For the OPC UA EDS adapter to start data collection, you need to configure the adapter by defining the following: Data source: Provide the data source from which the adapter should collect data. Data selection: Perform selection of OPC UA items to which the adapter should subscribe for data. Logging: Set up the logging attributes to manage the adapter logging behavior. For more information, see OPC UA data source configuration and OPC UA data selection configuration . Connection The OPC UA EDS adapter uses the binary opc.tcp protocol to communicate with the OPC UA servers. A secured connection is enabled by default where the X.509-type client and server certificates are exchanged and verified and the connection between the OPC UA EDS adapter and the configured OPC UA server is established. Stream creation The OPC UA EDS adapter creates types upon receiving the value update for a stream. One stream is created in Edge Data Store for every selected OPC UA item in the data selection configuration. Data collection The OPC UA EDS adapter collects time-series data from selected OPC UA dynamic variables through OPC UA subscriptions (unsolicited reads). The adapter supports the Data Access (DA) part of OPC UA specification. Streams by OPC UA EDS adapter The OPC UA EDS adapter creates a stream with two properties per selected OPC UA item. The properties are described in the following table: Property name Data type Description Timestamp DateTime Timestamp of the given OPC UA item value update. Value Based on type of incoming OPC UA value Value of the given OPC UA item update. Stream ID is a unique identifier for each stream created by the adapter for a given OPC UA item. If the Custom Stream ID is specified for the OPC UA item in data selection configuration, the OPC UA EDS adapter uses that as a stream ID for the stream. Otherwise, the adapter constructs the stream ID using the following format constructed from the OPC UA item node ID: \u003cAdapter Component ID\u003e.\u003cNamespace\u003e.\u003cIdentifier\u003e Note: The naming convention is affected by StreamIdPrefix and ApplyPrefixToStreamID settings in data source configuration. For more information, see OPC UA data source configuration ."
                                                 },
    "V1/OpcUa/OpcUaOverview.html":  {
                                        "href":  "V1/OpcUa/OpcUaOverview.html",
                                        "title":  "OPC UA EDS adapter",
                                        "keywords":  "OPC UA EDS adapter Overview OPC UA is an open standard, which ensures interoperability, security, and reliability of industrial automation devices and systems. OPC UA is recognized as one of the key communication and data modeling technologies of Industry 4.0, due to the fact that it works with many software platforms, and is completely scalable and flexible. The OPC UA EDS adapter transfers time-series data from OPC UA devices into Edge Data Store. You can add a single OPC UA EDS adapter during installation. To install multiple OPC UA EDS adapters, see Edge Data Store Configuration for adding a new component to Edge Data Store. As with other EDS adapters, the OPC UA EDS adapter is configured with data source and data selection JSON documents. The data source configurations are identical with other adapters, but OPC UA supports an option to generate a data selection file template that you can manually edit and use for subsequent configuration. For details, see OPC UA data selection configuration . Once you create a template file, you can reuse it on both Linux and Windows without changes. For more information on how to configure logging for the OPC UA EDS adapter, see Logging configuration ."
                                    },
    "V1/OpcUa/OPCUADataSourceConfiguration.html":  {
                                                       "href":  "V1/OpcUa/OPCUADataSourceConfiguration.html",
                                                       "title":  "Data source configuration",
                                                       "keywords":  "Data source configuration To use the OPC UA EDS adapter, you must configure the data source from which it will receive data. Configure OPC UA data source Note: You cannot modify OPC UA data source configurations manually. You must use the REST endpoints to add or edit the configuration. Complete the following to configure the OPC UA data source: Using any text editor, create a file that contains an OPC UA data source in JSON form. For content structure, see OPC UA data source example . Update the parameters as needed. For a table of all available parameters, see Parameters for OPC UA data source . Save the file as DataSource.config.json . Use any Configuration tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cEDS http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSource/ adapterId\u003e DataSource  . The following example shows the HTTPS request using curl (run this command from the same directory where the file is located): Note: During installation, you can add a single OPC UA EDS adapter named OpcUa1. The following example uses this component name. curl -v -d \"@DataSource.config.json\" -H \"Content-Type: application/json\" application json\" \"http://localhost:5590/api/v1/configuration/OpcUa1/DataSource\" \"http:  localhost:5590 api v1 configuration OpcUa1 DataSource\" Note: After you have completed data source configuration, the next step is to configure data selection. You can either have a default data selection file generated or you can create the data selection file yourself. For more information, see Data selection configuration . OPC UA data source schema The following table shows the basic behavior of the OpcUa_DataSource_schema.json file. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Parameters for OPC UA data source The following parameters can be used to configure an OPC UA data source: Parameter Required Type Nullable Description EndpointURL Required string Yes The endpoint URL of the OPC UA server. The following is an example of the URL format: opc.tcp://OPCServerHost:Port/OpcUa/SimulationServer opc.tcp:  OPCServerHost:Port OpcUa SimulationServer Note: If you change the EndpointURL on a configured OPC UA EDS adapter that has ComponentID_DataSelection.json file exported, you need to remove the ComponentID_DataSelection.json file from the configuration directory to trigger a new browse (export). UseSecureConnection Optional Boolean No When set to true, the OPC UA EDS adapter connects to a secure endpoint using OPC UA certificate exchange operation. The default is true. When set to false, the OPC UA EDS adapter connects to an unsecured endpoint of the server and certificate exchange operation is not required. Note: OSIsoft recommends setting this option to false for testing purposes only. For more information on how to configure security, see Adapter security . UserName Optional string Yes User name for accessing the OPC UA server. Password Optional string Yes Password for accessing the OPC UA server. Note: OSIsoft recommends using REST to configure the data source when the password must be specified. RootNodeIds Optional string Yes List of comma-separated NodeIds of those objects from which the OPC UA EDS adapter browses the OPC UA server address space. This option allows selecting only subsets of the OPC UA address by explicitly listing one or more NodeIds which are used to start the initial browse. For example: ns=5;s=85/0:Simulation, ns=5;s=85 0:Simulation, ns=3;s=DataItems. If not specified, it means that the whole address space will be browsed. IncomingTimestamp Optional string No Specifies whether the incoming timestamp is taken from the source, from the OPC UA server, or should be created by the OPC UA EDS adapter instance. Source - Default and recommended setting. The timestamp is taken from the source timestamp field. The source is what provides data for the item to the OPC UA server, such as a field device. Server - In case the OPC UA item has an invalid source timestamp field, the Server timestamp can be used. Connector - The OPC UA EDS adapter generates a timestamp for the item upon receiving it from the OPC UA server. StreamIdPrefix Optional string Yes Specifies what prefix is used for Stream IDs. Naming convention is StreamIdPrefixNodeId. Note: An empty string means no prefix will be added to the Stream IDs. Null value means ComponentID followed by dot character will be added to the stream IDs (for example, OpcUa1.NodeId). ApplyPrefixToStreamId Optional Boolean No Parameter applied to all data items collected from the data source that have custom stream ID configured. If configured, the adapter will apply the StreamIdPrefix property to all the streams with custom ID configured. The property does not affect any streams with default ID configured OPC UA data source example The following is an example of valid OPC UA data source configuration: { \"EndpointUrl\": \"opc.tcp://IP-Address/TestOPCUAServer\", \"opc.tcp:  IP-Address TestOPCUAServer\", \"UseSecureConnection\": true, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": null }"
                                                   },
    "V1/OpcUa/OPCUAAdapterSecurityConfiguration.html":  {
                                                            "href":  "V1/OpcUa/OPCUAAdapterSecurityConfiguration.html",
                                                            "title":  "Adapter security",
                                                            "keywords":  "Adapter security The OPC UA security standard is concerned with the authentication of client and server applications, the authentication of users and confidentiality of their communication. As the security model relies heavily on Transport Level Security (TLS) to establish a secure communication link with an OPC UA server, each client, including the EDS adapter, must have a digital certificate deployed and configured. Certificates uniquely identify client applications and machines on servers, and allow for creation of a secure communication link when trusted on both sides. OPC UA EDS adapter generates a self-signed certificate when the first secure connection attempt is made. Each OPC UA EDS adapter instance creates a certificate store where its own certificates, as well as those of the server, will be persisted. Configure OPC UA EDS adapter security In your data source configuration, set UseSecureConnection to true . For more information, see Data source configuration . The adapter verifies whether the server certificate is present in the adapter trusted certificates folder and is therefore trusted. In case the certificates were not exchanged before the first attempted connection, the adapter persists the server certificate within the adapter rejected certificates folder and the following warning message about the rejected server certificate will be printed: ~~2019-09-08 11:45:48.093 +01:00~~ [Warning] Rejected Certificate: \"DC=MyServer.MyDomain.int, O=Prosys OPC, CN=Simulation Manually move the server certificate from the RejectedCertificates\\certs folder to the Trusted\\certs folder using a file explorer or command-line interpreter. Linux example using command-line: mv /usr/share/OSIsoft/EdgeDataStore/OpcUa1/Certificates/RejectedCertificates/certsSimulationServer\\  usr share OSIsoft EdgeDataStore OpcUa1 Certificates RejectedCertificates certsSimulationServer\\ \\[F9823DCF607063DBCECCF6F8F39FD2584F46AEBB\\].der /usr/share/OSIsoft/EdgeDataStore/OpcUa1/Certificates/Trusted/certs/  usr share OSIsoft EdgeDataStore OpcUa1 Certificates Trusted certs  Note: Administrator or root privileges are required to perform this operation. Once the certificate is in the adapter trusted certificates folder, the adapter trusts the server and the connection attempt proceeds in making the connection call to the configured server. Add the adapter certificate to the server\u0027s trust store. The connection succeeds only when the adapter certificate is trusted on the server side. For more details on how to make a client certificate trusted, see your OPC UA server documentation. In general, servers work in a similar fashion as the clients, hence you can take a similar approach for making the server certificate trusted on the client side. When certificates are mutually trusted, the connection attempt succeeds and the adapter is connected to the most secure endpoint provided by the server. Certificate locations Adapter rejected certificates Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates\\RejectedCertificates\\certs Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates/RejectedCertificates/certs  usr share OSIsoft EdgeDataStore {ComponentId} Certificates RejectedCertificates certs Adapter trusted certificates Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates\\Trusted\\certs Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates/Trusted/certs  usr share OSIsoft EdgeDataStore {ComponentId} Certificates Trusted certs Adapter\u0027s certificate Windows: %programdata%\\OSIsoft\\EdgeDataStore\\{ComponentId}\\Certificates Linux: /usr/share/OSIsoft/EdgeDataStore/{ComponentId}/Certificates  usr share OSIsoft EdgeDataStore {ComponentId} Certificates"
                                                        },
    "V1/OMF/OMFOverview.html":  {
                                    "href":  "V1/OMF/OMFOverview.html",
                                    "title":  "OSIsoft Message Format (OMF)",
                                    "keywords":  "OSIsoft Message Format (OMF) The Edge Storage component supports both OMF version 1.0 and OMF version 1.1 for data ingress. The OMF ingress functionality is the same technology that is used in OSIsoft Cloud Services (OCS). Writing an OMF application to run on EDS is very similar to writing an OMF application to write data to OCS. No authentication is necessary for writing to Edge Data Store, as long as the application is running on the same device as Edge Data Store. Remote access to OMF data ingress is currently not supported. OMF specification You can find the OMF specification here: The OSIsoft Message Format . EDS uses OMF technology developed for use on OSIsoft Cloud Services (OCS). The behavior of OMF in EDS is very similar to OCS. Dynamic messages are supported, but static messages (usually used for creating PI AF assets) are not supported by EDS. Any static OMF messages sent to the EDS OMF REST endpoint will be ignored. OMF endpoint The route to the OMF endpoint provided by the Edge Storage component is the following: Method: POST Endpoint: http://localhost:5590/api/v1/tenants/default/namespaces/default/omf http:  localhost:5590 api v1 tenants default namespaces default omf Supported functionality The OMF endpoint for the Edge Storage component supports both OMF version 1.0 and OMF version 1.1 for data ingress. If you specify a later version of OMF, errors will be returned. The OMF endpoint for the Edge Storage component can only create messages; it does not support the update action. If a create data message is sent with the same time index, the values will be replaced at that index value. For efficiency reasons, OSIsoft recommends batching OMF messages that are sent to the EDS endpoint. Sending single messages or a small number of messages to the OMF endpoint can be successful, but it is relatively inefficient. When a single message or a small number of messages are sent at a time, the HTTP overhead of creating the request and processing the response on a small device is more expensive than the processing of the OMF message itself. While a large number of OMF messages per second can be processed by EDS platforms, OSIsoft recommends keeping the number of HTTP calls per second low to increase efficiency."
                                },
    "V1/Modbus/PrinciplesOfOperationModbus.html":  {
                                                       "href":  "V1/Modbus/PrinciplesOfOperationModbus.html",
                                                       "title":  "Principles of operation",
                                                       "keywords":  "Principles of operation This topic provides an operational overview of the Modbus TCP EDS adapter, focusing on streams creation and error handling. Adapter configuration For the Modbus TCP EDS adapter to be ready for data collection, you need to configure the adapter by defining the following: Data source: Provide the data source from which the adapter should collect data. Data selection: Perform selection of Modbus TCP items to which the adapter should subscribe for data. Logging: Set up the logging attributes to manage the adapter logging behavior. For more details, see Modbus TCP data source configuration and Modbus TCP data selection configuration . Connection The Modbus TCP EDS adapter communicates with the Modbus TCP devices through the TCP/IP TCP IP network by sending request packets that are constructed based on the data selection configurations, and collects the response packets returned by the devices. Stream creation From the parsed data selection configurations, the Modbus TCP EDS adapter creates types, streams and data based on the information provided. For each measurement in the data selection configuration, a stream is created in the Edge Data Store to store time series data. Data collection The Modbus TCP EDS adapter collects data from the Modbus TCP devices at the polling rates that you specify. The rates are set in each of the data selection configurations and can range from 0 milliseconds (as fast as possible) up to 1 day per polling. The adapter automatically optimizes the data collection process by grouping the requests to reduce the I/O I O load imposed to the Modbus TCP networks. Streams by Modbus TCP EDS adapter For each data selection configuration, the Modbus TCP EDS adapter creates a stream with two properties. The properties are described in the following table: Property name Data type Description Timestamp String The response time of the stream data from the Modbus TCP device. Value Specified by the data selection The value of the stream data from the Modbus TCP device. There is a unique identifier (Stream ID) for each stream created for the selected measurement. If a custom stream ID is specified for the measurement in the data selection configuration, the Modbus TCP EDS adapter will use that stream ID to create the stream. Otherwise, the connector constructs the stream ID using the following format: \u003cAdapter Component ID\u003e.\u003cUnit ID\u003e.\u003cRegister Type\u003e.\u003cRegister Offset\u003e Note: Naming convention is affected by StreamIdPrefix and ApplyPrefixToStreamID settings in data source configuration. For more information, see Modbus TCP data source configuration ."
                                                   },
    "V1/Modbus/ModbusTCPDataSourceConfiguration.html":  {
                                                            "href":  "V1/Modbus/ModbusTCPDataSourceConfiguration.html",
                                                            "title":  "Data source configuration",
                                                            "keywords":  "Data source configuration To use the Modbus TCP EDS adapter, you must configure it for the Modbus TCP data source from which it will polling data. Configure Modbus TCP data source Note: You cannot modify Modbus TCP data source configurations manually. You must use the REST endpoints to add or edit the configuration. Complete the following to configure the Modbus TCP data source: Using any text editor, create a file that contains a Modbus TCP data source in JSON form. You can create or copy this file to any directory on a device with Edge Data Store installed. For content structure, see Modbus TCP data source examples . Update the parameters as needed. For a table of all available parameters, see Parameters for Modbus TCP data source . Save the file as DataSource.config.json . Use any Configuration tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cEDS http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSource/ adapterId\u003e DataSource  . Note: During installation, it is possible to add a single Modbus TCP EDS adapter which is named Modbus1. The following example uses this component name. The following example shows the HTTPS request using curl (run this command from the same directory where the file is located): curl -v -d \"@DataSource.config.json\" -H \"Content-Type: application/json\" application json\" \"http://localhost:5590/api/v1/configuration/Modbus1/DataSource\" \"http:  localhost:5590 api v1 configuration Modbus1 DataSource\" Modbus TCP data source schema The following table describes the basic behavior of the Modbus_DataSource_schema.json file. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Parameters for Modbus TCP data source The following parameters are available for configuring a Modbus TCP data source. Parameter Required Type Nullable Description IpAddress Required string Yes The IP address of the device from which the data is to be collected using the Modbus TCP protocol. Host name is not supported. Port Optional number No The TCP port of the target device that listens for and responds to Modbus TCP requests. The value ranges from 0 to 65535. If not configured, the default TCP port is 502 (which is the default port for Modbus TCP protocol). StreamIdPrefix Optional number Yes Parameter applied to all data items collected from the data source. If not configured, the default value is the ID of the Modbus TCP EDS adapter. The custom StreamIdPrefix has the highest priority. ApplyPrefixToStreamId Optional Boolean No Parameter applied to all data items collected from the data source that have custom stream ID configured. If configured, the adapter will apply the StreamIdPrefix property to all the streams with custom ID configured. The property does not affect any streams with default ID configured ConnectTimeout Optional number No Parameter to specify the time (in milliseconds) to wait when Modbus TCP EDS adapter is trying to connect to the data source. The value ranges from 1000 ms to 30000 ms. The default value is 5000 ms. ReconnectInterval Optional number No Parameter to specify the time (in milliseconds) to wait before retrying to connect to the data source when the data source is offline. The value ranges from 100 ms to 30000 ms. The default value is 1000 ms. RequestTimeout Optional number No Parameter to specify the time (in milliseconds) that Modbus TCP EDS adapter waits for a pending request before marking it as timeout and dropping the request. The default value is 10000 ms. The value must be a positive integer, there is no value range. DelayBetweenRequests Optional number No Parameter to specify the minimum time (in milliseconds) between two successive requests sent to the data source. The value ranges from 0 ms to 1000 ms. The default value is 0 ms. MaxResponseDataLength Optional number No Parameter to limit the maximum length (in bytes) of data that can be read within one transaction. This feature is provided to support devices that limit the number of bytes that can be returned. If there is no device limitation, the request length should be the maximum length of 250 bytes. The value ranges from 2 to 250. The default value is 250 ms. Modbus TCP data source examples The following are examples of valid Modbus TCP data source configurations. Minimum data source configuration: { \"IpAddress\": \"127.0.0.2\", } Maximum data source configuration: { \"IpAddress\": \"127.0.0.4\", \"Port\": 502, \"StreamIdPrefix\": \"my.prefix\", \"ApplyPrefixToStreamId\": true, \"ConnectTimeout\": 5000, \"ReconnectInterval\": 1000, \"RequestTimeout\": 10000, \"DelayBetweenRequests\": 500, \"MaxResponseDataLength\": 125 }"
                                                        },
    "V1/Configuration/Schemas/System_Logging_schema.html":  {
                                                                "href":  "V1/Configuration/Schemas/System_Logging_schema.html",
                                                                "title":  "System logging configuration schema",
                                                                "keywords":  "System logging configuration schema The System logging configuration schema specifies how to formally describe the logging parameters for the System. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden System_Logging_schema.json System logging configuration properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference #/definitions/EdgeLogLevel # definitions EdgeLogLevel Optional No EdgeLoggerConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel reference #/definitions/EdgeLogLevel # definitions EdgeLogLevel Optional"
                                                            },
    "V1/Configuration/Schemas/EdgeSystem_schema.html":  {
                                                            "href":  "V1/Configuration/Schemas/EdgeSystem_schema.html",
                                                            "title":  "Sample Edge Data Store configuration",
                                                            "keywords":  "Sample Edge Data Store configuration The Edge Data Store configuration schema specifies how to formally describe the system parameters (logging, components, health endpoints, port). \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"Components\": [{ \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ], \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 } } Edge Data Store configuration properties Property Type Required Nullable Defined by Storage StorageConfiguration Optional Yes StorageConfiguration System SystemConfiguration Optional Yes SystemConfiguration {ComponentName} {ComponentConfiguration} Optional Yes {ComponentConfiguration}"
                                                        },
    "V1/Configuration/Schemas/ConfigurationSchemaList.html":  {
                                                                  "href":  "V1/Configuration/Schemas/ConfigurationSchemaList.html",
                                                                  "title":  "Configuration schemas",
                                                                  "keywords":  "Configuration schemas Edge Data Store is configured through a series of JSON files. The schemas for these files are provided in the installation directory and are documented following. Edge Data Store configuration schemas Use the following schemas to configure Edge Data Store: EdgeLoggerConfiguration PortConfiguration OmfHealthEndpointConfiguration EdgeDataStoreConfig EDS adapters configuration schemas Use the following schemas to configure EDS adapters: OPC UA DataSourceConfiguration DataCollectionItem EdgeLoggerConfiguration Modbus TCP DataSourceConfiguration DataSelectionConfiguration EdgeLoggerConfiguration Storage configuration schemas Use the following schemas to configure the Storage component: EdgeLoggerConfiguration StorageRuntimeConfiguration OEMConfiguration Periodic Egress Endpoints"
                                                              },
    "docfx.console.2.43.2/content/api/index.html":  {
                                                        "href":  "docfx.console.2.43.2/content/api/index.html",
                                                        "title":  "PLACEHOLDER",
                                                        "keywords":  "PLACEHOLDER TODO: Add .NET projects to src folder and run docfx to generate a REAL API Documentation !"
                                                    },
    "V1/Troubleshooting/DisasterRecovery.html":  {
                                                     "href":  "V1/Troubleshooting/DisasterRecovery.html",
                                                     "title":  "Disaster recovery",
                                                     "keywords":  "Disaster recovery This topic provides procedures for recovery from the failure of a device which has Edge Data Store installed. The disaster recovery process includes the following steps: Back up the Edge Data Store data from the failed device to another location Install Edge Data Store on a new device Move a .zip file of the backed-up data to the new device Restore the backed-up data files to the new device Procedures are provided for Windows and Linux systems. Prerequisite: This topic does not provide instruction for backing up or restoring credentials. All credentials are encrypted for security purposes, so they cannot be copied or transferred. After the new system has had the storage and configuration files copied over, and the service has started, do the following: Re-enter the credentials for the operating system using API calls. After updating, restart the Edge Data Store service. The new Edge device will be running as the previous device, and will contain all the data up to the point the previous device went down. Windows recovery Create a backup of Edge Data Store data from the failed device Prerequisite: Administrative access on the device to successfully restore on Windows system If your device is still able to boot, verify that Edge Data Store service has stopped. Use the Windows Task Manager to stop the Edge Data Store service. Locate the storage and configuration files. Note: Windows storage and configuration files should be in the following locations: _C:\\ProgramData\\OSIsoft\\EdgeDataStore\\Configuration_ _C:\\ProgramData\\OSIsoft\\EdgeDataStore\\Storage_ The ProgramData folder is typically hidden, so if it is not visible, go to the **View** tab in **Windows Explorer** and select the **Hidden Items** check box. Create a zip or tar file containing the storage and configuration directories, and move it to a USB device or other safe location. File transfer can be done with WinSCP, SFTP, or external device. Delete default storage and configuration folders When you have first installed Edge Data Store on the new device, the new system will have a default configuration. Use the Windows Task Manager to stop the Edge Data Store service. Once the service has stopped, navigate to the C:\\ProgramData\\OSIsoft\\EdgeDataStore directory, and delete the default storage and configuration folders from the new device. Restore backed up data files Copy or unzip the backup storage and configuration files into the C:\\ProgramData\\OSIsoft\\EdgeDataStore directory. Note: The C: drive may not be the default drive letter of your system. Refer to My Computer, This PC, or open a command prompt to verify the default drive letter. Use the Windows Task Manager to restart the Edge Data Store service. Linux recovery Create a backup of Edge Data Store data from the failed device Prerequisite: Root access on the Linux device is required. If your device is still able to boot, verify from a terminal window that Edge Data Store service has stopped, using the following command: sudo systemctl stop osisoft.edgedatastore Locate the storage and configuration files. Note: Linux storage and configuration files should be in the following locations: /usr/share/OSIsoft/EdgeDataStore/Configuration  usr share OSIsoft EdgeDataStore Configuration /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore /Storage  Storage Create a zip or tar file containing the storage and configuration directories, and move it to a USB device or other safe location. File transfer can be done with WinSCP, SFTP, or external device. Note: Using the cp command may result in a change in file ownership to the current user. Move the files to the new device When you have first installed Edge Data Store on the new device, the new system will have a default configuration. From a terminal window, stop the Edge Data Store service using the following command: sudo systemctl stop osisoft.edgedatastore Once the service has stopped, navigate to the /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore directory and extract your zip or tar file in that directory again using WinSCP, SFTP, or external device. Restore backed up data files Delete the default storage and configuration folders from the /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore directory. Copy or unzip the backup storage and configuration files into the EdgeDataStore directory. If the ownership of the two directories does not match, update it to edgedatastore for the user and group. Start the Edge Data Store service with the following command: sudo systemctl start osisoft.edgedatastore Verify that Edge Data Store is running with the following command: sudo systemctl status osisoft.edgedatastore Note: Default directory permissions are set to 755, and each subsequent file is 644. If you do not use tar it is possible to have permission issues with the recovery files. Tar matches via string name rather than the account ID/UID. ID UID."
                                                 },
    "V1/Support/Edge Data Store support.html":  {
                                                    "href":  "V1/Support/Edge Data Store support.html",
                                                    "title":  "Edge Data Store support",
                                                    "keywords":  "Edge Data Store support Documentation feedback If you have questions, comments, or other feedback about the Edge Data Store documentation, send an email to documentation@osisoft.com . Technical support and other resources For technical assistance, contact OSIsoft Technical Support at +1 510-297-5828 or through the OSIsoft Customer Portal Contact Us page . The Contact Us page offers additional contact options for customers outside of the United States. When you contact OSIsoft Technical Support, be prepared to provide this information: Product name, version, and build numbers Details about your computer platform (CPU type, operating system, and version number) Time that the difficulty started Log files at that time Details of any environment changes prior to the start of the issue Summary of the issue, including any relevant log files during the time the issue occurred To ask questions of others who use OSIsoft software, join the OSIsoft user community, PI Square . Members of the community can request advice and share ideas about the PI System. The PI Developers Club space within PI Square offers resources to help you with the programming and integration of OSIsoft products."
                                                },
    "V1/Security/SecurityOverview.html":  {
                                              "href":  "V1/Security/SecurityOverview.html",
                                              "title":  "Security",
                                              "keywords":  "Security The following topics discuss basic Edge Data Store security practices. REST APIs Edge Data Store supports REST APIs for configuration, data reading (through SDS), and data writing (through OMF and SDS). Edge Data Store provides only localhost access to REST APIs. Any code that reads or writes to the REST APIs must reside on the computer or device on which Edge Data Store is running. REST access is through HTTP. The default port is 5590. The port number can be changed during installation, or during configuration after installation. URLs must be of the form http:// http:   localhost:{port}/ localhost:{port}  or http:// http:   127.0.0.1:{port}/. 127.0.0.1:{port} . Note: Do not use the host\u0027s name or IP Address in the URL. Note: Docker users must use the \"host\" networking mode for the container. For information about using EDS with Docker, see Install Edge Data Store using Docker . Data egress Writing data to OSIsoft Cloud Services or OSIsoft PI Web API is not limited to the local machine. You can write data to either of these destinations using HTTPS. EDS adapters Modbus and OPC UA are not limited to the local machine. Both are enabled to access remote data sources through binary protocols. Secure storage Sensitive information such as passwords and client secrets are saved in configuration files in an encrypted form. Only the Edge Data Store runtime can properly store and retrieve these protected data items. Note: Do not manually edit configuration files. Altering encrypted values will cause failures. The only time unencrypted values for sensitive information are available is when you provide them to the system through the REST API, such as with initial configuration or update. From that point forward, the unencrypted values are not available, neither in the configuration files nor through the REST API. The REST API will only return a placeholder string for such values. You should use caution when you submit sensitive data items. For example, remove any temporary file containing unencrypted credentials used to submit configuration to the REST API from the system. Service and file system security The installer creates a specific user account that the Edge Data Store service runs under. You can only use this account for running the service. For example, you cannot use it for interactive sessions. You cannot typically configure this service account. Modifying the service configuration in this respect could cause system failure. The Edge Data Store binary files, configuration files, and data files are configured by the installer and runtime to allow appropriate access by the service account. You do not normally need to modify the permission and ownership assignments for these files, and should not modify them as failures could occur."
                                          },
    "V1/Reference/REST commands.html":  {
                                            "href":  "V1/Reference/REST commands.html",
                                            "title":  "REST commands",
                                            "keywords":  "REST commands The following tables provide an overview of available REST commands that you can use with components of Edge Data Store. Note: The difference between the POST and PUT methods is that POST enables you to create a configuration, while PUT replaces a configuration. If you use POST on an existing configuration, the request will fail. Administration Description Method Endpoint Delete and reset all event and configuration data related to the Edge Data Store component POST http://localhost:5590/api/v1/administration/Storage/Reset http:  localhost:5590 api v1 administration Storage Reset Reset Edge Data Store POST http://localhost:5590/api/v1/administration/System/Reset http:  localhost:5590 api v1 administration System Reset Stop an individual EDS adapter POST http://localhost:5590/api/v1/administration/EDS http:  localhost:5590 api v1 administration EDS adapterId/Stop adapterId Stop Start an individual EDS adapter POST http://localhost:5590/api/v1/administration/EDS http:  localhost:5590 api v1 administration EDS adapterId/Start adapterId Start Configuration Description Method Endpoint Verify correct installation of Edge Data Store Configure minimum Edge Data Store Configure maximum Ede Data Store PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration System Description Method Endpoint Configure system components PUT http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components Configure system port PUT http://localhost:5590/api/v1/configuration/system/port http:  localhost:5590 api v1 configuration system port Configure system logging PUT http://localhost:5590/api/v1/configuration/System/Logging http:  localhost:5590 api v1 configuration System Logging Configure system health endpoints PUT http://localhost:5590/api/v1/configuration/System/HealthEndpoints http:  localhost:5590 api v1 configuration System HealthEndpoints Storage Description Method Endpoint Configure data egress (to either OCS or PI Web API) PUT http://localhost:5590/api/v1/configuration/storage/PeriodicEgressEndpoints/ http:  localhost:5590 api v1 configuration storage PeriodicEgressEndpoints  EDS adapters Note: Substitute the ID number of the adapter that you are configuring, for example OpcUa1 or OpcUa2 or Modbus3 , and so on. OPC UA Description Method Endpoint Configure an OPC UA data source PUT http://localhost:5590/api/v1/configuration/OpcUa1/Datasource http:  localhost:5590 api v1 configuration OpcUa1 Datasource Configure OPC UA data selection PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection Change OPC UA logging configuration PUT http://localhost:5590/api/v1/configuration/OpcUa1/Logging http:  localhost:5590 api v1 configuration OpcUa1 Logging Modbus TCP Description Method Endpoint Configure a Modbus TCP data source PUT http://localhost:5590/api/v1/configuration/Modbus1/Datasource http:  localhost:5590 api v1 configuration Modbus1 Datasource Configure Modbus TCP data selection PUT http://localhost:5590/api/v1/configuration/Modbus1/Datasource http:  localhost:5590 api v1 configuration Modbus1 Datasource Change Modbus TCP logging configuration PUT http://localhost:5590/api/v1/configuration/Modbus1/Logging http:  localhost:5590 api v1 configuration Modbus1 Logging Tenants Types Description Method Endpoint Create an SDS type POST http://localhost:5590/api/v1/tenants/default/namespaces/default/types/Simple http:  localhost:5590 api v1 tenants default namespaces default types Simple Streams Description Method Endpoint Create an SDS stream POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple http:  localhost:5590 api v1 tenants default namespaces default streams Simple View streams that have been created in Storage POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/ http:  localhost:5590 api v1 tenants default namespaces default streams  Write data events to the SDS stream POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data Read last data value written to server POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data Last Read a time range of values written to server. (Example) POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100 http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data?startIndex=2017-07-08T13:00:00Z\u0026count=100 Read last container data value written to the server (using SDS) POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data Last Read a time range of container values written to server (using SDS) (Example) POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100 http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data?startIndex=2017-07-08T13:00:00Z\u0026count=100 OMF Description Method Endpoint Create an OMF type Create an OMF container Write data events to the OMF container POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  Create an OMF container POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf "
                                        },
    "V1/Reference/Reference.html":  {
                                        "href":  "V1/Reference/Reference.html",
                                        "title":  "Reference",
                                        "keywords":  "Reference This topic provides documentation of Sequential Data Store functions. It also includes current Release Notes for the Edge Data Store."
                                    },
    "V1/Administration/Reset the Storage component.html":  {
                                                               "href":  "V1/Administration/Reset the Storage component.html",
                                                               "title":  "Reset the Storage component",
                                                               "keywords":  "Reset the Storage component You can delete and reset all event and configuration data related to the Storage component, after which the product will be restarted. Complete the following to reset the Storage component: Start any Configuration tool capable of making HTTP requests. Execute a POST command to the following endpoint: http://localhost:5590/api/v1/administration/Storage/Reset http:  localhost:5590 api v1 administration Storage Reset Example using curl: curl -v -d \"\" http://localhost:5590/api/v1/Administration/Storage/Reset http:  localhost:5590 api v1 Administration Storage Reset An HTTP status 204 message indicates success."
                                                           },
    "V1/Administration/ManagementTools.html":  {
                                                   "href":  "V1/Administration/ManagementTools.html",
                                                   "title":  "Configuration tools",
                                                   "keywords":  "Configuration tools EdgeCmd utility You can use the EdgeCmd utility on both Linux and Windows to configure and manage Edge Data Store. For more information, see EdgeCmd utility . REST tools The following tools are available to facilitate the execution of REST calls. curl Edge Data Store documentation displays curl commands for configuration and management examples. curl is a command line tool supported on Windows and Linux that is used to make HTTP calls. curl has a large range of capabilities. You can accomplish any Edge Data Store administrative or programming task with curl. curl is also easily scripted, using Bash or PowerShell on either Linux or Windows. OSIsoft recommends this tool for managing Edge Data Store. Any system that can run Edge Data Store supports curl. Postman Postman is an effective REST tool for systems with GUI components. Edge Data Store is supported on platforms that lack this capability. It is particularly useful for learning more about Edge Data Store REST APIs. C#, Python, Go You can use any modern programming language to make REST calls to administer and write programs for Edge Data Store. Since the administrative and programming interfaces are unified in using REST, you can write applications that both manage Edge Data Store and read and write data. For example, you can access the Diagnostics namespace locally to monitor and act upon the local system state. System Tools Many OSIsoft customers use Windows computers, even though they may deploy Linux devices to host Edge Storage. You can install Edge Data Store on Windows 10, and the same custom applications developed on Windows should work on Linux, as long as the application development environment is supported on Linux. Edge Data Store has been designed to use platform independent programming. To facilitate working with Linux devices, Windows tools like PuTTY and WinSCP are very useful for copying files and remotely accessing Linux command lines."
                                               },
    "V1/Feedback.html":  {
                             "href":  "V1/Feedback.html",
                             "title":  "Feedback",
                             "keywords":  "Feedback If you have questions, comments, or other feedback about the Edge Data Store documentation, send an email to documentation@osisoft.com ."
                         },
    "docfx.console.2.43.2/content/articles/intro.html":  {
                                                             "href":  "docfx.console.2.43.2/content/articles/intro.html",
                                                             "title":  "Add your introductions here!",
                                                             "keywords":  "Add your introductions here!"
                                                         },
    "README.html":  {
                        "href":  "README.html",
                        "title":  "Edge-Data-Store-Docs",
                        "keywords":  "Edge-Data-Store-Docs"
                    },
    "V1/SDS/table_format.html":  {
                                     "href":  "V1/SDS/table_format.html",
                                     "title":  "Table format",
                                     "keywords":  "Table format A table is a convenient structure for analytics and display. REST APIs for retrieving multiple events from the data store support returning results in a table. You can set the form variable to specify a table or a table with headers. You can apply table format to any read that returns multiple values and summaries. The following is a request to retrieve values using the window parameters: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z The following response would be returned from the above code: Content-Type: application/json application json [ { \"Time\":\"2017-04-01T07:00:00Z\", \"State\":1 }, { \"Time\":\"2017-04-01T07:01:00Z\", \"State\":1, \"Measurement\":1.0 }, { \"Time\":\"2017-04-01T07:02:00Z\", \"State\":1, \"Measurement\":2.0 }, { \"Time\":\"2017-04-01T07:03:00Z\", \"State\":1, \"Measurement\":3.0 }, { \"Time\":\"2017-04-01T07:04:00Z\", \"State\":1, \"Measurement\":4.0 }, { \"Time\":\"2017-04-01T07:05:00Z\", \"State\":1, \"Measurement\":5.0 }, { \"Time\":\"2017-04-01T07:06:00Z\", \"State\":1, \"Measurement\":6.0 }, { \"Time\":\"2017-04-01T07:07:00Z\", \"State\":1, \"Measurement\":7.0 }, { \"Time\":\"2017-04-01T07:08:00Z\", \"State\":1, \"Measurement\":8.0 }, { \"Time\":\"2017-04-01T07:09:00Z\", \"State\":1, \"Measurement\":9.0 } ] To retrieve the results in table format, add the form variable and specify table . GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z \u0026form=table Response Content-Type: application/json application json { \"Name\":\"Simple\", \"Columns\":[ { \"Name\":\"Time\", \"Type\":\"DateTime\" }, { \"Name\":\"State\", \"Type\":\"Int32Enum\" }, { \"Name\":\"Measurement\", \"Type\":\"Double\" } ], \"Rows\":[ [ \"2017-04-01T07:00:00Z\", 1, 0.0 ], [ \"2017-04-01T07:01:00Z\", 1, 1.0 ], [ \"2017-04-01T07:02:00Z\", 1, 2.0 ], [ \"2017-04-01T07:03:00Z\", 1, 3.0 ], [ \"2017-04-01T07:04:00Z\", 1, 4.0 ], [ \"2017-04-01T07:05:00Z\", 1, 5.0 ], [ \"2017-04-01T07:06:00Z\", 1, 6.0 ], [ \"2017-04-01T07:07:00Z\", 1, 7.0 ], [ \"2017-04-01T07:08:00Z\", 1, 8.0 ], [ \"2017-04-01T07:09:00Z\", 1, 9.0 ] ] } To retrieve the results in table format with column headers, add the form variable and specify tableh . GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-04-01T07:00:00Z\u0026endIndex=2017-04-01T07:10:00Z \u0026form=tableh Response Content-Type: application/json application json { \"Name\":\"Simple\", \"Columns\":[ { \"Name\":\"Time\", \"Type\":\"DateTime\" }, { \"Name\":\"State\", \"Type\":\"Int32Enum\" }, { \"Name\":\"Measurement\", \"Type\":\"Double\" } ], \"Rows\":[ [ \"Time\", \"State\", \"Measurement\" ], [ \"2017-04-01T07:00:00Z\", 1, 0.0 ], [ \"2017-04-01T07:01:00Z\", 1, 1.0 ], [ \"2017-04-01T07:02:00Z\", 1, 2.0 ], [ \"2017-04-01T07:03:00Z\", 1, 3.0 ], [ \"2017-04-01T07:04:00Z\", 1, 4.0 ], [ \"2017-04-01T07:05:00Z\", 1, 5.0 ], [ \"2017-04-01T07:06:00Z\", 1, 6.0 ], [ \"2017-04-01T07:07:00Z\", 1, 7.0 ], [ \"2017-04-01T07:08:00Z\", 1, 8.0 ], [ \"2017-04-01T07:09:00Z\", 1, 9.0 ] ] }"
                                 },
    "V1/SDS/SequentialDataStore.html":  {
                                            "href":  "V1/SDS/SequentialDataStore.html",
                                            "title":  "SDS reference",
                                            "keywords":  "SDS reference Edge Data Store includes the Sequential Data Store (SDS) REST APIs for reading and writing data stored locally on the device where the Edge Data Store is running. SDS is the same technology used in OCS for storing data, so the usage of the REST APIs is very similar to OCS for reading and writing data. All data from all sources on the Edge Data Store (Modbus TCP, OPC UA, OMF, SDS) can be read using the SDS REST APIs on the local device, in the default tenant and the default namespace. In addition, the default tenant has a diagnostics namespace where diagnostic data are written by the Edge Data Store and installed components that can be read to monitor the health of a running system using the SDS REST APIs. The SDS instance running in EDS is an advanced storage engine that is also used in OCS. While it works very well for storing OMF compatible data in EDS, it can also be used for advanced scenarios where data stored in SDS cannot be converted to OMF. All data egress from EDS to both OCS and the PI System uses OMF, so for streams that will be egressed to the PI System or OCS, it is recommended that they have only a single time-based index. Multiple values are supported in a single stream, but for successful egress there is a limitation of a single time-based index only."
                                        },
    "V1/SDS/Searching.html":  {
                                  "href":  "V1/SDS/Searching.html",
                                  "title":  "Searching",
                                  "keywords":  "Searching You can search text and fields across the Sequential Data Store. This topic covers searching for SdsStreams, SdsTypes, and SdsStreamViews. Searching for streams The search functionality for streams is exposed through the REST API. The searchable properties are listed in the following table. Property Searchable Id Yes TypeId Yes Name Yes Description Yes Indexes No InterpolationMode No ExtrapolationMode No PropertyOverrides No Searching for streams is possible using the REST API and specifying the optional query parameter, as shown here: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} Streams?query={query}\u0026skip={skip}\u0026count={count} The Stream fields valid for search are identified in the fields table located on the Streams page. Searching for types Similarly, the search functionality for types is exposed through REST API. The query syntax and request parameters are the same. The only difference is the searchable properties differ for types as opposed to streams. Searchable properties are listed in the following table. For more information, see Types . Property Searchable Id Yes Name Yes Description Yes SdsTypeCode No InterpolationMode No ExtrapolationMode No Properties Yes, with limitations Searching for types is also possible using the REST API and specifying the optional query parameter, as shown here: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Types?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} Types?query={query}\u0026skip={skip}\u0026count={count} The Type fields valid for search are identified in the fields table located on the Types page. The Properties field is identified as being searchable but with limitations: Each SdsTypeProperty of a given SdsType has its name and Id included in the Properties field. This includes nested SdsTypes of the given SdsType. Therefore, the searching of properties will distinguish SdsTypes by their respective lists of relevant SdsTypeProperty Ids and names. Searching for stream views Similarly, the search functionality for stream views is exposed through REST API. The query syntax and the request parameters are the same. The only difference is the resource you are searching on. You can match on different properties for stream views than for streams and types. The searchable properties are listed in the following table. For more information, see Stream Views . Property Searchable Id Yes Name Yes Description Yes SourceTypeId Yes TargetTypeId Yes Properties Yes, with limitations As previously mentioned, searching for stream views is also possible using the REST API and specifying the optional query parameter, as shown here: GET api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews?query={query}\u0026skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} StreamViews?query={query}\u0026skip={skip}\u0026count={count} The Stream View fields valid for search are identified in the fields table located on the Stream Views page. The Properties field is identified as being searchable with limitations because SdsStreamViewProperty objects are not searchable. Only the SdsStreamViewProperty\u0027s SdsStreamView is searchable by its Id, SourceTypeId, and TargetTypeId, which are used to return the top level SdsStreamView object when searching. This includes nested SdsStreamViewProperties. How searching works By default, the query parameter is applied across all searchable fields of objects that are searched. For example, you can assume that a namespace contains the following streams: streamId Name Description stream1 tempA The temperature from DeviceA stream2 pressureA The pressure from DeviceA stream3 calcA calculation from DeviceA values Using the stream data above, the following table shows the results of a call to get streams with different Query values: QueryString Streams returned temperature Only stream1 returned. calc* Only stream3 returned. DeviceA* All three streams returned. humidity* No streams returned. The skip and count parameters determine which items are returned when a large number of them match the query criteria. count indicates the maximum number of items returned. The maximum value of the count parameter is 1000. skip indicates the number of matched items to skip over before returning matching items. You use the skip parameter when more items match the search criteria than can be returned in a single call. The orderby parameter is supported for searching both streams and types. It returns the result in sorted order. The default value for orderby parameter is ascending order. You can change it to descending order by specifying desc alongside the orderby field value. It can be used in conjunction with query , skip , and count parameters. Request The following examples show various ways to use the orderby parameter. GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=id asc GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name desc GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure\u0026orderby=name desc\u0026skip=10\u0026count=20 Search operators You can specify search operators in the query string to return more specific search results. Operators Description AND AND operator. For example, cat AND dog searches for streams containing both \"cat\" and \"dog\". AND must be in all caps. OR OR operator. For example, cat OR dog searches for streams containing either \"cat\" or \"dog\" or both. OR must be in all caps. NOT NOT operator. For example, cat NOT dog searches for streams that have the \"cat\" term or do not have \"dog\". NOT must be in all caps. * Wildcard operator. For example, cat* searches for streams that have a term that starts with \"cat\", ignoring case. : Field-scoped query. For example, id:stream* will search for streams where the id field starts with \"stream\", but will not search on other fields like name or description . Note: Field names are camel case and are case sensitive. ( ) Precedence operator. For example, motel AND (wifi OR luxury) searches for streams containing motel and either wifi or luxury (or both). Note: Regarding wildcard operator * , you can use the wildcard * only once for each search term, except for the case of a Contains type query clause. In that case, two wildcards are allowed: one as prefix and one as suffix for example, *Tank* is valid but *Ta*nk , Ta*nk* , and *Ta*nk* are not supported. The wildcard * only works when specifying a single search term. For example, you can search for Tank* , *Tank , Ta*nk but not Tank Meter* . : Operator You can determine which fields are searched by using the following syntax: fieldname:fieldvalue Request The following examples shows the \u0027:\u0027 operator. GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=name:pump api v1 Tenants default Namespaces {namespaceId} Streams?query=name:pump name:pressure * Operator You can use the \u0027*\u0027 character as a wildcard to specify an incomplete string. Query string Matches field value Does not match field value log* log logger analog *log analog alog logg *log* analog alogger lop l*g log logg lop Supported Not Supported * *log l*g log* *log* *l*g* *l*g l*g* Request The following examples shows the \u0027*\u0027 operator. GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query=log* api v1 Tenants default Namespaces {namespaceId} Streams?query=log* Other operator examples Query string Matches field value Does not match field value mud AND log log mud mud log mud log mud OR log log mud mud log mud AND (NOT log) mud mud log mud AND (log OR pump*) mud log mud pumps mud bath name:stream* AND (description:pressure OR description:pump) The name starts with \"stream\" and the description includes either term \"pressure\" or term \"pump\""
                              },
    "V1/Egress/Egress.html":  {
                                  "href":  "V1/Egress/Egress.html",
                                  "title":  "Data egress configuration",
                                  "keywords":  "Data egress configuration Edge Data Store provides an egress mechanism to copy and transfer data to another device or destination. Data is transferred through OMF. Supported destinations are OSIsoft Cloud Services or a PI Server. Configuration of egress includes specifying zero or more endpoints. An egress endpoint represents a destination to which data will be sent. Each egress endpoint is comprised of the properties specified in the Parameters section. It is executed independently of all other egress endpoints, and is expected to accept OMF messages. More than one endpoint for the same destination is allowed. Note: Some types, and consequently containers and data, cannot be egressed. For more information, see Egress Execution Details . Edge Data Store supports one tenant and two namespaces . The EDS tenant name is default, and the two namespaces are default (where adapter and OMF data is written) and diagnostics. Diagnostics is where the system and its components write information that can be used locally or egressed to a remote PI server or OCS for monitoring. You must create a separate egress definition for each namespace from which you want to egress data. The other topics in this section provide instructions for the sequential steps required to egress EDS data to a PI server or OCS: Prepare egress destinations in either a PI server or OCS. You will use the information produced by this action to define the endpoint in your egress configuration file. Create an egress configuration file for each specific endpoint to which you want to egress data."
                              },
    "V1/Egress/Egress execution details.html":  {
                                                    "href":  "V1/Egress/Egress execution details.html",
                                                    "title":  "Egress execution details",
                                                    "keywords":  "Egress execution details After you add configuration for an egress endpoint, egress execution will periodically occur for that endpoint. Egress is handled individually per configured endpoint. Only the first execution types and containers will be egressed; subsequently only new or changed types/containers types containers will be egressed. Note: Only streams with a single, timeseries-based index can be egressed. Type creation must be successful to perform container creation; likewise, container creation must be successful to perform data egress. Type, container and data items are batched into one or more OMF messages when egressing. Per the requirements defined in OMF, a single message will not exceed 192KB in size. Compression is automatically applied to outbound egress messages. On the destination, failure to add a single item will result in the message failing. In that case the Edge Data Store will fall back to egressing each item individually, per type or stream (that is each type, each stream, all data for a single stream). Types, containers, and data will continue to be egressed as long as the destination continues to respond to HTTP requests - retrying previous failures as needed. Certain HTTP failures during egress will result in a retry. Edge Data Store will retry an HTTP request a maximum of five times with exponentially increasing delays between each request. The total time waiting and retrying is currently set at 1 minute. During that time egress of other messages will be delayed. The following are retriable occurrences: TimeoutException HttpRequestException HttpStatusCode RequestTimeout (408) HttpStatusCode BadGateway (502) HttpStatusCode ServiceUnavailable (503) HttpStatusCode GatewayTimeout (504) For data collection and egress, in-memory and on-disk storage are used to track the last successfully egressed data event, per stream. Data is egressed in order and includes future events. Note: When an event with a future timestamp is successfully egressed, only values after the associated timestamp of that event will be egressed."
                                                },
    "V1/Egress/Configure data egress.html":  {
                                                 "href":  "V1/Egress/Configure data egress.html",
                                                 "title":  "Configure data egress",
                                                 "keywords":  "Configure data egress After you have configured one or more OMF destinations, you can configure data egress. Note: You cannot add egress configurations manually because some parameters are stored to disk encrypted. You must use the REST endpoints to add/edit add edit egress configuration. For additional endpoints, see REST URLs . Create egress endpoints Complete the following procedure to create new egress endpoints: Create a JSON file containing one or more egress endpoints. For content structure, see the following Examples . Update the parameters as needed. For a table of all available parameters, see Parameters . Save the file. Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/storage/periodicegressendpoints/ http:  localhost:5590 api v1 configuration storage periodicegressendpoints  Example using cURL: curl -v -d \"@Storage_PeriodicEgressEndspoints.config.json\" -H \"Content-Type: application/json\" application json\" \"http://localhost:5590/api/v1/configuration/storage/periodicegressendpoints\" \"http:  localhost:5590 api v1 configuration storage periodicegressendpoints\" Note The @ symbol is a required prefix for the above command. Parameters Parameter Required Type Description Backfill Optional bool An indicator of whether data should be backfilled. Enabling the backfill flag will result in all data from the earliest index to the latest stored index being egressed. Backfilling occurs for each stream, including when a new stream is added. Once backfilling is complete for a stream, any out-of-order data is not egressed. Defaults to false. ClientId Required for OCS endpoint string Used for authentication with the OCS OMF endpoint. ClientSecret Required for OCS endpoint string Used for authentication with the OCS OMF endpoint. DebugExpiration Optional string A property that enables persistence of detailed information, for each outbound HTTP request pertaining to this egress endpoint, to disk. The value of this property represents the date and time this detailed information should stop being persisted. Examples of valid strings representing date and time: UTC: ???yyyy-mm-ddThh:mm:ssZ???, Local: ???mm-dd-yyyy hh:mm:ss???. For more information, see Troubleshooting . Description Optional string Friendly description EgressFilter Optional string A filter used to determine which streams and types are egressed. For more information on valid filters, see Searching . Enabled Optional Boolean An indicator of whether egress is enabled when the egress endpoint is loaded. Defaults to true. Endpoint Required string Destination that accepts OMF v1.1 messages. Supported destinations include OCS and PI. ExecutionPeriod Required string Frequency of time between each egress action. Must be a string in the format d.hh:mm:ss.##. Id Optional string Unique identifier Name Optional string Friendly name NamespaceId Optional string Represents the namespace that will be egressed. There are two available namespaces: default and diagnostics. Default namespace is ???default???. Password Required for PI endpoint string Used for Basic authentication to the PI Web API OMF endpoint. StreamPrefix Optional string Prefix applied to any streams that are egressed. A null string or a string containing only empty spaces will be ignored. The following restricted characters will not be allowed: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % TokenEndpoint Optional for OCS endpoint string Used to retrieve an OCS token from an alternative endpoint. This is not normally necessary with OCS. Only use if directed to do so by customer support . TypePrefix Optional string Prefix applied to any types that are egressed. A null string or a string containing only empty spaces will be ignored. The following restricted characters will not be allowed: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % Username Required for PI endpoint string Used for Basic authentication to the PI Web API OMF endpoint. If domain is required, the backslash must be escaped (i.e., domain \\\\ username ). ValidateEndpointCertificate Optional bool Used to disable verification of destination certificate. Use for testing only with self-signed certificates. Defaults to true. Examples The following are valid egress configuration examples. Egress data to OCS. All streams, every 15 seconds. [{ \"Id\": \"OCS\", \"ExecutionPeriod\" : \"00:00:15\", \"Endpoint\" : \"https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", \"https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\" : \"{clientId}\", \"ClientSecret\" : \"{clientSecret}\" }] Egress data to OCS - streams with a specific TypeId value, every 15 seconds. [{ \"Id\": \"OCS\", \"ExecutionPeriod\" : \"00:00:15\", \"EgressFilter\" : \"TypeId:myType\", \"Endpoint\" : \"https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", \"https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\" : \"{clientId}\", \"ClientSecret\" : \"{clientSecret}\" }] Egress data to OCS - all streams, every 15 seconds, including backfilling. [{ \"Id\": \"OCS\", \"ExecutionPeriod\" : \"00:00:15\", \"Backfill\" : true, \"Endpoint\" : \"https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", \"https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\" : \"{clientId}\", \"ClientSecret\" : \"{clientSecret}\" }] Egress diagnostic data to OCS - every 1 hour. [{ \"Id\": \"OCS\", \"ExecutionPeriod\" : \"01:00:00\", \"Endpoint\" : \"https://{OcsLocation}/api/Tenants/{tenantId}/Namespaces/{namespaceId}/omf\", \"https:  {OcsLocation} api Tenants {tenantId} Namespaces {namespaceId} omf\", \"ClientId\" : \"{clientId}\", \"ClientSecret\" : \"{clientSecret}\", \"NamespaceId\" : \"diagnostics\" }] Egress data to PI - all streams, every 15 seconds, including both type and stream prefix. All properties explicitly listed. [{ \"Id\": \"PI\", \"Name\" : null, \"Description\" : null, \"ExecutionPeriod\" : \"00:00:15\", \"Enabled\" : true, \"Backfill\" : false, \"EgressFilter\" : null, \"Endpoint\" : \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"ClientId\" : null, \"ClientSecret\" : null, \"Username\" : \"{username}\", \"Password\" : \"{password}\", \"StreamPrefix\" : \"1ValidPrefix.\", \"TypePrefix\" : \"AlsoValid_\", \"DebugExpiration\" : null, \"NamespaceId\" : \"default\", \"TokenEndpoint\" : null, \"ValidateEndpointCertificate\" : true }] Egress data to PI - streams whose Id contains \"Modbus\" or \"Opc\", every 1 minute. Includes use of domain for username. [{ \"Id\": \"PI\", \"ExecutionPeriod\" : \"00:01:00\", \"EgressFilter\" : \"Id:*Modbus* OR Id:*Opc*\", \"Endpoint\" : \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"Username\" : \"{domain}\\\\{username}\", \"Password\" : \"{password}\" }] Egress data to PI - streams containing a field that begins with \"Unique\", every 1 hour. [{ \"Id\": \"PI\", \"ExecutionPeriod\" : \"01:00:00\", \"EgressFilter\" : \"Unique*\", \"Endpoint\" : \"https://{webApiLocation}/piwebapi/omf/\", \"https:  {webApiLocation} piwebapi omf \", \"Username\" : \"{username}\", \"Password\" : \"{password}\" }] REST URLs Relative URL HTTP verb Action api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints GET Gets all configured egress endpoints. api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints DELETE Deletes all configured egress endpoints. api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints POST Adds an array of egress endpoints, fails if any endpoint already exists. api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints POST Adds a single egress endpoint, fails if endpoint already exists. api/v1/configuration/storage/periodicegressendpoints api v1 configuration storage periodicegressendpoints PUT Replaces all egress endpoints. api/v1/configuration/storage/periodicegressendpoints/{id} api v1 configuration storage periodicegressendpoints {id} GET Gets configured endpoint with id . api/v1/configuration/storage/periodicegressendpoints/{id} api v1 configuration storage periodicegressendpoints {id} DELETE Deletes configured endpoint with id . api/v1/configuration/storage/periodicegressendpoints/{id} api v1 configuration storage periodicegressendpoints {id} PUT Replaces egress endpoint with id , fails if endpoint does not exist. api/v1/configuration/storage/periodicegressendpoints/{id} api v1 configuration storage periodicegressendpoints {id} PATCH Allows partial updating of configured endpoint with id ."
                                             },
    "V1/Docker/EdgeDocker.html":  {
                                      "href":  "V1/Docker/EdgeDocker.html",
                                      "title":  "Install Edge Data Store using Docker",
                                      "keywords":  "Install Edge Data Store using Docker Docker is a set of tools that can be used on Linux to manage application deployments. If you want to use Docker, you must be familiar with the underlying technology and have determined that it is appropriate for your planned use of the Edge Data Store. Docker is not a requirement to use Edge Data Store. This topic provides examples of how to create a Docker container with the Edge Data Store. Create a Docker container containing the Edge Data Store Create the following Dockerfile in the directory where you want to create and run the container: ARM32 ARM64 AMD64 (x64) FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends libicu60 libssl1.0.0 ADD ./EdgeDataStore_linux-arm.tar.gz . EdgeDataStore_linux-arm.tar.gz . ENTRYPOINT [\"./EdgeDataStore_linux-arm/OSIsoft.Data.System.Host\"] [\". EdgeDataStore_linux-arm OSIsoft.Data.System.Host\"] FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends libicu60 libssl1.0.0 ADD ./EdgeDataStore_linux-arm64.tar.gz . EdgeDataStore_linux-arm64.tar.gz . ENTRYPOINT [\"./EdgeDataStore_linux-arm64/OSIsoft.Data.System.Host\"] [\". EdgeDataStore_linux-arm64 OSIsoft.Data.System.Host\"] FROM ubuntu WORKDIR /   RUN apt-get update \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends libicu60 libssl1.0.0 ADD ./EdgeDataStore_linux-x64.tar.gz . EdgeDataStore_linux-x64.tar.gz . ENTRYPOINT [\"./EdgeDataStore_linux-x64/OSIsoft.Data.System.Host\"] [\". EdgeDataStore_linux-x64 OSIsoft.Data.System.Host\"] Copy the EdgeDataStore_linux-arm.tar.gz file to the same directory as the Dockerfile. Run the following command line (sudo may be necessary): docker build -t edgedatastore Run the Edge Data Store Docker containers REST access from the local machine from Docker Complete the following to run the container: Open command line. Type the following in the command line (sudo may be necessary): docker run -d --network host edgedatastore Port 5590 is accessible from the host and you can make REST calls to Edge Data Store from applications on the local host computer. In this example, all data stored by the Edge Data Store is stored in the container itself. When the container is deleted, the data stored is also deleted. Persistent storage on the local file system from Docker Complete the following to run the container: Open command line. Type the following in the command line (sudo may be necessary): docker run -d --network host -v /edgeds:/usr/share/OSIsoft/  edgeds: usr share OSIsoft  edgedatastore Port 5590 is accessible from the host and you can make REST calls to Edge Data Store from applications on the local host computer. In this example, all data that would be written to the container is instead written to the host directory. In this example the host directory is a simple directory on the local machine, /edgeds.  edgeds. You can specify any directory you want. Port number change To use a different port other than 5590, see System port configuration . Changing the configuration of the Edge Data Store running in the container changes the port exposed to the local machine. Limiting local host access to Docker If you remove the --network host option from the docker run command, no REST access is possible from outside the container. This may be of value where you want to host an application in the same container as Edge Data Store, and do not want to have external REST access enabled."
                                  },
    "V1/SDS/Writing_Data_API.html":  {
                                         "href":  "V1/SDS/Writing_Data_API.html",
                                         "title":  "API calls for writing data",
                                         "keywords":  "API calls for writing data Example type, stream, and data Many of the following API methods descriptions contain example requests and responses in JSON to highlight usage and specific behaviors. The following type, stream, and data are used in the examples: Example type SimpleType is an SdsType with a single index and two additional properties. This type is defined in Python and Javascript: Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class SimpleType(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2, } var SimpleType = function () { this.Time = null; this.State = null; this.Value = null; } Example stream Simple is an SdsStream of type SimpleType . Example data Simple has stored values as follows: 11/23/2017 11 23 2017 12:00:00 PM: Ok 0 11/23/2017 11 23 2017 1:00:00 PM: Ok 10 11/23/2017 11 23 2017 2:00:00 PM: Ok 20 11/23/2017 11 23 2017 3:00:00 PM: Ok 30 11/23/2017 11 23 2017 4:00:00 PM: Ok 40 All times are represented at offset 0, GMT. Insert Values Inserts data into the specified stream. Returns an error if data is already present at the index of any event. Request POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Request body A serialized list of one or more events of the stream type. Response The response includes a status code. Note: This request will return an error if an event already exists for any index in the request. If any individual index encounters a problem, the entire operation is rolled back and no insertions are made. The streamId and index that caused the issue are included in the error response. Example The following request is used to insert events into stream Simple of SimpleType , POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data The request body specifies the values to insert: [ { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T18:00:00Z\", \"State\": 0, \"Measurement\": 60 } ] Patch Values Modifies the specified stream event(s). Patching affects only the data item parameters that are included in the call. Request PATCH api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?select={selectExpression} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string selectExpression Comma-separated list of strings that indicates the event fields to be changed in stream events. Request body A serialized collection of one or more patch property events. Response The response includes a status code. Consider you have a stream Simple of SimpleType , to change one property, Measurement , for one event specify the following request: PATCH api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?select=measurement With the following request body, [ { \"Time\":\"2017-11-23T12:00:00Z\", \"Measurement\":500.0 } ] This request will only change the Measurement value at the specified event index. Note: Patching is used to patch the events of the selected fields for one or more events in the stream. Only the fields indicated in selectExpression are modified. The events to be modified are indicated by the index value of each entry in the collection. If there is a problem patching any individual event, the entire operation is rolled back and the error will indicate the streamId and index of the problem. Remove Values There are two options for specifying which events to remove from a stream: Index Collection : One or more indexes can be specified in the request. Window : A window can be specified with a start index and end index. Index Collection Removes the event at each index from the specified stream. Different overloads are available to make it easier to indicate the index where you want to remove a data event. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?index={index}[\u0026index={index}???] Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string index One or more indexes of events to remove. Response The response includes a status code. Note: If any individual event fails to be removed, the entire operation is rolled back and no events are removed. The streamId and index that caused the issue are included in the error response. If you attempt to remove events at indexes that have no events, an error is returned. If this occurs, you can use Window request format to remove any events from a specified ???window??? of indexes, which will not return an error if no data is found. Window Removes events at and between the start index and end index. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026endIndex={endIndex} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string startIndex The index defining the beginning of the window. string endIndex The index defining the end of the window. Response The response includes a status code. Note: If any individual event fails to be removed, the entire operation is rolled back and no removes are done. Replace Values Writes one or more events over existing events in the specified stream. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?allowCreate=false Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Request body A serialized list of one or more events of the stream type. Response The response includes a status code. Note: This request returns an error if the stream does not have an event to be replaced at the specified index. If any individual event fails to be replaced, the entire operation is rolled back and no replaces are performed. The index that caused the issue and the streamId are included in the error response. Update Values Writes one or more events to the specified stream. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Request body A serialized list of one or more events of the stream type. Response The response includes a status code. Note: This request performs an insert or a replace depending on whether an event already exists at the event indexes. If any item fails to write, the entire operation is rolled back and no events are written to the stream. The index that caused the issue is included in the error response."
                                     },
    "V1/SDS/Writing_Data.html":  {
                                     "href":  "V1/SDS/Writing_Data.html",
                                     "title":  "Writing data",
                                     "keywords":  "Writing data The SDS REST APIs provide programmatic access to read and write SDS data. This topic describes the APIs used to write SdsStream data. All writes rely on a stream???s key or primary index. The primary index determines the order of events in the stream. Secondary indexes are updated, but they do not contribute to the request. All references to indexes are to the primary index. Single stream writes The following support writing multiple values: Insert Values inserts a collection of events. Patch Values updates specific fields for a collection of events. Replace Values replaces a collection of events. Remove Values deletes the events based on the request parameters. Update Values add or replaces a collection of events. The base URI for writing SDS data to a single stream is: api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Request body format With the exception of Remove Values, all single stream write calls require a request body containing the events to insert or modify. The events must be formatted as a serialized JSON array of the stream\u0027s type. JSON arrays are comma-delimited lists of a type enclosed within square brackets. The following code shows a list of three WaveData events that are properly formatted for insertion. For the complete example, see the OCS-Samples . [ { \"Order\":2, \"Tau\":0.25722883666666846, \"Radians\":1.6162164471269089, \"Sin\":1.9979373673043652, \"Cos\":-0.090809010174665111, \"Tan\":-44.003064529862513, \"Sinh\":4.8353589272389, \"Cosh\":5.2326566823391856, \"Tanh\":1.8481468289554672 }, { \"Order\":4, \"Tau\":0.25724560000002383, \"Radians\":1.6163217742567466, \"Sin\":1.9979277915696148, \"Cos\":-0.091019446679060964, \"Tan\":-43.901119254534827, \"Sinh\":4.8359100947709592, \"Cosh\":5.233166005842703, \"Tanh\":1.8481776000882766 }, { \"Order\":6, \"Tau\":0.25724560000002383, \"Radians\":1.6163217742567466, \"Sin\":1.9979277915696148, \"Cos\":-0.091019446679060964, \"Tan\":-43.901119254534827, \"Sinh\":4.8359100947709592, \"Cosh\":5.233166005842703, \"Tanh\":1.8481776000882766 } ] You can serialize your data using one of many available JSON serializers available at Introducing JSON . Response format Supported response formats include JSON, verbose JSON, and SDS. The default response format for SDS is JSON, which is used in all examples in this documentation. Default JSON responses do not include any values that are equal to the default value for their type. Verbose JSON responses include all values in the returned JSON payload, including defaults. To specify verbose JSON return, add the header Accept-Verbosity with a value of verbose to the request. Verbose has no impact on writes; writes return only error messages. To specify SDS format, set the Accept header in the request to application/sds application sds . Indexes SDS writes rely on the primary index for positioning within streams and locating existing events. Most writes use the index as specified by the value. Deletes are the exception to this rule. When deleting, indexes are specified as strings in the URI. For more details about working with indexes, see the Indexes page. To specify compound indexes in the URI, specify each field that composes the index, in the specified order, separated by the pipe character, ???|???."
                                 },
    "V1/SDS/Units_of_Measure.html":  {
                                         "href":  "V1/SDS/Units_of_Measure.html",
                                         "title":  "Units of measure",
                                         "keywords":  "Units of measure The Sequential Data Store (SDS) provides a collection of built-in units of measure (Uom). These units of measure can be associated with SdsStreams and SdsTypes to provide unit information for stream data that model measurable quantities. If data has unit information associated with it, SDS is able to support unit conversions when retrieving data. For more information, see Reading data . Since a unit of measurement (such as meter) defines the magnitude of a quantity (such as Length), SDS represents this through two objects: SdsUom and SdsUomQuantity. SdsUom An SdsUom represents a single unit of measure, such as \u0027meter\u0027. The following table shows the required and optional SdsUom fields. Property Type Optionality Description Example Id String Required Unique identifier for the unit of measure meters per second Abbreviation String Optional Abbreviation for the unit of measure m/s m s Name String Optional Full name for the unit of measure Meters per second DisplayName String Optional Friendly display name for the unit of measure meters per second QuantityId String Required The identifier for the quantity that this unit of measure quantifies Velocity ConversionFactor Double Required Used for unit conversions. When a value of this unit is multiplied by the ConversionFactor and then incremented by the ConversionOffset, the value in terms of the base unit of the corresponding quantity is returned. 1.0 ConversionOffset Double Required Used for unit conversions. See details for ConversionFactor. 0.0 SdsUomQuantity Represents a single measurable quantity, such as length). The following table shows the required and optional SdsUomQuantity fields. Property Type Optionality Description Example Id String Required Unique identifier for the quantity Velocity Name String Optional Full name for the quantity Velocity BaseUom SdsUom Required The base unit of measure for this quantity. All other Uoms measuring this quantity will have ConversionFactors and ConversionOffsets relative to the BaseUom. SdsUom representing \"meters per second\" Dimensions short[] Optional Reserved for internal use. Represents the seven base SI dimensions: Length, Mass, Time, Electric Current, Thermodynamic Temperature, Amount of Substance, and Luminous Density. [1,0,-1,0,0,0,0] Supported quantities A list of the supported quantities and their base unit of measures follows. Supported quantities are read-only. Quantity Id Base Uom Id Angular Velocity radian per second Area square meter Computer Storage byte Density kilogram per cubic meter Dynamic Viscosity pascal second Electric Charge coulomb Electric Current ampere Electric Potential volt Electric Resistance ohm Energy joule Entropy and Heat Capacity joule per kelvin Force newton Frequency hertz Length meter Luminous Intensity candela Mass kilogram Mass Flow Rate kilogram per second Molar Flow Rate mole per second Molecular Weight kilogram per mole Amount of Substance mole Plane Angle radian Power watt Pressure pascal Quantity count Ratio percent Specific Energy joule per kilogram Specific Entropy and Specific Heat Capacity joule per kilogram kelvin Specific Volume cubic meter per kilogram Speed meter per second Temperature kelvin Temperature (Delta) delta kelvin Time second Volume cubic meter Volume Flow Rate cubic meter per second Supported units of measure A list of the supported units of measure follows. Supported units of measure are read-only. Uom Id Abbreviation Quantity Id Conversion Factor Conversion Offset count count Quantity 1 0 Ampere hour Ah Electric Charge 3600 0 coulomb C Electric Charge 1 0 kilogram per second kg/s kg s Mass Flow Rate 1 0 long ton per day lton/d lton d Mass Flow Rate 0.011759802 0 million pound per day MMlb/d MMlb d Mass Flow Rate 5.24991169 0 short ton per day ston/d ston d Mass Flow Rate 0.010499823 0 thousand pound per day klb/d klb d Mass Flow Rate 0.005249912 0 gram per second g/s g s Mass Flow Rate 0.001 0 pound per second lb/s lb s Mass Flow Rate 0.45359237 0 tonne per day t/d t d Mass Flow Rate 0.011574074 0 long ton lton Mass 1016.046909 0 million pound MM lb Mass 453592.37 0 ounce oz Mass 0.028349523 0 short ton ston Mass 907.18474 0 thousand pound klb Mass 453.59237 0 ton ton Mass 907.18474 0 gram g Mass 0.001 0 milligram mg Mass 1.00E-06 0 pound lb Mass 0.45359237 0 tonne t Mass 1000 0 kilogram kg Mass 1 0 second s Time 1 0 hour h Time 3600 0 day d Time 86400 0 month month Time 2628000 0 week week Time 604800 0 year yr Time 31536000 0 minute min Time 60 0 dyne dyne Force 1.00E-05 0 kilogram-force kgf Force 9.80665 0 pound-force lbf Force 4.448221615 0 newton N Force 1 0 watt W Power 1 0 million British thermal unit per day MM Btu/d Btu d Power 12211.29459 0 million British thermal unit per hour MM Btu/h Btu h Power 293071.0702 0 gigawatt GW Power 1000000000 0 megawatt MW Power 1000000 0 British thermal unit per hour Btu/h Btu h Power 0.29307107 0 calorie per second cal/s cal s Power 4.1868 0 horsepower hp Power 745.6998716 0 joule per second J/s J s Power 1 0 kilowatt kW Power 1000 0 megajoule per hour MJ/h MJ h Power 277.7777778 0 million calorie per hour MMcal/h MMcal h Power 1163 0 mole per second mol/s mol s Molar Flow Rate 1 0 gram mole per second gmol/s gmol s Molar Flow Rate 1 0 kilogram mole per second kmol/s kmol s Molar Flow Rate 1000 0 pound mole per second lbmol/s lbmol s Molar Flow Rate 453.59237 0 meter m Length 1 0 centimeter cm Length 0.01 0 inch in Length 0.0254 0 International nautical mile nmi Length 1852 0 kilometer km Length 1000 0 millimeter mm Length 0.001 0 foot ft Length 0.3048 0 mile mi Length 1609.344 0 sixteenth of an inch sxi Length 0.0015875 0 yard yd Length 0.9144 0 candela cd Luminous Intensity 1 0 meter per second m/s m s Speed 1 0 centimeter per second cm/s cm s Speed 0.01 0 foot per second ft/s ft s Speed 0.3048 0 International nautical mile per hour nmi/h nmi h Speed 0.514444444 0 kilometer per hour km/h km h Speed 0.277777778 0 mile per hour mi/h mi h Speed 0.44704 0 revolution per minute rpm Angular Velocity 0.104719755 0 radian per second rad/s rad s Angular Velocity 1 0 barrel per day bbl/d bbl d Volume Flow Rate 1.84E-06 0 cubic centimeter per second cm3/s cm3 s Volume Flow Rate 1.00E-06 0 cubic foot per second ft3/s ft3 s Volume Flow Rate 0.028316847 0 cubic meter per hour m3/h m3 h Volume Flow Rate 0.000277778 0 Imperial gallon per minute Imp gal/min gal min Volume Flow Rate 7.58E-05 0 liter per second L/s L s Volume Flow Rate 0.001 0 US gallon per minute US gal/min gal min Volume Flow Rate 6.31E-05 0 cubic meter per second m3/s m3 s Volume Flow Rate 1 0 pascal Pa Pressure 1 0 atmosphere atm Pressure 101325 0 bar bar Pressure 100000 0 inches of mercury inHg Pressure 3386.388158 0 kilogram-force per square centimeter kgf/cm2 kgf cm2 Pressure 98066.5 0 kilogram-force per square meter kgf/m2 kgf m2 Pressure 9.80665 0 kilopascal kPa Pressure 1000 0 millimeter of mercury mmHg Pressure 133.3223684 0 newton per square meter N/m2 N m2 Pressure 1 0 pound-force per square inch psi Pressure 6894.757293 0 pound-force per square inch (customary) psia Pressure 6894.757293 0 torr torr Pressure 133.3223684 0 square meter m2 Area 1 0 square foot ft2 Area 0.09290304 0 acre acre Area 4046.856422 0 square mile mi2 Area 2589988.11 0 square yard yd2 Area 0.83612736 0 hectare ha Area 10000 0 square centimeter cm2 Area 0.0001 0 square inch in2 Area 0.00064516 0 square kilometer km2 Area 1000000 0 square millimeter mm2 Area 1.00E-06 0 yobibyte YiB Computer Storage 1.21E+24 0 zebibyte ZiB Computer Storage 1.18E+21 0 exbibyte EiB Computer Storage 1.15E+18 0 pebibyte PiB Computer Storage 1.13E+15 0 tebibyte TiB Computer Storage 1.10E+12 0 gibibyte GiB Computer Storage 1073741824 0 mebibyte MiB Computer Storage 1048576 0 kibibyte KiB Computer Storage 1024 0 yottabyte YB Computer Storage 1.00E+24 0 zettabyte ZB Computer Storage 1.00E+21 0 exabyte EB Computer Storage 1.00E+18 0 petabyte PB Computer Storage 1.00E+15 0 terabyte TB Computer Storage 1.00E+12 0 gigabyte GB Computer Storage 1000000000 0 megabyte MB Computer Storage 1000000 0 kilobyte kB Computer Storage 1000 0 byte B Computer Storage 1 0 kelvin K Temperature 1 0 degree Celsius ??C Temperature 1 273.15 degree Rankine ??R Temperature 0.555555556 -2.56E-13 degree Fahrenheit ??F Temperature 0.555555556 255.3722222 milliampere mA Electric Current 0.001 0 ampere A Electric Current 1 0 joule per gram J/g J g Specific Energy 1000 0 joule per kilogram J/kg J kg Specific Energy 1 0 British thermal unit per pound Btu/lb Btu lb Specific Energy 2326 0 kilocalorie per kilogram kcal/kg kcal kg Specific Energy 4186.8 0 kilojoule per kilogram kJ/kg kJ kg Specific Energy 1000 0 kilojoule per pound kJ/lb kJ lb Specific Energy 2204.622622 0 British thermal unit per degree Rankine Btu/??R Btu ??R Entropy and Heat Capacity 1899.100535 0 British thermal unit per degree Fahrenheit Btu/??F Btu ??F Entropy and Heat Capacity 1899.100535 0 kilojoule per kelvin kJ/K kJ K Entropy and Heat Capacity 1000 0 joule per kelvin J/K J K Entropy and Heat Capacity 1 0 cubic foot per pound ft3/lb ft3 lb Specific Volume 0.062427961 0 cubic centimeter per gram cm3/g cm3 g Specific Volume 0.001 0 cubic meter per kilogram m3/kg m3 kg Specific Volume 1 0 hertz Hz Frequency 1 0 mole mol Amount of Substance 1 0 gram mole gmol Amount of Substance 1 0 kilogram mole kmol Amount of Substance 1000 0 pound mole lbmol Amount of Substance 453.59237 0 percent % Ratio 1 0 parts per billion ppb Ratio 1.00E-07 0 parts per million ppm Ratio 0.0001 0 ohm ?? Electric Resistance 1 0 gram per gram mole g/gmol g gmol Molecular Weight 0.001 0 pound per pound mole lb/lbmol lb lbmol Molecular Weight 0.001 0 kilogram per mole kg/mol kg mol Molecular Weight 1 0 kilogram per kilogram mole kg/kmol kg kmol Molecular Weight 0.001 0 British thermal unit per pound degree Rankine Btu/(lb Btu (lb ??R) Specific Entropy and Specific Heat Capacity 4186.8 0 British thermal unit per pound degree Fahrenheit Btu/(lb Btu (lb ??F) Specific Entropy and Specific Heat Capacity 4186.8 0 joule per gram kelvin J/(g J (g K) Specific Entropy and Specific Heat Capacity 1000 0 kilojoule per kilogram kelvin kJ/(kg kJ (kg K) Specific Entropy and Specific Heat Capacity 1000 0 joule per kilogram kelvin J/(kg J (kg K) Specific Entropy and Specific Heat Capacity 1 0 kilovolt kV Electric Potential 1000 0 millivolt mV Electric Potential 0.001 0 megavolt MV Electric Potential 1000000 0 volt V Electric Potential 1 0 joule J Energy 1 0 gigawatt hour GWh Energy 3.60E+12 0 megawatt hour MWh Energy 3600000000 0 watt hour Wh Energy 3600 0 British thermal unit Btu Energy 1055.055853 0 calorie cal Energy 4.1868 0 gigajoule GJ Energy 1000000000 0 kilojoule kJ Energy 1000 0 kilowatt hour kWh Energy 3600000 0 megajoule MJ Energy 1000000 0 watt second Ws Energy 1 0 kilocalorie kcal Energy 4186.8 0 million calorie MMcal Energy 4186800 0 million British thermal unit MM Btu Energy 1055055853 0 acre foot acre ft Volume 1233.481838 0 million imperial gallon Imp Mgal Volume 4546.09 0 thousand imperial gallon Imp kgal Volume 4.54609 0 barrel bbl Volume 0.158987295 0 Imperial gallon Imp gal Volume 0.00454609 0 million US gallon US Mgal Volume 3785.411784 0 thousand US gallon US kgal Volume 3.785411784 0 cubic centimeter cm3 Volume 1.00E-06 0 cubic foot ft3 Volume 0.028316847 0 kiloliter kL Volume 1 0 liter L Volume 0.001 0 megaliter M L Volume 1000 0 milliliter mL Volume 1.00E-06 0 thousand cubic meter k m3 Volume 1000 0 US gallon US gal Volume 0.003785412 0 million barrel MMbbl Volume 158987.2949 0 thousand barrel kbbl Volume 158.9872949 0 cubic meter m3 Volume 1 0 kilogram per cubic meter kg/m3 kg m3 Density 1 0 gram per liter g/L g L Density 1 0 kilogram per liter kg/L kg L Density 1000 0 pound per barrel lb/bbl lb bbl Density 2.853010174 0 pound per cubic foot lb/ft3 lb ft3 Density 16.01846337 0 pound per US gallon lb/US lb US gal Density 119.8264273 0 tonne per cubic meter t/m3 t m3 Density 1000 0 radian rad Plane Angle 1 0 degree ?? Plane Angle 0.017453293 0 revolution r Plane Angle 6.283185307 0 pascal second Pa*s Dynamic Viscosity 1 0 poise P Dynamic Viscosity 0.1 0 delta degree Fahrenheit delta ??F Temperature (Delta) 0.555555556 0 delta degree Rankine delta ??R Temperature (Delta) 0.555555556 0 delta kelvin delta K Temperature (Delta) 1 0 delta degree Celsius delta ??C Temperature (Delta) 1 0 SdsUomQuantity API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsUomQuantitys. For general SdsUomQuantity information, see Units of Measure . Get Quantity Returns the quantity corresponding to the specified quantity Id within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Quantities/{quantityId} api v1 Tenants default Namespaces {namespaceId} Quantities {quantityId} Parameters string namespaceId The namespace; either default or diagnostics. string quantityId The quantity identifier. Response The response includes a status code and a response body. Response body The requested SdsUomQuantity. Example response body for quantityId = \"Length\": HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"Length\", \"Name\": \"Length\", \"BaseUom\": { \"Id\": \"meter\", \"Abbreviation\": \"m\", \"Name\": \"meter\", \"DisplayName\": \"meter\", \"QuantityId\": \"Length\", \"ConversionFactor\": 1 }, \"Dimensions\": [ 1, 0, 0, 0, 0, 0, 0 ] } Get Quantities Returns a list of all quantities available within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Quantities?skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} Quantities?skip={skip}\u0026count={count} Parameters string namespaceId The namespace; either default or diagnostics. int skip An optional parameter representing the zero-based offset of the first SdsUomQuantity to retrieve. If not specified, a default value of 0 is used. int count An optional parameter representing the maximum number of SdsUomQuantity to retrieve. If not specified, a default value of 100 is used. Response The response includes a status code and a response body. Response body A list of SdsUomQuantity objects. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"Angular Velocity\", \"Name\": \"Angular Velocity\", \"BaseUom\": { \"Id\": \"radian per second\", \"Abbreviation\": \"rad/s\", \"rad s\", \"Name\": \"radian per second\", \"DisplayName\": \"radian per second\", \"QuantityId\": \"Angular Velocity\", \"ConversionFactor\": 1 }, \"Dimensions\": [ 0, 0, -1, 0, 0, 0, 0 ] }, { \"Id\": \"Area\", \"Name\": \"Area\", \"BaseUom\": { \"Id\": \"square meter\", \"Abbreviation\": \"m2\", \"Name\": \"square meter\", \"DisplayName\": \"square meter\", \"QuantityId\": \"Area\", \"ConversionFactor\": 1 }, \"Dimensions\": [ 2, 0, 0, 0, 0, 0, 0 ] }, ] Get Quantity Uom Returns the unit of measure associated with the specified uomId belonging to the quantity with the specified quantityId. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Quantities/{quantityId}/Units/{uomId} api v1 Tenants default Namespaces {namespaceId} Quantities {quantityId} Units {uomId} Parameters string namespaceId The namespace; either default or diagnostics. string quantityId The quantity identifier. string uomId The unit of measure identifier. Response The response includes a status code and a response body. Response body The requested SdsUom Example response for quantityId = \"Length\" and uomId =\"mile\": HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"mile\", \"Abbreviation\": \"mi\", \"Name\": \"mile\", \"DisplayName\": \"mile\", \"QuantityId\": \"Length\", \"ConversionFactor\": 1609.344 } Get Quantity Uoms Returns the list of units of measure that belongs to the quantity with the specified quantityId. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Quantities/{quantityId}/Units api v1 Tenants default Namespaces {namespaceId} Quantities {quantityId} Units Parameters string namespaceId The namespace; either default or diagnostics. string quantityId The quantity identifier. Response The response includes a status code and a response body. Response body A collection of SdsUom objects for the specified quantity. Example response for quantityId = \"Electric Current\": HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"milliampere\", \"Abbreviation\": \"mA\", \"Name\": \"milliampere\", \"DisplayName\": \"milliampere\", \"QuantityId\": \"Electric Current\", \"ConversionFactor\": 0.001 }, { \"Id\": \"ampere\", \"Abbreviation\": \"A\", \"Name\": \"ampere\", \"DisplayName\": \"ampere\", \"QuantityId\": \"Electric Current\", \"ConversionFactor\": 1 } ] SdsUom API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsUoms. For general SdsUom information, see Units of Measure . Get Uom Returns the unit of measure corresponding to the specified uomId within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Units/{uomId} api v1 Tenants default Namespaces {namespaceId} Units {uomId} Parameters string namespaceId The namespace; either default or diagnostics. string uomId The unit of measure identifier. Response The response includes a status code and a response body. Response body The requested SdsUom. Example response body for uomId = \"ounce\": HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"ounce\", \"Abbreviation\": \"oz\", \"Name\": \"ounce\", \"DisplayName\": \"ounce\", \"QuantityId\": \"Mass\", \"ConversionFactor\": 0.028349523 } Get Uoms Returns a list of all available units of measure in the system. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Units?skip={skip}\u0026count={count} api v1 Tenants default Namespaces {namespaceId} Units?skip={skip}\u0026count={count} Parameters string namespaceId The namespace; either default or diagnostics. int skip An optional parameter representing the zero-based offset of the first SdsUomQuantity to retrieve. If not specified, a default value of 0 is used. int count An optional parameter representing the maximum number of SdsUomQuantity to retrieve. If not specified, a default value of 100 is used. Response The response includes a status code and a response body. Response body A list of SdsUom objects. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"count\", \"Abbreviation\": \"count\", \"Name\": \"count\", \"DisplayName\": \"count\", \"QuantityId\": \"Quantity\", \"ConversionFactor\": 1 }, { \"Id\": \"Ampere hour\", \"Abbreviation\": \"Ah\", \"Name\": \"Ampere hour\", \"DisplayName\": \"Ampere hour\", \"QuantityId\": \"Electric Charge\", \"ConversionFactor\": 3600 }, { \"Id\": \"coulomb\", \"Abbreviation\": \"C\", \"Name\": \"coulomb\", \"DisplayName\": \"coulomb\", \"QuantityId\": \"Electric Charge\", \"ConversionFactor\": 1 } ] Associate a unit of measure with an SdsType At SdsType creation, you can associate an SdsUom with a SdsTypeProperty . Associate a unit of measure with an SdsStream At SdsStream creation, you can override any unit of measure associated with an SdsTypeProperty belonging to the SdsType of the stream. This enables the reuse of an SdsType that may have default unit information associated with it already."
                                     },
    "V1/OpcUa/OPCUADataSelectionConfiguration.html":  {
                                                          "href":  "V1/OpcUa/OPCUADataSelectionConfiguration.html",
                                                          "title":  "Data selection configuration",
                                                          "keywords":  "Data selection configuration In addition to the data source configuration, you need to provide a data selection configuration to specify the data you want the OPC UA EDS adapter to collect from the data sources. When you add a data source, the OPC UA EDS adapter browses the entire OPC UA server address space and exports the available OPC UA variables into a JSON file for data selection. Data is collected automatically based upon user demands. OPC UA data from OPC UA variables is read through subscriptions (unsolicited reads). You can either have the data selection configuration file generated for you or you can create it manually yourself. Generate default OPC UA data selection configuration file A default OPC UA data selection file will be created if there is no OPC UA data selection configuration, but a valid OPC UA data source exists. Note: To avoid possibly expensive browse operations, OSIsoft recommends that you manually create a data selection file instead of generating the default data selection file. Complete the following steps for this default data selection file to be generated: Add an OPC UA EDS adapter with a unique ComponentId. During the installation of Edge Data Store, enabling the OPC UA EDS adapter results in addition of a unique component that also satisfies this condition. Configure a valid OPC UA data source . Once you complete these steps, a default OPC UA data selection configuration file will be generated in the configuration directory for the corresponding platform. The following are example locations of the file created. In this example, it is assumed that the ComponentId of the OPC UA component is the default OpcUa1: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\Configuration\\OpcUa1_DataSelection.json Linux: /usr/share/OSIsoft/EdgeDataStore/Configuration/OpcUa1_DataSelection.json  usr share OSIsoft EdgeDataStore Configuration OpcUa1_DataSelection.json Copy the file to a different directory. The contents of the file will look something like: [ { \"Selected\": false, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"StreamId\": null } ] In a text editor, edit the file and change the value of any Selected key from false to true in order to transfer the OPC UA data to be stored in Edge Data Store. In the same directory where you edited the file, run the following curl command: curl -i -d \"@OpcUa1_DataSelection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection Configure OPC UA data selection Note: You cannot modify OPC UA data selection configurations manually. You must use the REST endpoints to add or edit the configuration. Complete the following to configure the OPC UA data selection: Using any text editor, create a file that contains an OPC UA data selection in JSON form. For content structure, see OPC UA data selection example . Update the parameters as needed. For a table of all available parameters, see Parameters for OPC UA data selection . Save the file as DataSelection.config.json . Use any Configuration tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cEDS http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSelection/ adapterId\u003e DataSelection  The following example shows the HTTPS request using curl (run this command from the same directory where the file is located): Note: During installation, you can add a single OPC UA EDS adapter named OpcUA1. The following example uses this component name. curl -v -d \"@DataSelection.config.json\" -H \"Content-Type: application/json\" application json\" \"http://localhost:5590/api/v1/configuration/OpcUa1/DataSelection\" \"http:  localhost:5590 api v1 configuration OpcUa1 DataSelection\" OPC UA data selection schema The following table shows the basic behavior of the OpcUa_DataSelection_schema.json file. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Parameters for OPC UA data selection The following parameters can be used to configure OPC UA data selection: Parameter Required Type Nullable Description Selected Optional Boolean No Use this field to select or clear a measurement. To select an item, set to true. To remove an item, leave the field empty or set to false. If not configured, the default value is false. Name Optional string Yes The optional friendly name of the data item collected from the data source. If not configured, the default value will be the stream id. NodeId Required string Yes The NodeId of the variable. StreamID Optional string Yes The custom stream ID used to create the streams. If not specified, the OPC UA EDS adapter will generate a default stream ID based on the measurement configuration. A properly configured custom stream ID follows these rules: Is not case-sensitive. Can contain spaces. Cannot start with two underscores (\"__\"). Can contain a maximum of 100 characters. Cannot use the following characters: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % \u003c \u003e | Cannot start or end with a period. Cannot contain consecutive periods. Cannot consist of only periods. OPC UA data selection example The following is an example of valid OPC UA data selection configuration: [ { \"Selected\": true, \"Name\": \"Random1\", \"NodeId\": \"ns=5;s=Random1\", \"StreamId\": \"CustomStreamName\" }, { \"Selected\": false, \"Name\": \"Sawtooth1\", \"NodeId\": \"ns=5;s=Sawtooth1\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Sinusoid1\", \"NodeId\": \"ns=5;s=Sinusoid1\", \"StreamId\": null } ]"
                                                      },
    "V1/OpcUa/OpcUaDataSelection.html":  {
                                             "href":  "V1/OpcUa/OpcUaDataSelection.html",
                                             "title":  "Generate default OPC UA data selection configuration file",
                                             "keywords":  "Generate default OPC UA data selection configuration file When you add a data source, the OPC UA EDS adapter browses the entire OPC UA server address space and exports the available OPC UA variables into a JSON file for data selection. Data is collected automatically based upon user demands. OPC UA data from OPC UA variables is read through subscriptions (unsolicited reads). A default OPC UA data selection file will be created if there is no OPC UA data selection configuration, but a valid OPC UA data source exists. Note: To avoid possibly expensive browse operations, OSIsoft recommends that you manually create a data selection file instead of generating the default data selection file. For more information, see Data selection configuration . Complete the following steps in order for this default data selection file to be generated: Add an OPC UA EDS adapter with a unique ComponentId. During the installation of Edge Data Store, enabling the OPC UA EDS adapter results in addition of a unique component that also satisfies this condition. Configure a valid OPC UA data source . Once you complete these steps, a default OPC UA data selection configuration file will be generated in the configuration directory for the corresponding platform. The following are example locations of the file created. In this example, it is assumed that the ComponentId of the OPC UA component is the default OpcUa1: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\Configuration\\OpcUa1_DataSelection.json Linux: /usr/share/OSIsoft/EdgeDataStore/Configuration/OpcUa1_DataSelection.json  usr share OSIsoft EdgeDataStore Configuration OpcUa1_DataSelection.json Copy the file to a different directory. The contents of the file will look something like: [ { \"Selected\": false, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"StreamId\": null } ] In a text editor, edit the file and change the value of any Selected key from false to true in order to transfer the OPC UA data to be stored in Edge Data Store. In the same directory where you edited the file, run the following curl command: curl -i -d \"@OpcUa1_DataSelection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection"
                                         },
    "V1/Installation/System Requirements.html":  {
                                                     "href":  "V1/Installation/System Requirements.html",
                                                     "title":  "System requirements",
                                                     "keywords":  "System requirements Edge Data Store is supported on a variety of platforms and processors. OSIsoft provides ready to use install kits for the following platforms: Operating System Installation Kit Processor(s) Windows 10 x64 EdgeDataStore.msi Intel/AMD Intel AMD 64 bit processors Debian 9 or later x64 EdgeDataStore_linux-x64.deb Intel/AMD Intel AMD 64 bit processors Debian 9 or later arm32 EdgeDataStore_linux-arm.deb ARM32 Raspberry PI 2,3,4 (Raspbian) BeagleBone Debian 9 or later arm64 EdgeDataStore_linux-arm64.deb Raspberry PI 3,4 (Ubuntu ARM64 Server) Google Coral Dev Board Nvidia Nano Jetson OSIsoft also provides instructions for using Edge Data Store with Docker containers. For more information, see Install Edge Data Store using Docker . If you want to build your own custom installers or containers for Linux, tar.gz files are provided with binaries."
                                                 },
    "V1/Installation/InstallationOverview.html":  {
                                                      "href":  "V1/Installation/InstallationOverview.html",
                                                      "title":  "Installation",
                                                      "keywords":  "Installation You can install Edge Data Store in one of two ways: Using an install kit. For more information, see Install Edge Data Store . Using Docker containers. For more information, see Docker . For a list of supported platforms and processors, see System requirements ."
                                                  },
    "V1/Installation/Install Edge Data Store.html":  {
                                                         "href":  "V1/Installation/Install Edge Data Store.html",
                                                         "title":  "Install Edge Data Store",
                                                         "keywords":  "Install Edge Data Store You can install Edge Data Store using an install kit, as described in this section, or by using Docker containers. For more information, see Install Edge Data Store using Docker . For a list of supported platforms and processors, see System requirements . Note: You can change the port assignment either during or after installation. For more information on how to change the port number, see System port configuration . Windows (Windows 10 x64) You must have administrative privileges to run the installer. Complete the following to install Edge Data Store on Windows: Copy the EdgeDataStore.msi file to the file system of the device. To start the installer, double-click the EdgeDataStore.msi file in Windows Explorer. Alternatively, you can start the installer from the command line with the following command: msiexec /i  i EdgeDataStore.msi PORT=5590 INSTALLFOLDER=\"C:\\otherdir\" Note: You can use the optional INSTALLFOLDER parameter (must be in all caps) to specify an alternate location for Edge Data Store\u0027s binary components. The default value is \"C:\\Program Files\\OSISoft\\EdgeDataStore\". OSIsoft recommends you use the default value. In the OSIsoft Edge Data Store Setup window, click Next . Optional: Change the install folder and port number (default 5590) and select the Modbus or OpcUa component or both. Note: Valid values are in the range of 1024 to 65535. Select a port not already in use on the host because the installer will not check for this case. In the command line, use the optional PORT parameter (must be in all caps) to specify the port. If you omit PORT=nnnn, the default port will be used. The UI will start with the port pre-set to the value specified; validity will be checked as mentioned previously, with the install proceeding only when a valid port number is provided. However, if the \"quiet\" or \"no ui\" flag for msiexec is specified and the PORT value on the command line is not valid, the install will proceed with the default 5590 value. Click Next \u003e Install . When the install finishes, Edge Data Store will be installed and running on the port specified. Click Finish . Linux You must have administrative privileges to install the software, for example root or sudo privilege. Complete the following to install Edge Data Store on Linux: Open a terminal window and type the sudo command for the installation kit appropriate to your operating system and processor. Debian 9 or later (Intel/AMD (Intel AMD 64 bit processors) sudo apt install ./EdgeDataStore_linux-x64.deb . EdgeDataStore_linux-x64.deb Debian 9 or later (ARM32, Raspberry PI 2,3,4: Raspbian, BeagleBone) sudo apt install ./EdgeDataStore_linux-arm.deb . EdgeDataStore_linux-arm.deb Debian 9 or later (Raspberry PI 3,4: Ubuntu ARM64 Server, Google Coral Dev Board, Nvidia Nano Jetson) sudo apt install ./EdgeDataStore_linux-arm64.deb . EdgeDataStore_linux-arm64.deb A validation check for prerequisites will be completed. If the Linux OS is up to date, the install will succeed. If the install fails, run the following commands from the terminal window and try the install again: sudo apt update sudo apt upgrade After the check for prerequisites succeeds, you will be prompted if you want to change the default port (5590). Optional: Type the port value you want and press Enter. If 5590 is acceptable, press Enter. Note: If you specify an invalid value for the port, the install will proceed with the default value of 5590. You will then be prompted if you want to install a Modbus TCP or OPC UA EDS adapter in addition to the default Storage component. The default is not to install them. You can add them after the installation is complete if you want. If you want to install neither EDS adapter, press Enter to proceed. The install will complete and Edge Data Store will be running on your device."
                                                     },
    "V1/DataIngress/EDSDataIngress.html":  {
                                               "href":  "V1/DataIngress/EDSDataIngress.html",
                                               "title":  "Data ingress configuration",
                                               "keywords":  "Data ingress configuration Edge Data Store supports the following protocols to ingress or store data: OPC UA EDS adapter : Use standard OPC UA equipment and protocols to send data into EDS. Modbus TCP EDS adapter : Use standard Modbus TCP equipment and protocols to send data into EDS. OMF Ingress : Use the OSIsoft Message Format to send data from a custom application into EDS. OMF is a simple REST and JSON based data format designed for simplicity of custom application design. SDS Ingress : Use the OSIsoft Sequential Data Store (SDS) REST to send data from a custom application into EDS. SDS offers the most options for how to send, store, and retrieve data from EDS."
                                           },
    "V1/CommandLine/Access EdgeCmd utility.html":  {
                                                       "href":  "V1/CommandLine/Access EdgeCmd utility.html",
                                                       "title":  "Access EdgeCmd utility",
                                                       "keywords":  "Access EdgeCmd utility The following EdgeCmd utility locations are based on the installation instructions in EdgeCmd utility . Windows Complete the following to access EdgeCmd utility on Windows: Open a command prompt. Type the following in the command prompt and press Enter: C:\\Program Files\\OSIsoft\\EdgeCmd\\edgecmd.exe Note: Specify the full path when you use EdgeCmd utility on Windows. Linux Complete the following to access EdgeCmd utility on Linux: Open a terminal window. Type the following in the terminal and press Enter: /opt/OSIsoft/EdgeCmd/edgecmd  opt OSIsoft EdgeCmd edgecmd Note: You can access EdgeCmd utility without using the full path on Linux."
                                                   },
    "V1/Administration/Stop and start an EDS adapter.html":  {
                                                                 "href":  "V1/Administration/Stop and start an EDS adapter.html",
                                                                 "title":  "Stop and start an EDS adapter",
                                                                 "keywords":  "Stop and start an EDS adapter By default, when Edge Data Store starts, all currently configured EDS adapters are started and remain running until the product shuts down. Stop an EDS adapter Complete the follwing to stop an EDS adapter: Start any Configuration tool capable of making HTTP requests. Execute a POST command to the following endpoint, replacing \u003cadapterId\u003e with the adapter that you want to stop: http://localhost:5590/api/v1/administration/\u003cadapterId\u003e/Stop http:  localhost:5590 api v1 administration \u003cadapterId\u003e Stop Example Stop the OpcUa1 adapter using curl: curl -v -d \"\" http://localhost:5590/api/v1/Administration/OpcUa1/Stop http:  localhost:5590 api v1 Administration OpcUa1 Stop An HTTP status 204 message indicates success. Start an EDS adapter Complete the following to start an EDS adapter: Start any Configuration tool capable of making HTTP requests. Execute a POST command to the following endpoint, replacing \u003cadapterId\u003e with the adapter that you want to start: http://localhost:5590/api/v1/administration/\u003cadapterId\u003e/Start http:  localhost:5590 api v1 administration \u003cadapterId\u003e Start Example Stop the Modbus1 adapter using curl: curl -v -d \"\" http://localhost:5590/api/v1/Administration/Modbus1/Start http:  localhost:5590 api v1 Administration Modbus1 Start An HTTP status 204 message indicates success."
                                                             },
    "V1/SDS/SDS_Views.html":  {
                                  "href":  "V1/SDS/SDS_Views.html",
                                  "title":  "Stream views",
                                  "keywords":  "Stream views An SdsStreamView provides a way to map stream data requests from one data type to another. You can apply a stream View to any read or GET operation. SdsStreamView is used to specify the mapping between source and target types. SDS attempts to determine how to map Properties from the source to the destination. When the mapping is straightforward, such as when the properties are in the same position and of the same data type, or when the properties have the same name, SDS will map the properties automatically. When SDS is unable to determine how to map a source property, the property is removed. If SDS encounters a target property that it cannot map to, the property is added and configured with a default value. To map a property that is beyond the ability of SDS to map on its own, you should define an SdsStreamViewProperty and add it to the SdsStreamView???s Properties collection. The following table shows the required and optional SdsStreamView fields. Fields that are not included are reserved for internal SDS use. For more information on search limitations, see Searching . Property Type Optionality Searchability Details Id String Required Yes Identifier for referencing the stream view. Name String Optional Yes Friendly name. Description String Optional Yes Description text. SourceTypeId String Required Yes Identifier of the SdsType of the SdsStream. TargetTypeId String Required Yes Identifier of the SdsType to convert events to. Properties IList\u003cSdsStreamViewProperty\u003e Optional Yes, with limitations Property level mapping. Rules for the stream view identifier (SdsStreamView.Id) Is not case sensitive. Can contain spaces. Cannot contain forward slash (\"/\"). (\" \"). Can contain a maximum of 100 characters. Properties /   SdsStreamViewProperty The SdsStreamView Properties collection provides detailed instructions for specifying the mapping of event properties. Each SdsStreamViewProperty in the properties collection defines the mapping of an event???s property. SdsStreamView properties are required only when property mapping is not straightforward. Additionally, if you do not want a type property mapped, it is not necessary to create an SdsStreamView property for it. The following table shows the required and optional SdsStreamViewProperty fields. Property Type Optionality Details SourceId String Required Identifier of the SdsTypeProperty from the source SdsType Properties list. TargetId String Required Identifier of the SdsTypeProperty from the target SdsType Properties list. SdsStreamView SdsStreamView Optional Additional mapping instructions for derived types. The SdsStreamView field supports nested properties. SdsStreamViewMap When an SdsStreamView is added, SDS defines a plan mapping. Plan details are retrieved as an SdsStreamViewMap. The SdsStreamViewMap provides a detailed property-by-property definition of the mapping. The following table shows the SdsStreamViewMap fields. The SdsStreamViewMap cannot be written to SDS, so required and optional have no meaning. Property Type Optionality Details SourceTypeId String Required Identifier of the SdsType of the SdsStream. TargetTypeId String Required Identifier of the SdsType to convert events to. Properties IList\u003cSdsStreamViewMapProperty\u003e Optional Property level mapping. Properties /   SdsStreamViewMapProperty The SdsStreamViewMapProperty is similar an SdsStreamViewProperty but adds a mode detailing one or more actions taken on the property. The following table shows the SdsStreamViewMapProperty fields. The SdsStreamViewMap cannot be written; it can only be retrieved from SDS, so required and optional have no meaning. Property Type Details SourceTypeId String Identifier of the SdsType of the SdsStream. TargetTypeId String Identifier of the SdsType to convert events to. Mode SdsStreamViewMode Aggregate of actions applied to the properties. SdsStreamViewModes are combined via binary arithmetic. SdsStreamViewMap SdsStreamViewMap Mapping for derived types. The available SdsStreamViewModes are shown in the following table. Name Value Description None 0x0000 No action. FieldAdd 0x0001 Add a property matching the specified SdsTypeProperty. FieldRemove 0x0002 Remove the property matching the specified SdsTypeProperty. FieldRename 0x0004 Rename the property matching the source SdsTypeProperty to the target SdsTypeProperty. FieldMove 0x0008 Move the property from the location in the source to the location in the target. FieldConversion 0x0016 Converts the source property to the target type. InvalidFieldConversion 0x0032 Cannot perform the specified mapping. Changing stream type Stream Views can be used to change the type defining a stream. You cannot modify the SdsType; types are immutable. But you can map a stream from its current type to a new type. To update a stream\u0027s type, define an SdsStreamView and PUT the stream view to the following: PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Type?streamViewId={streamViewId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Type?streamViewId={streamViewId} For details, see Update Stream Type . Working with SdsStreamViews When working with Stream Views either invoke HTTP directly or use some of the sample code. Both Python and JavaScript samples have SdsStreamView definitions. The JSON for a simple mapping between a source type with identifier Sample and a target type with identifier Sample1 would appear as follows. { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple1\" } The SdsStreamViewMap would appear as follows. { \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple1\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\", \"Mode\":4 } ] } SdsStreamView API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsStreamViews. See Stream Views for general SdsStreamView information. Get Stream View Returns the stream view corresponding to the specified streamViewId within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId} api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Parameters string namespaceId The namespace; either default or diagnostics. string streamViewId The stream view identifier. Response The response includes a status code and a response body. Response body The requested SdsStreamView. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\" } ] } Get Stream View Map Returns the stream view map corresponding to the specified streamViewId within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId}/Map api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Map Parameters string namespaceId The namespace; either default or diagnostics. string streamViewId The stream view identifier. Response The response includes a status code and a response body. Response body The requested SdsStreamView. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\", \"Mode\":20 }, { \"SourceId\":\"State\", \"Mode\":2 }, { \"TargetId\":\"State\", \"Mode\":1 } ] } Get Stream Views Returns a list of stream views within a given namespace. If specifying the optional search query parameter, the list of stream views returned will match the search criteria. If the search query parameter is not specified, the list will include all stream views in the Namespace. See Searching for information about specifying those respective parameters. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} api v1 Tenants default Namespaces {namespaceId} StreamViews?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string namespaceId The namespace; either default or diagnostics. string query An optional parameter representing a string search. For information about specifying the search parameter, see Searching . int skip An optional parameter representing the zero-based offset of the first SdsStreamView to retrieve. If not specified, a default value of 0 is used. int count An optional parameter representing the maximum number of SdsStreamViews to retrieve. If not specified, a default value of 100 is used. string orderby An optional parameter representing sorted order which SdsStreamViews will be returned. A field name is required. The sorting is based on the stored values for the given field (of type string). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc would sort the returned results by the name values, descending. If no value is specified, there is no sorting of results. Response The response includes a status code and a response body. Response body A collection of zero or more SdsStreamViews. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\":\"StreamView\", \"Name\":\"StreamView\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\" }, { \"Id\":\"StreamViewWithProperties\", \"Name\":\"StreamViewWithProperties\", \"SourceTypeId\":\"Simple\", \"TargetTypeId\":\"Simple3\", \"Properties\":[ { \"SourceId\":\"Time\", \"TargetId\":\"Time\" }, { \"SourceId\":\"State\", \"TargetId\":\"State\" }, { \"SourceId\":\"Measurement\", \"TargetId\":\"Value\" } ] } ] Get or Create Stream View If a stream view with a matching identifier already exists, the stream view passed in is compared with the existing stream view. If the stream views are identical, a Found (302) status is returned and the stream view. If the stream views are different, the Conflict (409) error is returned. If no matching identifier is found, the specified stream view is created. Request POST api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId} api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Parameters string namespaceId The namespace; either default or diagnostics. string streamViewId The stream view identifier. The identifier must match the SdsStreamView.Id field. Request body The request content is the serialized SdsStreamView. Response The response includes a status code and a response body. Response body The newly created or matching SdsStreamView. Create or Update Stream View Creates or updates the definition of a stream view. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId} api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Parameters string namespaceId The namespace; either default or diagnostics. string streamViewId The stream view identifier. Request body The request content is the serialized SdsStreamView. Response The response includes a status code and a response body. Response body The newly created or updated SdsStreamView. Delete Stream View Deletes a stream view from the specified tenant and namespace. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/StreamViews/{streamViewId} api v1 Tenants default Namespaces {namespaceId} StreamViews {streamViewId} Parameters string namespaceId The namespace; either default or diagnostics. string streamViewId The stream view identifier. Response The response includes a status code."
                              },
    "V1/SDS/SDS_Types.html":  {
                                  "href":  "V1/SDS/SDS_Types.html",
                                  "title":  "Types",
                                  "keywords":  "Types The Sequential Data Store (SDS) stores streams of events and provides convenient ways to find and associate events. Events are stored in SdsStreams. An SdsType defines the shape or structure of the event and how to associate events within the SdsStream. SdsTypes can define simple atomic types, such as integers, floats, strings, arrays, and dictionaries. They can also define complex types using SdsTypes. You can define complex, nested types using the Properties collection of an SdsType. An SdsType used to define an SdsStream must have a key. A key is a property, or a combination of properties that constitute an ordered, unique identity. The key is ordered, so it functions as an index. It is known as the primary index. While a timestamp (DateTime) is a very common type of key, any type that can be ordered is permitted. Other indexes (secondary indexes), are defined in the SdsStream. For more details on indexes, see Indexes . When you define a type, consider how the events will be represented in a stream. The SdsType defines each event in the stream. An event is a single unit whose properties have values that relate to the index; that is, each property of an SdsType event is related to the event???s index. Each event is a single unit. An SdsType is referenced by its identifier or Id field. SdsType identifiers must be unique within a Namespace. An SdsType can also refer other SdsTypes by using their identifiers. This enables type reusability. Nested types and base types are automatically created as separate types. For further information, see Type Reusability SdsTypes define how events are associated and read within a collection of events, or SdsStream. The read characteristics when attempting to read non-existent indexes, indexes that fall between, before or after existing indexes, are determined by the interpolation and extrapolation settings of the SdsType. For more information about read characteristics, see Interpolation and Extrapolation . SdsTypes are immutable. After you create an SdsType, you cannot change its definition. If the definition of an SdsType is incorrect, you must delete and recreate it. In addition, the SdsType may be deleted only if no streams, stream views, or types reference it. Only SdsTypes used to define SdsStreams or SdsStreamViews are required to be added to the Sequential data store. SdsTypes that define properties or base types are contained within the parent SdsType and are not required to be added to the Data Store independently. The following table shows the required and optional SdsType fields. Fields that are not included are reserved for internal SDS use. For search limitations, see Searching . Property Type Optionality Searchable Details Id String Required Yes Identifier for referencing the type. Name String Optional Yes Friendly name. Description String Optional Yes Description text. SdsTypeCode SdsTypeCode Required No Numeric code identifying the base SdsType. InterpolationMode SdsInterpolationMode Optional No Interpolation setting of the type. Default is Continuous. ExtrapolationMode SdsExtrapolationMode Optional No Extrapolation setting of the type. Default is All. Properties IList\u003cSdsTypeProperty\u003e Required Yes, with limitations List of SdsTypeProperty items. Rules for the type identifier (SdsType.Id) Is not case sensitive. Can contain spaces. Cannot contain forward slash (\"/\"). (\" \"). Can contain a maximum of 100 characters. SdsTypeCode The SdsTypeCode is a numeric identifier used by the Data Store to identify SdsTypes. A SdsTypeCode exists for every supported type. Atomic types, such as strings, floats, and arrays, are defined entirely by the SdsTypeCode. Atomic types do not need fields to define the type. Types requiring additional definition, such as enums and objects, are identified using a generic SdsTypeCode, such as ByteEnum, Int32Enum, NullableInt32Enum, or Object, plus additional SdsProperty fields. Supported Types The following types are supported and defined by the SdsTypeCode: Type SdsTypeCode Array 400 Boolean 3 BooleanArray 203 Byte 6 ByteArray 206 ByteEnum 606 Char 4 CharArray 204 DateTime 16 DateTimeArray 216 DateTimeOffset 20 DateTimeOffsetArray 220 DBNull 2 Decimal 15 DecimalArray 215 Double 14 DoubleArray 214 Empty 0 Guid 19 GuidArray 219 IDictionary 402 IEnumerable 403 IList 401 Int16 7 Int16Array 207 Int16Enum 607 Int32 9 Int32Array 209 Int32Enum 609 Int64 11 Int64Array 211 Int64Enum 611 NullableBoolean 103 NullableByte 106 NullableByteEnum 706 NullableChar 104 NullableDateTime 116 NullableDateTimeOffset 120 NullableDecimal 115 NullableDouble 114 NullableGuid 119 NullableInt16 107 NullableInt16Enum 707 NullableInt32 109 NullableInt32Enum 709 NullableInt64 111 NullableInt64Enum 711 NullableSByte 105 NullableSByteEnum 705 NullableSingle 113 NullableTimeSpan 121 NullableUInt16 108 NullableUInt16Enum 708 NullableUInt32 110 NullableUInt32Enum 710 NullableUInt64 112 NullableUInt64Enum 712 Object 1 SByte 5 SByteArray 205 SByteEnum 605 Single 13 SingleArray 213 String 18 StringArray 218 TimeSpan 21 TimeSpanArray 221 UInt16 8 UInt16Array 208 UInt16Enum 608 UInt32 10 UInt32Array 210 UInt32Enum 610 UInt64 12 UInt64Array 212 UInt64Enum 612 Version 22 VersionArray 222 SdsTypeProperty The properties collection defines the fields in an SdsType. The following table shows the required and optional SdsTypeProperty fields. Fields that are not included are reserved for internal SDS use. Property Type Optionality Details Id String Required Identifier for referencing the type. Name String Optional Friendly name. Description String Optional Description text. SdsType SdsType Required Field defining the property\u0027s Type. IsKey Boolean Required Identifies the property as the Key (Primary Index). Value Object Optional Value of the property. Order Int Optional Order of comparison within a compound index. InterpolationMode SdsInterpolationMode Optional Interpolation setting of the property. Default is null. Uom String Optional Unit of Measure of the property The SdsTypeProperty identifier follows the same rules as the SdsType identifier. IsKey is a Boolean value used to identify the SdsType Key. A key defined by more than one property is called a compound key. The maximum number of properties that can define a compound key is three. In a compound key, each property that is included in the key is specified as IsKey. The Order field defines the precedence of fields applied to the index. The Value field is used for properties that represent a value. An example of a property with a value is an enum???s named constant. When representing an enum in a SdsType, the SdsType properties collection defines the enum???s constant list. The SdsTypeProperty Identifier represents the name of the constant and the SdsTypeProperty value represents the value of the constant (see the enum State definitions below). InterpolationMode is assigned when the property of the event should be interpolated in a specific way that differs from the InterpolationMode of the SdsType. InterpolationMode is only applied to a property that is not part of the Index. If the InterpolationMode is not set, the property is are interpolated in the manner defined by the IntepolationMode of the SdsType. An SdsType with the InterpolationMode set to Discrete cannot have a property with an InteroplationMode. For more information on interpolation of events, see Interpolation . Uom is the unit of measure for the property. The Uom of a property may be specified by the name or the abbreviation. The names and abbreviations of Uoms are case sensitive. The InterpolationMode and Uom of a property can be overridden on the stream. For more information, see Streams . Supported units of measure For a list of units of measures that are supported for an SdsTypeProperty, see Units of Measure . Working with SdsTypes The following discussion refers to the following types and are defined in Python and JavaScript samples. In the sample code, SdsType , SdsTypeProperty , and SdsTypeCode are defined as in the code snippets shown here: Python class SdsTypeCode(Enum): Empty = 0 Object = 1 DBNull = 2 Boolean = 3 Char = 4 ... class SdsTypeProperty(object): \"\"\"SDS type property definition\"\"\" def __init__(self): self.__isKey = False @property def Id(self): return self.__id @Id.setter def Id(self, id): self.__id = id ... @property def IsKey(self): return self.__isKey @IsKey.setter def IsKey(self, iskey): self.__isKey = iskey @property def SdsType(self): return self.__SdsType @SdsType.setter def SdsType(self, SdsType): self.__SdsType=SdsType ... class SdsType(object): \"\"\"SDS type definitions\"\"\" def __init__(self): self.SdsTypeCode = SdsTypeCode.Object @property def Id(self): return self.__id @Id.setter def Id(self, id): self.__id = id ... @property def BaseType(self): return self.__baseType @BaseType.setter def BaseType(self, baseType): self.__baseType = baseType @property def SdsTypeCode(self): return self.__typeCode @SdsTypeCode.setter def SdsTypeCode(self, typeCode): self.__typeCode = typeCode @property def Properties(self): return self.__properties @Properties.setter def Properties(self, properties): self.__properties = properties JavaScript SdsTypeCodeMap: { Empty: 0, \"Object\": 1, DBNull: 2, \"Boolean\": 3, Char: 4, ... SdsTypeProperty: function (SdsTypeProperty) { if (SdsTypeProperty.Id) { this.Id = SdsTypeProperty.Id; } if (SdsTypeProperty.Name) { this.Name = SdsTypeProperty.Name; } if (SdsTypeProperty.Description) { this.Description = SdsTypeProperty.Description; } if (SdsTypeProperty.SdsType) { this.SdsType = SdsTypeProperty.SdsType; } if (SdsTypeProperty.IsKey) { this.IsKey = SdsTypeProperty.IsKey; } }, SdsType: function (SdsType) { if (SdsType.Id) { this.Id = SdsType.Id } if (SdsType.Name) { this.Name = SdsType.Name; } if (SdsType.Description) { this.Description = SdsType.Description; } if (SdsType.SdsTypeCode) { this.SdsTypeCode = SdsType.SdsTypeCode; } if (SdsType.Properties) { this.Properties = SdsType.Properties; } }, Working with the following types (both Python and JavaScript classes are shown): Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getMeasurement, setMeasurement) def getMeasurement(self): return self.__measurement def setMeasurement(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2, } var Simple = function () { this.Time = null; this.State = null; this.Measurement = null; } Define the SdsType as follows: Python # Create the properties # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime # State is not a pre-defined type. A SdsType must be defined to represent the enum stateTypePropertyOk = SdsTypeProperty() stateTypePropertyOk.Id = \"Ok\" stateTypePropertyOk.Value = State.Ok stateTypePropertyWarning = SdsTypeProperty() stateTypePropertyWarning.Id = \"Warning\" stateTypePropertyWarning.Value = State.Warning stateTypePropertyAlarm = SdsTypeProperty() stateTypePropertyAlarm.Id = \"Alarm\" stateTypePropertyAlarm.Value = State.Alarm stateType = SdsType() stateType.Id = \"State\" stateType.Name = \"State\" stateType.Properties = [ stateTypePropertyOk, stateTypePropertyWarning, \\ stateTypePropertyAlarm ] state = SdsTypeProperty() state.Id = \"State\" state.Name = \"State\" state.SdsType = stateType # Value property is a simple non-indexed, pre-defined type value = SdsTypeProperty() value.Id = \"Measurement\" value.Name = \"Measurement\" value.SdsType = SdsType() value.SdsType.Id = \"Double\" value.SdsType.Name = \"Double\" # Create the Simple SdsType simpleType = SdsType() simpleType.Id = \"Simple\" simpleType.Name = \"Simple\" simpleType.Description = \"Basic sample type\" simpleType.SdsTypeCode = SdsTypeCode.Object simpleType.Properties = [ time ] JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    State is not a pre-defined type. An SdsType must be defined to represent the enum var stateTypePropertyOk = new SdsObjects.SdsTypeProperty({ \"Id\": \"Ok\", \"Value\": State.Ok }); var stateTypePropertyWarning = new SdsObjects.SdsTypeProperty({ \"Id\": \"Warning\", \"Value\": State.Warning }); var stateTypePropertyAlarm = new SdsObjects.SdsTypeProperty({ \"Id\": \"Alarm\", \"Value\": State.Alarm }); var stateType = new SdsObjects.SdsType({ \"Id\": \"State\", \"Name\": \"State\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Int32Enum, \"Properties\": [stateTypePropertyOk, stateTypePropertyWarning, stateTypePropertyAlarm, stateTypePropertyRed] }); //    Measurement property is a simple non-indexed, pre-defined type var measurementProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"doubleType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Double }) }); //    Create the Simple SdsType var simpleType = new SdsObjects.SdsType({ \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": \"This is a simple SDS type \", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [timeProperty, stateProperty, measurementProperty] }); Working with a derived class is easy. For the following derived class: class Derived(Simple): @property def Observation(self): return self.__observation @Observation.setter def Observation(self, observation): self.__observation = observation Extend the SdsType as follows: Python # Observation property is a simple non-indexed, standard data type observation = SdsTypeProperty() observation.Id = \"Observation\" observation.Name = \"Observation\" observation.SdsType = SdsType() observation.SdsType.Id = \"String\" observation.SdsType.Name = \"String\" observation.SdsType.SdsTypeCode = SdsTypeCode.String # Create the Derived SdsType derived = SdsType() derived.Id = \"Derived\" derived.Name = \"Derived\" derived.Description = \"Derived sample type\" derived.BaseType = simpleType # Set the base type to the derived type derived.SdsTypeCode = SdsTypeCode.Object derived.Properties = [ observation ] JavaScript var observationProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Observation\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"strType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.String }) }); var derivedType = new SdsObjects.SdsType({ \"Id\": \"Derived\", \"Name\": \"Derived\", \"Description\": \" Derived sample type\", \"BaseType\": simpleType, \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [ observationProperty ] }); Type reusability An SdsType can also refer other SdsTypes by using their identifiers. This enables type reusability. For example, if there is a common index and value property for a group of types that may have additional properties, a base type can be created with those properties. { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } If a new type should be created with properties additional to the ones above, you can add a reference to the base type by simply specifying the base type\u0027s Id. { \"Id\": \"Complex\", \"Name\": \"Complex\", \"SdsTypeCode\": 1, \"BaseType\":{ \"Id\":\"Simple\" }, \"Properties\": [ { \"Id\": \"Depth\", \"Name\": \"Depth\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } The new type may also include the full type definition of the reference type instead of specifying only the Id. For example: { \"Id\": \"Complex\", \"Name\": \"Complex\", \"SdsTypeCode\": 1, \"BaseType\":{ \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] }, \"Properties\": [ { \"Id\": \"Depth\", \"Name\": \"Depth\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } If the full definition is sent, the referenced types (base type \"Simple\" in the above example) should match the actual type initially created. If the full definition is sent and the referenced types do not exist, they will be created automatically by SDS. Further type creations can reference them as demonstrated above. Note: When trying to get types back from SDS, the results will also include types that were automatically created by SDS. Base types and properties of type Object, Enum, and user-defined collections such as Array, List and Dictionary will be treated as referenced types. Note that streams cannot be created using these referenced types. If a stream of particular type is to be created, the type should contain at least one property with a valid index type as described in Indexes . The index property may also be in the base type as shown in the example above. SdsType API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsTypes. See Types for general SdsType information. Get Type Returns the type corresponding to the specified typeId within a given namespace. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Types/{typeId} api v1 Tenants default Namespaces {namespaceId} Types {typeId} Parameters string namespaceId The namespace; either default or diagnostics. string typeId The type identifier. Response The response includes a status code and a response body. Response body The requested SdsType. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"SdsTypeCode\": 16 } }, { \"Id\": \"State\", \"Name\": \"State\", \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"SdsTypeCode\": 609, \"Properties\": [ { \"Id\": \"Ok\", \"Value\": 0 }, { \"Id\": \"Warning\", \"Value\": 1 }, { \"Id\": \"Alarm\", \"Value\": 2 } ] } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"SdsTypeCode\": 14 } } ] } Get Type Reference Count Returns a dictionary mapping the object name to the number of references held by streams, stream views and parent types for the specified type. For more information on the use of types to define streams and stream views, see Streams and Steam Views . For further details about type referencing, see: Type Reusability . Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Types/{typeId}/ReferenceCount api v1 Tenants default Namespaces {namespaceId} Types {typeId} ReferenceCount Parameters string namespaceId The namespace; either default or diagnostics. string typeId The type identifier. Response The response includes a status code and a response body. Response body A dictionary mapping object name to number of references. Example response body: { \"SdsStream\": 3, \"SdsStreamView\": 2, \"SdsType\": 1 } Get Types Returns a list of types within a given namespace. If specifying the optional search query parameter, the list of types returned will match the search criteria. If the search query parameter is not specified, the list will include all types in the namespace. For information about specifying those respective parameters, see Searching . Note: The results will also include types that were automatically created by SDS as a result of type referencing. For further details about type referencing, see: Type Reusability . Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Types?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} api v1 Tenants default Namespaces {namespaceId} Types?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string namespaceId The namespace; either default or diagnostics. string query An optional query string to match which SdsTypes will be returned. For information about specifying the query parameter, see the Searching topic. int skip An optional value representing the zero-based offset of the first SdsType to retrieve. If not specified, a default value of 0 is used. int count An optional value representing the maximum number of SdsTypes to retrieve. If not specified, a default value of 100 is used. string orderby An optional parameter representing sorted order which SdsTypes will be returned. A field name is required. The sorting is based on the stored values for the given field (of type string). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc would sort the returned results by the name values, descending. If no value is specified, there is no sorting of results. Response The response includes a status code and a response body. Response body A collection of zero or more SdsTypes. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"SdsTypeCode\": 16 } }, { \"Id\": \"State\", \"Name\": \"State\", \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"SdsTypeCode\": 609, \"Properties\": [ { \"Id\": \"Ok\", \"Value\": 0 }, { \"Id\": \"Warning\", \"Value\": 1 }, { \"Id\": \"Alarm\", \"Value\": 2 } ] } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"SdsTypeCode\": 14 } } ] }, ] Get or Create Type Creates the specified type. If a type with a matching identifier already exists, SDS compares the existing type with the type that was sent. If the types are identical, a Found (302) error is returned with the Location header set to the URI where the type may be retrieved using a Get function. If the types do not match, a Conflict (409) error is returned. Note: A Conflict (409) error will also be returned if the type contains reference to any existing type, but the referenced type definition in the body does not match the existing type. You may reference an existing type without including the reference type definition in the body by using only the Ids. For further details about type referencing, see: Type Reusability . For a matching type ( Found ), clients that are capable of performing a redirect that includes the authorization header can automatically redirect to retrieve the type. However, most clients, including the .NET HttpClient, consider redirecting with the authorization token to be a security vulnerability. When a client performs a redirect and strips the authorization header, SDS cannot authorize the request and returns Unauthorized (401). For this reason, OSIsoft recommends that when using clients that do not redirect with the authorization header, you should disable automatic redirect and perform the redirect manually. Request POST api/v1/Tenants/default/Namespaces/{namespaceId}/Types/{typeId} api v1 Tenants default Namespaces {namespaceId} Types {typeId} Parameters string namespaceId The namespace; either default or diagnostics. string typeId The type identifier. The identifier must match the SdsType.Id field in the request body. Request body The request content is the serialized SdsType. Example SdsType content: { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"SdsTypeCode\": 16 } }, { \"Id\": \"State\", \"Name\": \"State\", \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"SdsTypeCode\": 609, \"Properties\": [ { \"Id\": \"Ok\", \"Value\": 0 }, { \"Id\": \"Warning\", \"Value\": 1 }, { \"Id\": \"Alarm\", \"Value\": 2 } ] } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"SdsTypeCode\": 14 } } ] } Response The response includes a status code and a response body. Response body The request content is the serialized SdsType. OSIsoft recommends that you use JSON. Example Response body: HTTP/1.1 HTTP 1.1 201 Content-Type: application/json application json { \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": null, \"SdsTypeCode\": 1, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"Description\": null, \"Order\": 0, \"IsKey\": true, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"19a87a76-614a-385b-ba48-6f8b30ff6ab2\", \"Name\": \"DateTime\", \"Description\": null, \"SdsTypeCode\": 16, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"State\", \"Name\": \"State\", \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"e20bdd7e-590b-3372-ab39-ff61950fb4f3\", \"Name\": \"State\", \"Description\": null, \"SdsTypeCode\": 609, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": [ { \"Id\": \"Ok\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 0, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Warning\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 1, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Alarm\", \"Name\": null, \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": null, \"Value\": 2, \"Uom\": null, \"InterpolationMode\": null } ], \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"Description\": null, \"Order\": 0, \"IsKey\": false, \"FixedSize\": 0, \"SdsType\": { \"Id\": \"6fecef77-20b1-37ae-aa3b-e6bb838d5a86\", \"Name\": \"Double\", \"Description\": null, \"SdsTypeCode\": 14, \"IsGenericType\": false, \"IsReferenceType\": false, \"GenericArguments\": null, \"Properties\": null, \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 }, \"Value\": null, \"Uom\": null, \"InterpolationMode\": null } ], \"BaseType\": null, \"DerivedTypes\": null, \"InterpolationMode\": 0, \"ExtrapolationMode\": 0 } Delete Type Deletes a type from the specified tenant and namespace. Note that a type cannot be deleted if any streams, stream views, or other types reference it. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Types/{typeId} api v1 Tenants default Namespaces {namespaceId} Types {typeId} Parameters string namespaceId The namespace; either default or diagnostics. string typeId The type identifier. Response The response includes a status code."
                              },
    "V1/Health/Health.html":  {
                                  "href":  "V1/Health/Health.html",
                                  "title":  "Edge Data Store health",
                                  "keywords":  "Edge Data Store health Insight into the health of the Edge Data Store and the components that make it up can be critical for ensuring that your needs for data collection are being met. To that end, Edge Data Store and its components produce health information. When configured, Edge Data Store transfers health information to OMF endpoints, including the types and containers that represent available health information. Configure Edge Data Store health endpoints Edge Data Store has the ability to report system health to one or more OMF endpoints capable of receiving health messages. To enable this functionality, you must configure one or more health endpoints. Configuration parameters for Edge Data Store health endpoints Parameter Required Description Id Optional The ID can be any alphanumeric string; for example, Endpoint1. If you do not specify an ID, Edge Data Store generates one automatically. Endpoint Required The URL of the ingress endpoint which accepts OMF health messages. UserName Required for PI Web API endpoints The user name used for authentication to PI Web API OMF endpoint. Password Required for PI Web API endpoints The password used for authentication to PI Web API OMF endpoint. ClientId Required for OSIsoft Cloud Services. The Client Id used for authentication to OSIsoft Cloud Services. ClientSecret Required for OSIsoft Cloud Services. The Client Secret used for authentication to OSIsoft Cloud Services. Buffering Optional Options are memory, disk, or none. The default is none. MaxBufferSizeMB Optional The limit on the maximum megabytes of data to buffer for messages to this endpoint if an integer is \u003e0. This parameter is useful if you want to limit memory or disk usage growth in the event of disconnection to the endpoint. If the buffer is full, old messages will be discarded for new messages. The default is 0. ValidateEndpointCertificate Optional Edge EDS adapter validates the endpoint certificate if set to true (recommended). If set to false, Edge EDS adapter accepts any endpoint certificate. OSIsoft recommends you disable endpoint certificate validation for testing purposes only. EDS adapter health The following health types and streams are created to reflect the health of EDS adapters. The Connectors static type includes these properties and servers as a root AF element with the ID Connectors. Type Property Description string Id Connectors - root AF element string Description Collection of Connector assets EDS adapter health The Connector Health static type includes the following properties, which are logged in a stream with the ID {machinename}.{componentid}. The stream is linked to root AF element (Connectors). Type Property Description string Id {machinename}.{componentId} string Description {productname} health string Connector Type {adaptertype} string Version {adapterversion} Device status The DeviceStatus dynamic type includes the following values, which are logged in a stream with the ID Connectors.{machinename}.{componentid}.DeviceStatus. The stream is linked to {machinename}.{componentid} static stream. Type Property Description string Time Timestamp of event string DeviceStatus Device status value Next health message expected The NextHealthMessageExpected dynamic type includes the following values, which are logged in a stream with the ID Connectors.{machinename}.{componentid}.NextHealthMessageExpected. The stream is linked to {machinename}.{componentid} static stream. Heard beat message is expected once a minute. Type Property Description string Time Timestamp of event string NextHealthMessageExpected Time when next health message is expected. Storage component health The following health types and streams are created to reflect the health of the Storage component. The Storage static type includes the following properties and servers as a root AF element with the ID Storage. Type Property Description string Id Storage - root AF element string Description Storage Health Storage health The Storage Health static type includes the following properties, which are logged in a stream with the ID {machinename}.Storage. The stream is linked to root AF element (Storage). Type Property Description string Id {machinename}.Storage string Description {productname} health string Connector Type {adaptertype} string Version {storageversion} Storage device status The DeviceStatus dynamic type includes the following values, which are logged in a stream with the ID Storage.{machinename}.DeviceStatus. The stream is linked to {machinename}.Storage static stream. Type Property Description string Time Timestamp of event string DeviceStatus Device status value Storage next health message expected The NextHealthMessageExpected dynamic type includes the following values, which are logged in a stream with the ID Storage.{machinename}.NextHealthMessageExpected. The stream is linked to {machinename}.Storage static stream. Heard beat message is expected once a minute. Type Property Description string Time Timestamp of event string NextHealthMessageExpected Time when next health message is expected."
                              },
    "V1/Egress/Prepare egress destinations.html":  {
                                                       "href":  "V1/Egress/Prepare egress destinations.html",
                                                       "title":  "Prepare egress destinations",
                                                       "keywords":  "Prepare egress destinations The various OSIsoft OMF destinations may require additional configuration. See the following details to prepare an OSIsoft destination to receive OMF messages. OCS To prepare OCS to receive OMF messages from EDS, create an OMF connection in OCS. Creating an OMF connection results in an available OMF endpoint that can be used by the EDS egress mechanism. Complete the following steps to create an OMF connection: Create a Client . The Client Id and Client Secret will be used for the corresponding properties in the egress configuration. Create an OMF type Connection . The connection should link the created client to an existing namespace where the data will be stored. The OMF Endpoint URL for the connection will be used as the egress configuration Endpoint property. PI Server To prepare a PI Server to receive OMF messages from EDS, a PI Web API OMF endpoint must be available. Complete the following steps: Install PI Web API and enable the OSIsoft Message Format (OMF) Services feature. During configuration, choose an AF database and PI Data Archive where metadata and data will be stored. The account used in an egress configuration needs permissions to create AF elements, element templates, and PI points. Configure PI Web API to use Basic authentication. See the PI Web API documentation for complete steps, as well as best practices and recommendations. Note: The certificate used by PI Web API must be trusted by the device running EDS, otherwise the egress configuration ValidateEndpointCertificate property needs to be set to false (this can be the case with a self-signed certificate but should only be used for testing purposes)."
                                                   },
    "V1/Administration/Reset Edge Data Store.html":  {
                                                         "href":  "V1/Administration/Reset Edge Data Store.html",
                                                         "title":  "Reset Edge Data Store",
                                                         "keywords":  "Reset Edge Data Store Edge Data Store provides a method by which you can perform a complete reset of the product. When you perform a reset, all event data and Edge Data Store configuration is deleted, and the product is restarted. Note: All configuration and stored data will be lost as a result of performing this action. Complete the following to reset Edge Data Store: Start any Configuration tool capable of making HTTP requests. Execute a POST command to the following endpoint: http://localhost:5590/api/v1/administration/System/Reset http:  localhost:5590 api v1 administration System Reset Example using curl: curl -v -d \"\" http://localhost:5590/api/v1/Administration/System/Reset http:  localhost:5590 api v1 Administration System Reset An HTTP status 204 message indicates success."
                                                     },
    "V1/index.html":  {
                          "href":  "V1/index.html",
                          "title":  "OSIsoft Edge Data Store",
                          "keywords":  "OSIsoft Edge Data Store ======= Overview Design considerations Security Quick start guides OPC UA EDS adapter quick start Modbus TCP adapter quick start OMF quick start OCS egress quick start PI egress quick start SDS Read/Write Read Write quick start Command line quick start - Linux Command line quick start - Windows Installation System requirements Linux and Windows platform differences Install Edge Data Store Docker Verify installation Uninstall Edge Data Store EdgeCmd utility Install EdgeCmd utility Configuration Configuration tools System configuration System components configuration System port configuration Edge Data Store configuration Data ingress configuration OPC UA EDS adapter Supported features Principles of operation Data source configuration Data selection configuration Adapter security Modbus TCP EDS adapter Supported features Principles of operation Data source configuration Data selection configuration OSIsoft Message Format (OMF) Storage Storage runtime configuration Data egress configuration Prepare egress destinations Egress execution details Diagnostics configuration Health endpoints configuration Logging configuration Administration Retrieve product version information Reset Edge Data Store Reset the Storage component Stop and start an EDS adapter Troubleshoot Edge Data Store Disaster recovery Reference Sequential Data Store (SDS) Types Streams Stream views Indexes Writing data API calls for writing data Reading Data API calls for reading data Filter expressions Table format Units of measure Compression Searching EdgeCmd commands Release Notes"
                      },
    "V1/SDS/indexes.html":  {
                                "href":  "V1/SDS/indexes.html",
                                "title":  "Indexes",
                                "keywords":  "Indexes Indexes speed up and order the results of searches. A key uniquely identifies a record within a collection of records. Keys are unique within the collection. In SDS, the key of an SdsType is also an index. The key is often referred to as the primary index, while all other indexes are referred to as secondary indexes or secondaries . An SdsType that is used to define an SdsStream must specify a key. When inserting data into an SdsStream, every key value must be unique. SDS will not store more than a single event for a given key. An event with a particular key may be deleted or updated, but two events with the same key cannot exist. Secondary indexes are defined on SdsStreams and are applied to a single property. You can define many secondary indexes. Secondary index values do not need to be unique. The following table contains supported index types: Type SdsTypeCode Boolean 3 Byte 6 Char 4 DateTime 16 DateTimeOffset 20 Decimal 15 Double 14 Guid 19 Int16 7 Int32 9 Int64 11 SByte 5 Single 13 String 18 TimeSpan 21 UInt16 8 UInt32 10 UInt64 12 Working with indexes The following discusses the types defined in the Python and Java Script samples. To build an SdsType representation of the following sample class, see Sample : Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class Simple(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2 } var Simple = function () { this.Time = null; this.State = null; this.Value = null; } Sample The following code is used to build an SdsType representation of the sample class above: Python # Create the properties # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime # State is not a pre-defined type. An SdsType must be defined to represent the enum stateTypePropertyOk = SdsTypeProperty() stateTypePropertyOk.Id = \"Ok\" stateTypePropertyOk.Measurement = State.Ok stateTypePropertyWarning = SdsTypeProperty() stateTypePropertyWarning.Id = \"Warning\" stateTypePropertyWarning.Measurement = State.Warning stateTypePropertyAlarm = SdsTypeProperty() stateTypePropertyAlarm.Id = \"Alarm\" stateTypePropertyAlarm.Measurement = State.Alarm stateType = SdsType() stateType.Id = \"State\" stateType.Name = \"State\" stateType.Properties = [ stateTypePropertyOk, stateTypePropertyWarning,\\ stateTypePropertyAlarm ] state = SdsTypeProperty() state.Id = \"State\" state.Name = \"State\" state.SdsType = stateType # Measurement property is a simple non-indexed, pre-defined type measurement = SdsTypeProperty() measurement.Id = \"Measurement\" measurement.Name = \"Measurement\" measurement.SdsType = SdsType() measurement.SdsType.Id = \"Double\" measurement.SdsType.Name = \"Double\" # Create the Simple SdsType simple = SdsType() simple.Id = str(uuid.uuid4()) simple.Name = \"Simple\" simple.Description = \"Basic sample type\" simple.SdsTypeCode = SdsTypeCode.Object simple.Properties = [ time, state, measurement ] JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); //    State is not a pre-defined type. A SdsType must be defined to represent the enum var stateTypePropertyOk = new SdsObjects.SdsTypeProperty({ \"Id\": \"Ok\", \"Value\": State.Ok }); var stateTypePropertyWarning = new SdsObjects.SdsTypeProperty({ \"Id\": \"Warning\", \"Value\": State.Warning }); var stateTypePropertyAlarm = new SdsObjects.SdsTypeProperty({ \"Id\": \"Alarm\", \"Value\": State.Alarm }); var stateType = new SdsObjects.SdsType({ \"Id\": \"State\", \"Name\": \"State\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Int32Enum, \"Properties\": [stateTypePropertyOk, stateTypePropertyWarning, stateTypePropertyAlarm, stateTypePropertyRed] }); //    Value property is a simple non-indexed, pre-defined type var valueProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Value\", \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"doubleType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Double }) }); //    Create the Simple SdsType var simpleType = new SdsObjects.SdsType({ \"Id\": \"Simple\", \"Name\": \"Simple\", \"Description\": \"This is a simple Sds type\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.Object, \"Properties\": [timeProperty, stateProperty, valueProperty] }); The Time property is identified as the Key by defining its SdsTypeProperty as follows: Python # Time is the primary key time = SdsTypeProperty() time.Id = \"Time\" time.Name = \"Time\" time.IsKey = True time.SdsType = SdsType() time.SdsType.Id = \"DateTime\" time.SdsType.Name = \"DateTime\" time.SdsType.SdsTypeCode = SdsTypeCode.DateTime JavaScript //    Time is the primary key var timeProperty = new SdsObjects.SdsTypeProperty({ \"Id\": \"Time\", \"IsKey\": true, \"SdsType\": new SdsObjects.SdsType({ \"Id\": \"dateType\", \"SdsTypeCode\": SdsObjects.SdsTypeCodeMap.DateTime }) }); Note: The time.IsKey field is set to true. To read data using the key, you define a start index and an end index. For DateTime, use ISO 8601 representation of dates and times. To query for a window of values between January 1, 2010 and February 1, 2010, you would define indexes as ???2010-01-01T08:00:00.000Z??? and ???2010-02-01T08:00:00.000Z???, respectively. For additional information, see Reading data ."
                            },
    "V1/Modbus/ModbusOverview.html":  {
                                          "href":  "V1/Modbus/ModbusOverview.html",
                                          "title":  "Modbus TCP EDS adapter",
                                          "keywords":  "Modbus TCP EDS adapter Overview Modbus TCP is a commonly available communication protocol used for connecting and transmitting information between industrial electronic devices. The Modbus TCP EDS adapter polls Modbus TCP slave devices, and transfers time series data from the data source devices into Edge Data Store. Polling is based on the measurement configuration provided, and models the register measurements in a Modbus TCP data source. The Modbus TCP EDS adapter communicates with any device conforming to the Modbus TCP/IP TCP IP protocol through a gateway or router. The Modbus TCP slave devices and routers do not need to be on the same subnet as Edge Data Store. You can add a single Modbus TCP EDS adapter during installation. If you want multiple Modbus TCP EDS adapters, see Edge Data Store configuration for instructions on how to add a new component to Edge Data Store. For more information on how to configure logging for the Modbus TCP EDS adapter, see Logging configuration . To view data in the streams being written by Modbus, see the SDS reference . To egress the data to OSIsoft Cloud Services or the PI System, see Data egress configuration ."
                                      },
    "V1/LinuxWindows/LinuxWindows.html":  {
                                              "href":  "V1/LinuxWindows/LinuxWindows.html",
                                              "title":  "Linux and Windows platform differences",
                                              "keywords":  "Linux and Windows platform differences When developing applications to work with the Edge Data Store, there is no difference between Linux and Windows installations. Edge Data Store installation best practices differ between Linux and Windows, as described in the following sections. File locations Windows Program binaries are placed in the C:\\Program Files\\OSIsoft\\EdgeDataStore directory by default. For information about changing this location, see Install Edge Data Store . Configuration, log, and data files are placed under C:\\ProgramData\\OSIsoft\\EdgeDataStore This is not configurable. This folder structure will not be automatically removed if you uninstall EDS. For information about clearing these files, see Uninstall Edge Data Store . Key material for encrypted secrets of configuration files is stored using the Windows DPAPI in a secure Windows store. This is not configurable. Linux Program binaries are placed in the /opt/OSIsoft/EdgeDataStore  opt OSIsoft EdgeDataStore directory. Configuration, log, and data files are placed under /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore . This folder structure will not be automatically removed if you uninstall EDS. For information about clearing these files, see Uninstall Edge Data Store . Key material for encrypted secrets of configuration files is stored using limited access files under /usr/share/OSIsoft/EdgeDataStore  usr share OSIsoft EdgeDataStore . You cannot configure file locations for Linux. When the Debian installer is used, Edge Data Store is installed using the service identity osisoft.edgedatastore.service. If you need to restart the service from the Linux command line, use the following command: sudo systemctl restart osisoft.edgedatastore.service File descriptors (handles) When installed on a Linux operating system, EDS is configured with a file descriptor limit that may be higher than the corresponding limit for most processes. The limit is controlled by the LimitNOFILE variable in the file /lib/systemd/system/osisoft.edgedatastore.service  lib systemd system osisoft.edgedatastore.service . Linux operating systems impose a limit on the number of file descriptors used in a process. The number of open file descriptors is directly related to the number of streams used in EDS (for example, data ingress). Overall, every stream uses two file descriptors. EDS will no longer function properly when it reaches the limit of available file descriptors. To prevent this, it is necessary to either limit the number of streams used in EDS or increase the maximum allowed file descriptors per process. The following figures were identified on a Raspberry Pi 3 Model B+ using a Raspbian operating system. An installation of EDS with no user-defined streams had an average of 424 open file descriptors. The same installation with 250 streams had an average of 932 open file descriptors. The file descriptor limit per process for the operating system used was 1024. Note Figures will differ on other Linux operating systems and devices, and may differ slightly from execution to execution, so it is important to understand your system. Windows has an object called a handle that is used in much the same way that Linux uses file descriptors. However, Windows does not have a limitation on the number of handles."
                                          },
    "V1/SDS/SDS_Streams.html":  {
                                    "href":  "V1/SDS/SDS_Streams.html",
                                    "title":  "Streams",
                                    "keywords":  "Streams SDS stores collections of events and provides convenient ways to find and associating events. Events of consistent structure are stored in streams, called SdsStreams. An SdsType defines the structure of events in an SdsStream. SdsStreams are referenced by their identifier or Id field. SdsStream identifiers must be unique within a namespace. An SdsStream must include a TypeId that references the identifier of an existing SdsType. When an SdsStream contains data, you must use a stream view to update the stream type. The following table shows the required and optional SdsStream fields. Fields not listed are reserved for internal SDS use. Property Type Optionality Searchability Details Id String Required Yes An identifier for referencing the stream. TypeId String Required Yes The SdsType identifier of the type to be used for this stream. Name String Optional Yes Friendly name. Description String Optional Yes Description text. Indexes IList\u003cSdsStreamIndex\u003e Optional No Used to define secondary indexes for stream. InterpolationMode SdsInterpolationMode Optional No Interpolation setting of the stream. Default is null. ExtrapolationMode SdsExtrapolationMode Optional No Extrapolation setting of the stream. Default is null. PropertyOverrides IList\u003cSdsStreamPropertyOverride\u003e Optional No Used to define unit of measure and interpolation mode overrides for a stream. Rules for the stream identifier (SdsStream.Id) Is not case sensitive. Can contain spaces. Cannot contain forward slash (\"/\"). (\" \"). Can contain a maximum of 100 characters. Indexes The Key or Primary Index is defined at the SdsType. Secondary Indexes are defined at the SdsStream. Secondary indexes are applied to a single property; there are no compound secondary indexes. Only SdsTypeCodes that can be ordered are supported for use in a secondary index. Indexes are discussed in greater detail here: Indexes Interpolation and extrapolation The InterpolationMode, ExtrapolationMode, and PropertyOverrides can be used to determine how a specific stream reads data. These read characteristics are inherited from the type if they are not defined at the stream level. For more information about type read characteristics and how these characteristics dictate how events are read see Types . PropertyOverrides PropertyOverrides provide a way to override interpolation behavior and unit of measure for individual SdsType properties for a specific stream. The SdsStreamPropertyOverride object has the following structure: Property Type Optionality Details SdsTypePropertyId String Required SdsTypeProperty identifier. InterpolationMode SdsInterpolationMode Optional Interpolation setting. Default is null. Uom String Optional Unit of measure. The unit of measure can be overridden for any type property defined by the stream type, including primary keys and secondary indexes. For more information about type property units of measure, see Types . Read characteristics of the stream are determined by the type and the PropertyOverrides of the stream. The interpolation mode for non-index properties can be defined and overridden at the stream level. For more information about type read characteristics, see Types . When specifying property interpolation overrides, if the SdsType InterpolationMode is Discrete , it cannot be overridden at any level. When InterpolationMode is set to Discrete and an event it not defined for that index, a null value is returned for the entire event. SdsStream API The REST APIs provide programmatic access to read and write SDS data. The APIs in this section interact with SdsStreams. See Streams for general SdsStream information. Get Stream Returns the specified stream. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Response The response includes a status code and a response body. Response body The requested SdsStream. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Id\":\"Simple\", \"Name\":\"Simple\", \"TypeId\":\"Simple\", } Get Streams Returns a list of streams. If the optional search query parameter is specified, the list of streams returned will match the search criteria. If the search query parameter is not specified, the list will include all streams in the namespace. See Searching for information about specifying those respective parameters. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} api v1 Tenants default Namespaces {namespaceId} Streams?query={query}\u0026skip={skip}\u0026count={count}\u0026orderby={orderby} Parameters string namespaceId The namespace; either default or diagnostics. string query An optional parameter representing a string search. For information about specifying the search parameter, see Searching . int skip An optional parameter representing the zero-based offset of the first SdsStream to retrieve. If not specified, a default value of 0 is used. int count An optional parameter representing the maximum number of SdsStreams to retrieve. If not specified, a default value of 100 is used. string orderby An optional parameter representing sorted order which SdsStreams will be returned. A field name is required. The sorting is based on the stored values for the given field (of type string). For example, orderby=name would sort the returned results by the name values (ascending by default). Additionally, a value can be provided along with the field name to identify whether to sort ascending or descending, by using values asc or desc , respectively. For example, orderby=name desc would sort the returned results by the name values, descending. If no value is specified, there is no sorting of results. Response The response includes a status code and a response body. Response body A collection of zero or more SdsStreams. Example response body: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Id\":\"Simple\", \"TypeId\":\"Simple\" }, { \"Id\":\"Simple with Secondary\", \"TypeId\":\"Simple\", \"Indexes\":[ { \"SdsTypePropertyId\":\"Measurement\" } ] }, { \"Id\":\"Compound\", \"TypeId\":\"Compound\" }, ] Get Stream Type Returns the type definition that is associated with a given stream. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Type api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Type Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Response The response includes a status code and a response body. Response body The requested SdsType. Get or Create Stream Creates the specified stream. If a stream with a matching identifier already exists, SDS compares the existing stream with the stream that was sent. If the streams are identical, a Found (302) error is returned with the Location header set to the URI where the stream may be retrieved using a Get function. If the streams do not match, a Conflict (409) error is returned. For a matching stream (Found), clients that are capable of performing a redirect that includes the authorization header can automatically redirect to retrieve the stream. However, most clients, including the .NET HttpClient, consider redirecting with the authorization token to be a security vulnerability. When a client performs a redirect and strips the authorization header, SDS cannot authorize the request and returns Unauthorized (401). For this reason, it is recommended that when using clients that do not redirect with the authorization header, you should disable automatic redirect. Request POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. The stream identifier must match the identifier in content. Request body The request content is the serialized SdsStream. Response The response includes a status code and a response body. Response body The newly created SdsStream. Create or Update Stream Creates the specified stream. If a stream with the same Id already exists, the definition of the stream is updated. The following changes are permitted: Name Description Indexes InterpolationMode ExtrapolationMode PropertyOverrides Note that modifying Indexes will result in re-indexing all of the stream\u0027s data for each additional secondary index. For more information on secondary indexes, see Indexes . Unpermitted changes result in an error. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Request body The request content is the serialized SdsStream. Response The response includes a status code. Update Stream Type Updates a stream???s type. The type is modified to match the specified stream view. Defined Indexes and PropertyOverrides are removed when updating a stream type. Request PUT api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Type?streamViewId={streamViewId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Type?streamViewId={streamViewId} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string streamViewId The stream view identifier. Request body The request content is the serialized SdsStream. Response The response includes a status code. Response body On failure, the content contains a message describing the issue. Delete Stream Deletes a stream. Request DELETE api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Response The response includes a status code."
                                },
    "V1/SDS/Reading_Data_API.html":  {
                                         "href":  "V1/SDS/Reading_Data_API.html",
                                         "title":  "API calls for reading data",
                                         "keywords":  "API calls for reading data Example type, stream, and data Many of the API methods described below contain example requests and responses in JSON to highlight usage and specific behaviors. The following type, stream, and data are used in the examples: Example type SimpleType is an SdsType with a single index. This type is defined in Python and Javascript: Python class State(Enum): Ok = 0 Warning = 1 Alarm = 2 class SimpleType(object): Time = property(getTime, setTime) def getTime(self): return self.__time def setTime(self, time): self.__time = time State = property(getState, setState) def getState(self): return self.__state def setState(self, state): self.__state = state Measurement = property(getValue, setValue) def getValue(self): return self.__measurement def setValue(self, measurement): self.__measurement = measurement JavaScript var State = { Ok: 0, Warning: 1, Alarm: 2, } var SimpleType = function () { this.Time = null; this.State = null; this.Value = null; } Example stream Simple is an SdsStream of type SimpleType . Example data Simple has stored values as follows: 11/23/2017 11 23 2017 12:00:00 PM: Ok 0 11/23/2017 11 23 2017 1:00:00 PM: Ok 10 11/23/2017 11 23 2017 2:00:00 PM: Ok 20 11/23/2017 11 23 2017 3:00:00 PM: Ok 30 11/23/2017 11 23 2017 4:00:00 PM: Ok 40 All times are represented at offset 0, GMT. Get First Value Returns the first value in the stream. If no values exist in the stream, null is returned. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/First api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data First Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Response The response includes a status code and a response body containing a serialized event. Get Last Value Returns the last value in the stream. If no values exist in the stream, null is returned. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Last api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Last Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Response The response includes a status code and a response body containing a serialized event. Find Distinct Value Returns a stored event based on the specified index and searchMode . Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?index={index}\u0026searchMode={searchMode} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string index The index. string searchMode The SdsSearchMode , the default is exact . Response The response includes a status code and a response body containing a serialized collection with one event. Depending on the request index and searchMode , it is possible to have an empty collection returned. Example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?index=2017-11-23T13:00:00Z\u0026searchMode=Next The request has an index that matches the index of an existing event, but since a SdsSearchMode of next was specified, the response contains the next event in the stream after the specified index: Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?index=2017-11-23T13:30:00Z\u0026searchMode=Next The request specifies an index that does not match an index of an existing event. The next event in the stream is retrieved. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Get Values Returns a collection of stored values at indexes based on request parameters. SDS supports three ways of specifying which stored events to return: Filtered : A filtered request accepts a filter expression . Range : A range request accepts a start index and a count. Window : A window request accepts a start index and end index. This request has an optional continuation token for large collections of events. Filtered Returns a collection of stored values as determined by a filter . The filter limits results by applying an expression against event fields. Filter expressions are explained in detail in the Filter expressions section. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?filter={filter} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string filter The filter expression (see Filter expressions ) Response The response includes a status code and a response body containing a serialized collection of events. Example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?filter=Measurement gt 10 The events in the stream with Measurement greater than 10 are returned. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note: State is not included in the JSON as its value is the default value. Range Returns a collection of stored values as determined by a startIndex and count . Additional optional parameters specify the direction of the range, how to handle events near or at the start index, whether to skip a certain number of events at the start of the range, and how to filter the data. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data ?startIndex={startIndex}\u0026count={count}[\u0026skip={skip}\u0026reversed={reversed} \u0026boundaryType={boundaryType}\u0026filter={filter}] Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string startIndex Index identifying the beginning of the series of events to return. int count The number of events to return. int skip Optional value specifying the number of events to skip at the beginning of the result. bool reversed Optional specification of the direction of the request. By default, range requests move forward from startIndex, collecting events after startIndex from the stream. A reversed request will collect events before startIndex from the stream. SdsBoundaryType boundaryType Optional SdsBoundaryType specifies the handling of events at or near startIndex. string filter Optional filter expression. Response The response includes a status code and a response body containing a serialized collection of events. Example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100 This request will return a response with up to 100 events starting at 13:00 and extending forward toward the end of the stream: Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note: State is not included in the JSON as its value is the default value. Example To reverse the direction of the request, set reversed to true. The following request will return up to 100 events starting at 13:00 and extending back toward the start of the stream: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\" } ] Note: State is not included in the JSON as its value is the default value. Further, Measurement is not included in the second, 12:00:00, event as zero is the default value for numbers. The following request specifies a boundary type of Outside for a reversed-direction range request. The response will contain up to 100 events. The boundary type Outside indicates that up to one event outside the boundary will be included in the response. For a reverse direction range request, this means one event forward of the specified start index. In a default direction range request, it would mean one event before the specified start index. GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true\u0026boundaryType=2 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 0 } ] The event outside of the index is the next event or the event at 14:00 because the request operates in reverse. Adding a filter to the request means only events that meet the filter criteria are returned: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T13:00:00Z\u0026count=100\u0026reversed=true\u0026boundaryType=2\u0026filter=Measurement gt 10 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Window Returns a collection of stored events based on the specified startIndex and endIndex . For handling events at and near the boundaries of the window, a single SdsBoundaryType that applies to both the start and end indexes can be passed with the request, or separate boundary types may be passed for the start and end individually. Paging is supported for window requests with a large number of events. To retrieve the next page of values, include the continuationToken from the results of the previous request. For the first request, specify a null or empty string for the continuationToken . Requests GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026endIndex={endIndex} GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026endIndex={endIndex}\u0026boundaryType={boundaryType} GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026startBoundaryType={startBoundaryType} \u0026endIndex={endIndex}\u0026endBoundaryType={endBoundaryType} GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026endIndex={endIndex} \u0026count={count}\u0026continuationToken={continuationToken} GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data? api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data? ?startIndex={startIndex}\u0026startBoundaryType={startBoundaryType} \u0026endIndex={endIndex}\u0026endBoundaryType={endBoundaryType}\u0026filter={filter}\u0026count={count} \u0026continuationToken={continuationToken} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string startIndex Index bounding the beginning of the series of events to return. string endIndex Index bounding the end of the series of events to return. int count Optional maximum number of events to return. If count is specified, a continuationToken must also be specified. SdsBoundaryType boundaryType Optional SdsBoundaryType specifies handling of events at or near the start and end indexes. SdsBoundaryType startBoundaryType Optional SdsBoundaryType specifies the first value in the result in relation to the start index. If startBoundaryType is specified, endBoundaryType must be specified. SdsBoundaryType endBoundaryType Optional SdsBoundaryType specifies the last value in the result in relation to the end index. If startBoundaryType is specified, endBoundaryType must be specified. string filter Optional filter expression string continuationToken Optional token used to retrieve the next page of data. If count is specified, a continuationToken must also be specified. Response The response includes a status code and a response body containing a serialized collection of events. A continuation token can be returned if specified in the request. Example The following requests all stored events between 12:30 and 15:30: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z The response will contain the event stored at the specified index: Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 } ] Note: State is not included in the JSON as its value is the default value. Example When the request is modified to specify a boundary type of Outside, the value before 13:30 and the value after 15:30 are included: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026boundaryType=2 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T12:00:00Z\" }, { \"Time\": \"2017-11-23T13:00:00Z\", \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"Measurement\": 40 } ] Note: State is not included in the JSON as its value is the default value. Further, Measurement is not included in the second, 12:00:00, event as zero is the default value for numbers. If instead a start boundary of Inside, only values inside the start boundary (after 13:30) are included in the result. With an end boundary of Outside one value outside the end index (after 15:30) is included: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026\u0026startBoundaryType=1 \u0026endIndex=2017-11-23T15:30:00Z\u0026endBoundaryType=2 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 } ] To page the results of the request, a continuation token may be specified. This requests the first page of the first two stored events between start index and end index by indicating count is 2 and continuationToken is an empty string: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026count=2\u0026continuationToken= Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Results\": [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ], \"ContinuationToken\": \"2017-11-23T14:00:00.0000000Z\" } This request uses the continuation token from the previous page to request the next page of stored events: GET api/v1/Tenants/default}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default} Namespaces {namespaceId} Streams Simple Data ?startIndex=2017-11-23T12:30:00Z\u0026endIndex=2017-11-23T15:30:00Z \u0026count=2\u0026continuationToken=2017-11-23T14:00:00Z Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json { \"Results\": [ { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 } ], \"ContinuationToken\": null } In this case, the results contain the final event. The returned continuation token is null. Get Interpolated Values Returns a collection of values based on request parameters. The stream\u0027s read characteristics determine how events are calculated for indexes at which no stored event exists. Interpolation is not supported for streams with compound indexes. SDS supports two ways of specifying which interpolated events to return: Index Collection : One or more indexes can be passed to the request to retrieve events at specific indexes. Interval : An interval can be specified with a start index, end index, and count. This will return the specified count of events evenly spaced from start index to end index. Index Collection Returns events at the specified indexes. If no stored event exists at a specified index, the stream\u0027s read characteristics determine how the returned event is calculated. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data  Interpolated?index={index}[\u0026index={index}...] Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string index One or more indexes. Response The response includes a status code and a response body containing a serialized collection of events. Depending on the specified indexes and read characteristics of the stream, it is possible to have less events returned than specified indexes. An empty collection can also be returned. Example Consider a stream of type Simple with the default InterpolationMode of Continuous and ExtrapolationMode of All . In the following request, the specified index matches an existing stored event: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default Namespaces {namespaceId} Streams Simple Data  Interpolated?index=2017-11-23T13:00:00Z The response will contain the event stored at the specified index. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 } ] The following request specifies an index for which no stored event exists: GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default Namespaces {namespaceId} Streams Simple Data  Interpolated?index=2017-11-23T13:30:00Z Because the index is a valid type for interpolation and the stream has an InterpolationMode of Continuous , this request receives a response with an event interpolated at the specified index: Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:30:00Z\", \"State\": 0, \"Measurement\": 15 } ] Consider a stream of type Simple with an InterpolationMode of Discrete and ExtrapolationMode of All . In the following request, the specified indexes only match two existing stored events: GET api/v1/Tenants/default}/Namespaces/{namespaceId}/Streams/Simple/Data api v1 Tenants default} Namespaces {namespaceId} Streams Simple Data Interpolated?index=2017-11-23T12:30:00Z\u0026index=2017-11-23T13:00:00Z\u0026index=2017-11-23T14:00:00Z For this request, the response contains events for two of the three specified indexes. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 } ] Interval Returns events at evenly spaced intervals based on the specified start index, end index, and count. If no stored event exists at an index interval, the stream\u0027s read characteristics determine how the returned event is calculated. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data  Interpolated?startIndex={startIndex}\u0026endIndex={endIndex}\u0026count={count} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string startIndex The index defining the beginning of the window. string endIndex The index defining the end of the window. int count The number of events to return. Read characteristics of the stream determine how the events are constructed. Response The response includes a status code and a response body containing a serialized collection of events. Depending on the read characteristics and input parameters, it is possible for a collection to be returned with less events than specified in the count. For a stream, named Simple, of type Simple for the following request: GET api/v1/Tenants/default}/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default} Namespaces {namespaceId} Streams Simple Data  Interpolated?startIndex=2017-11-23T13:00:00Z\u0026endIndex=2017-11-23T15:00:00Z\u0026count=3 The start and end fall exactly on event indexes, and the number of events from start to end match the count of three (3). Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 30 } ] Get Summaries Returns summary intervals between a specified start and end index. Index types that cannot be interpolated do not support summary requests. Strings are an example of indexes that cannot be interpolated. Summaries are not supported for streams with compound indexes. Interpolating between two indexes that consist of multiple properties is not defined and results in non-determinant behavior. Summary values supported by SdsSummaryType enum: Summary Enumeration value Count 1 Minimum 2 Maximum 4 Range 8 Mean 16 StandardDeviation 64 Total 128 Skewness 256 Kurtosis 512 WeightedMean 1024 WeightedStandardDeviation 2048 WeightedPopulationStandardDeviatio 4096 Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data  Summaries?startIndex={startIndex}\u0026endIndex={endIndex}\u0026count={count}\u0026filter={filter} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string startIndex The start index for the intervals. string endIndex The end index for the intervals. int count The number of intervals requested. string filter Optional filter expression. string streamViewId Optional stream view identifier. Response The response includes a status code and a response body containing a serialized collection of SdsIntervals. Each SdsInterval has a start, end, and collection of summary values. Property Details Start The start of the interval End The end of the interval Summaries The summary values for the interval, keyed by summary type. The nested dictionary contains property name keys and summary calculation result values. Example The following request calculates two summary intervals between the startIndex and endIndex : GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default Namespaces {namespaceId} Streams Simple Data  Summaries?startIndex=2017-11-23T12:00:00Z\u0026endIndex=2017-11-23T16:00:00Z\u0026count=2 Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Start\": { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 0 }, \"End\": { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, \"Summaries\": { \"Count\": { \"Measurement\": 2 }, \"Minimum\": { \"Measurement\": 0 }, \"Maximum\": { \"Measurement\": 20 }, \"Range\": { \"Measurement\": 20 }, \"Total\": { \"Measurement\": 20 }, \"Mean\": { \"Measurement\": 10 }, \"StandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"PopulationStandardDeviation\": { \"Measurement\": 5 }, \"WeightedMean\": { \"Measurement\": 10 }, \"WeightedStandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"WeightedPopulationStandardDeviation\": { \"Measurement\": 5 }, \"Skewness\": { \"Measurement\": 0 }, \"Kurtosis\": { \"Measurement\": -2 } } }, { \"Start\": { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 20 }, \"End\": { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, \"Summaries\": { \"Count\": { \"Measurement\": 2 }, \"Minimum\": { \"Measurement\": 20 }, \"Maximum\": { \"Measurement\": 40 }, \"Range\": { \"Measurement\": 20 }, \"Total\": { \"Measurement\": 60 }, \"Mean\": { \"Measurement\": 30 }, \"StandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"PopulationStandardDeviation\": { \"Measurement\": 5 }, \"WeightedMean\": { \"Measurement\": 30 }, \"WeightedStandardDeviation\": { \"Measurement\": 7.0710678118654755 }, \"WeightedPopulationStandardDeviation\": { \"Measurement\": 5 }, \"Skewness\": { \"Measurement\": 0 }, \"Kurtosis\": { \"Measurement\": -2 } } } ] Get Sampled Values Returns data sampled by intervals between a specified start and end index. Sampling is driven by a specified property or properties of the stream\u0027s SdsType. Property types that cannot be interpolated do not support sampling requests. Strings are an example of a property that cannot be interpolated. For more information see Interpolation. Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/ api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data  Sampled?startIndex={startIndex}\u0026endIndex={endIndex}\u0026intervals={intervals}\u0026sampleBy={sampleBy} \u0026boundaryType={boundaryType}\u0026startBoundaryType={startBoundaryType} \u0026endBoundaryType={endBoundaryType}\u0026filter={filter}\u0026streamViewId={streamViewId} Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. string startIndex The start index for the intervals. string endIndex The end index for the intervals. int intervals The number of intervals requested. string sampleBy Property or properties to use when sampling. SdsBoundaryType boundaryType Optional SdsBoundaryType specifies the handling of events at or near the startIndex and endIndex. SdsBoundaryType startBoundaryType Optional SdsBoundaryType specifies the handling of events at or near the startIndex. SdsBoundaryType endBoundaryType Optional SdsBoundaryType specifies the handling of events at or near the endIndex. string filter Optional filter expression. Response The response includes a status code and a response body containing a serialized collection of events. Example The following request returns two sample intervals between the startIndex and endIndex : GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/Simple/Data/ api v1 Tenants default Namespaces {namespaceId} Streams Simple Data  Sampled?startIndex=2019-01-01T00:00:00Z\u0026endIndex=2019-01-02T00:00:00Z\u0026intervals=2\u0026sampleBy=Measurement Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2019-01-01T00:00:01Z\", \"State\": 1, \"Measurement\": 1 }, { \"Time\": \"2019-01-01T00:11:50Z\", \"State\": 2, \"Measurement\": 0.00006028870675578446 }, { \"Time\": \"2019-01-01T11:55:33Z\", \"Measurement\": 6.277981349066863 }, { \"Time\": \"2019-01-01T12:00:00Z\", \"Measurement\": 3.101013140344655 }, { \"Time\": \"2019-01-01T12:00:01Z\", \"State\": 1, \"Measurement\": 4.101013140344655 }, { \"Time\": \"2019-01-01T12:01:50Z\", \"State\": 2, \"Measurement\": 0.0036776111121028521 }, { \"Time\": \"2019-01-01T23:57:23Z\", \"State\": 2, \"Measurement\": 6.2816589601789659 }, { \"Time\": \"2019-01-02T00:00:00Z\", \"Measurement\": 6.20202628068931 } ] Note: State is not included in the JSON when its value is the default value. Join Values Returns data from multiple streams, which are joined based on the request specifications. The streams must be of the same SdsType. SDS supports the following types of joins: SdsJoinMode Enumeration value Operation Inner 0 Results include the stored events with common indexes across specified streams. Outer 1 Results include the stored events for all indexes across all streams. Interpolated 2 Results include events for each index across all streams for the request index boundaries. Some events may be interpolated. MergeLeft 3 Results include events for each index across all streams selecting events at the indexes based on left to right order of the streams. MergeRight 4 Results include events for each index across all streams selecting events at the indexes based on right to left order of the streams. SDS supports two types of join requests: GET : The stream, joinMode, start index, and end index are specified in the request URI path. POST : Only the SdsJoinMode is specified in the URI. The streams and read specification for each stream are specified in the body of the request. GET Request GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams={streams}\u0026joinMode={joinMode} \u0026startIndex={startIndex}\u0026endIndex={endIndex} Parameters string namespaceId The namespace; either default or diagnostics. string streams Commas separated list of stream identifiers. SdsJoinMode joinMode Type of join, that is inner, outer, and so on. string startIndex Index identifying the beginning of the series of events to return. string endIndex Index identifying the end of the series of events to return. Response The response includes a status code and a response body containing multiple serialized events. See examples for specifics. Examples To join multiple streams, for example Simple1 and Simple2, assume that Simple1 presents the following data: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 } ] And assume that Simple2 presents the following data: HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] The following are responses for various Joins request options: Inner join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=inner \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response Measurements from both streams with common indexes. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 } ] ] Outer join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=outer \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response All Measurements from both Streams, with default values at indexes where a Stream does not have a value. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, null ], [ null, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, null ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 } ], [ null, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 } ], [ { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, null ], [ null, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] ] Interpolated join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=interpolated \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response All measurements from both Streams with missing values interpolated. If the missing values are between valid measurements within a stream, they are interpolated. If the missing values are outside of the boundary values, they are extrapolated. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 15 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 } ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 55 } ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 } ], [ { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 35 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 } ], [ { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 75 } ], [ { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] ] MergeLeft join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=mergeleft \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response This is similar to OuterJoin , but the value at each index is the first available value at that index when iterating the given list of streams from left to right. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] MergeRight join example GET api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?streams=Simple1,Simple2\u0026joinMode=mergeright \u0026startIndex=0001-01-01T00:00:00.0000000\u0026endIndex=9999-12-31T23:59:59.9999999 Response This is similar to OuterJoin , but the value at each index is the first available value at that index when iterating the given list of streams from right to left. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, { \"Time\": \"2017-11-23T12:00:00Z\", \"State\": 0, \"Measurement\": 50 }, { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 60 }, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 }, { \"Time\": \"2017-11-23T16:00:00Z\", \"State\": 0, \"Measurement\": 40 }, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] POST request POST api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?joinMode={joinMode} Parameters string namespaceId The namespace; either default or diagnostics. SdsJoinMode joinMode Type of join, that is inner, outer, and so on. Request body Read options specific to each stream. Response The response includes a status code and a response body containing multiple serialized events. Consider the following outer join request: POST api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data/Joins api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Joins ?joinMode=outer where in the request body, different start indexes and end indexes are specified per stream: [ { \"StreamId\": \"Simple1\", \"Options\": { \"StartIndex\": \"2017-11-23T11:00:00Z\", \"EndIndex\": \"2017-11-23T14:00:00Z\", \"StartBoundaryType\": \"Exact\", \"EndBoundaryType\": \"Exact\", \"Count\": 100, \"Filter\": \"\" } }, { \"StreamId\": \"Simple2\", \"Options\": { \"StartIndex\": \"2017-11-23T15:00:00Z\", \"EndIndex\": \"2017-11-23T17:00:00Z\", \"StartBoundaryType\": \"Exact\", \"EndBoundaryType\": \"Exact\", \"Count\": 100, \"Filter\": \"\" } } ] Only events within the stream\u0027s specified index boundaries are considered for the outer join operation. Response body HTTP/1.1 HTTP 1.1 200 Content-Type: application/json application json [ [ { \"Time\": \"2017-11-23T11:00:00Z\", \"State\": 0, \"Measurement\": 10 }, null ], [ { \"Time\": \"2017-11-23T13:00:00Z\", \"State\": 0, \"Measurement\": 20 }, null ], [ { \"Time\": \"2017-11-23T14:00:00Z\", \"State\": 0, \"Measurement\": 30 }, null ], [ null, { \"Time\": \"2017-11-23T15:00:00Z\", \"State\": 0, \"Measurement\": 70 } ], [ null, { \"Time\": \"2017-11-23T17:00:00Z\", \"State\": 0, \"Measurement\": 80 } ] ] Note: Not all the values from streams were included since they are restricted by individual queries for each Stream."
                                     },
    "V1/SDS/Reading_Data.html":  {
                                     "href":  "V1/SDS/Reading_Data.html",
                                     "title":  "Reading data",
                                     "keywords":  "Reading data The REST APIs provide programmatic access to read and write data. This section identifies and describes the APIs used to read streams data. Results are influenced by types , stream views , filter expressions , and table format . Single stream reads The following methods for reading a single value are available: Get First Value returns the first value in the stream. Get Last Value returns the last value in the stream. Find Distinct Value returns a value based on a starting index and search criteria. In addition, the following methods support reading multiple values: Get Values retrieves a collection of stored values based on the request parameters. Get Interpolated Values retrieves a collection of stored or calculated values based on the request parameters. Get Summaries retrieves a collection of evenly spaced summary intervals based on a count and specified start and end indexes. Get Sampled Values retrieves a collection of sampled data based on the request parameters. All single stream reads are HTTP GET actions. Reading data involves getting events from streams. The base reading URI from a single stream is as follows: api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Parameters string namespaceId The namespace; either default or diagnostics. string streamId The stream identifier. Bulk reads SDS supports reading from multiple streams in one request. The following method for reading data from multiple streams is available. Join Values retrieves a collection of events across multiple streams and joins the results based on the request parameters. Multi-stream reads can be HTTP GET or POST actions. The base reading URI for reading from multiple streams is the following: api/v1/Tenants/default/Namespaces/{namespaceId}/Bulk/Streams/Data api v1 Tenants default Namespaces {namespaceId} Bulk Streams Data Parameters string namespaceId The namespace; either default or diagnostics. Response format Supported response formats include JSON, verbose JSON, and SDS. JSON is the default response format for SDS, which is used in all examples in this documentation. Default JSON responses do not include any values that are equal to the default value for their type. Verbose JSON responses include all values, including defaults, in the returned JSON payload. To specify verbose JSON return, add the header Accept-Verbosity with a value of verbose to the request. SDS format is specified by setting the Accept header in the request to application/sds application sds . Indexes and reading data Most read operations take at least one index as a parameter. Indexes may be specified as strings. You can find additional details about working with indexes on the Indexes page. Interpolation Interpolation determines how a stream behaves when asked to return an event at an index between two existing events. InterpolationMode determines how the returned event is constructed. The table below lists InterpolationModes: Mode Enumeration value Operation Default 0 The default InterpolationMode is Continuous. Continuous 0 Interpolates the data using previous and next index values. StepwiseContinuousLeading 1 Returns the data from the previous index. StepwiseContinuousTrailing 2 Returns the data from the next index. Discrete 3 Returns ???null???. Note that Continuous cannot return events for values that cannot be interpolated, such as when the type is not numeric. The table below describes how the Continuous InterpolationMode affects indexes that occur between data in a stream: InterpolationMode = Continuous or Default Type Result for an index between data in a stream Comment Numeric Types Interpolated* Rounding is done as needed for integer types. Time related Types Interpolated DateTime, DateTimeOffset, TimeSpan Nullable Types Interpolated** Limited support for nullable numeric types. Array and List Types No event is returned String Type No event is returned Boolean Type Returns value of nearest index Enumeration Types Returns Enum value at 0 This may have a value for the enumeration. GUID No event is returned Version No event is returned IDictionary or IEnumerable No event is returned Dictionary, Array, List, and so on. *When extreme values are involved in an interpolation (for example, Decimal.MaxValue) the call might result in a BadRequest exception. **Nullable types are interpolated in the same manner as their non-nullable equivalents as long as the values surrounding the desired interpolation index are non-null. If either of the values are null, the interpolated value will be null. If the InterpolationMode is not assigned, the events are interpolated in the default manner, unless the interpolation mode is overridden in the SdsTypeProperty or the SdsStream. For more information on overriding the interpolation mode on a specific type property, see SdsTypeProperty . For more information on overriding the interpolation mode for a specific stream, see Streams . Extrapolation Extrapolation defines how a stream responds to requests with indexes that precede or follow all data in the steam. ExtrapolationMode acts as a master switch to determine whether extrapolation occurs and at which end of the data. ExtrapolationMode works with the InterpolationMode to determine how a stream responds. The following tables show how ExtrapolationMode affects returned values for each InterpolationMode value: ExtrapolationMode with InterpolationMode = Default (or Continuous), StepwiseContinuousLeading and StepwiseContinuousTrailing ExtrapolationMode Enumeration value Index before data Index after data All 0 Returns first data value Returns last data value. None 1 No event is returned No event is returned. Forward 2 No event is returned Returns last data value. Backward 3 Returns first data value No event is returned. ExtrapolationMode with InterpolationMode = Discrete ExtrapolationMode Enumeration value Index before data Index after data All 0 No event is returned. No event is returned. None 1 No event is returned. No event is returned. Forward 2 No event is returned. No event is returned. Backward 3 No event is returned. No event is returned. If the ExtrapolationMode is not assigned, the events are extrapolated in the default manner, unless the extrapolation mode is overridden on the SdsStream. For more information on overriding the extrapolation mode on a specific stream, see Streams . For additional information about the effect of read characteristics, see the documentation on the read method you are using. Read characteristics When you request data at an index for which no stored event exists, the read characteristics determine whether the result is an error, no event, interpolated event, or extrapolated event. The combination of the type of the index and the interpolation and extrapolation modes of the SdsType and the SdsStream determine the read characteristics. Filter expressions You can apply filter expressions to any read that returns multiple values, including Get Values, Get Range Values, Get Window Values, and Get Intervals. The filter expression is applied to the collection events conditionally filtering events that do not meet the filter conditions. For details on filter expressions, see the Filter expressions section. Table format You can organize results of a query into tables by directing the form parameter to return a table. Two forms of table are available: table and header table. When you specify the form parameter as table, ?form=table , events are returned in row column form. Results include a collection named Columns that lists column name and type and a collection named Rows containing a collection of rows matching the order of the columns. When you specify a form of type table-headers , ?form=tableh , it results in a collection where the Rows collection contains a column header list. For details on table formats, see the Table format section. SdsBoundaryType The SdsBoundaryType enum defines how data on the boundary of queries is handled: around the start index for range value queries, and around the start and end index for window values. The following are valid SdsBoundaryType values: Boundary Enumeration value Operation Exact 0 Results include the event at the specified index boundary if a stored event exists at that index. Inside 1 Results include only events within the index boundaries. Outside 2 Results include up to one event that falls immediately outside of the specified index boundary. ExactOrCalculated 3 Results include the event at the specified index boundary. If no stored event exists at that index, one is calculated based on the index type and interpolation and extrapolation settings. SdsSearchMode The SdsSearchMode enum defines search behavior when seeking a stored event near a specified index. The following are valid values for SdsSearchMode : Mode Enumeration value Operation Exact 0 If a stored event exists at the specified index, that event is returned. Otherwise, no event is returned. ExactOrNext 1 If a stored event exists at the specified index, that event is returned. Otherwise, the next event in the stream is returned. Next 2 Returns the stored event after the specified index. ExactOrPrevious 3 If a stored event exists at the specified index, that event is returned. Otherwise, the previous event in the stream is returned. Previous 4 Returns the stored event before the specified index. Transforming data SDS provides the ability to transform data upon reads. The supported data transformations are: Reading with SdsStreamViews : Changing the shape of the returned data Unit of Measure Conversions : Converting the unit of measure of the data Data transformations are supported for all single stream reads, but transformations have specific endpoints. The following are the base URIs for the transformation endpoints: api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/First api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform First api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Last api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Last api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Interpolated api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Interpolated api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Summaries api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Summaries api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/Sampled api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Sampled Reading with SdsStreamViews When you transform data with an SdsStreamView, the data read is converted to the target type specified in the SdsStreamView. For details on working with stream views, see the Stream Views section. All stream view transformations are GET HTTP requests. You specify the stream view by appending the stream view identifier to requests to the transformation endpoint. For example, the following request would return the first event in the stream as the target type in the stream view specified by the streamViewId : GET api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform/First?streamViewId={streamViewId} api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform First?streamViewId={streamViewId} All single stream data reads support stream view transformations. When you request data with an SdsStreamView, the read characteristics defined by the target type of the SdsStreamView determine what is returned. The read characteristics are discussed in the code samples. Unit conversion of data SDS supports assigning Units of Measure (UOM) to stream data. If stream data has UOM information associated, SDS supports reading data with unit conversions applied. On each read data request, unit conversions are specified by a user defined collection of SdsStreamPropertyOverride objects in read requests. The SdsStreamPropertyOverride object has the following structure: Property Type Optionality Description SdsTypePropertyId String Required Identifier for an SdsTypeProperty with a UOM assigned. Uom String Required Target unit of measure. InterpolationMode SdsInterpolationMode N/A N A Currently not supported in context of data reads. This is supported in the REST API through HTTP POST calls with a request body containing a collection of SdsStreamPropertyOverride objects. All unit conversions are POST HTTP requests. The unit conversion transformation URI is as follows: POST api/v1/Tenants/default/Namespaces/{namespaceId}/Streams/{streamId}/Data/Transform api v1 Tenants default Namespaces {namespaceId} Streams {streamId} Data Transform Request body The Request Body contains a collection of SdsStreamPropertyOverride objects. The example request body below requests SDS to convert the Measurement property of the returned data from meter to centimeter. [ { \"SdsTypePropertyId\" : \"Measurement\", \"Uom\" : \"centimeter\" } ] All single stream data reads with streams that have specified UOMs support UOM conversions."
                                 },
    "V1/Overview/CommandLineQuickStartWindows.html":  {
                                                          "href":  "V1/Overview/CommandLineQuickStartWindows.html",
                                                          "title":  "Command line quick start - Windows",
                                                          "keywords":  "Command line quick start - Windows This topic provides quick start instructions for using EdgeCmd in the Windows environment. You can utilize the edgecmd.exe command line tool to configure Edge Data Store, in addition to curl commands and REST calls. You can install EdgeCmd from a Windows command prompt. Complete the following to access EdgeCmd on Windows: Complete the following to access EdgeCmd on Windows: Open a command prompt. Navigate to the directory where Edge Data Store is installed. This is usually in the following location: C:\\Program Files\\OSIsoft\\EdgeDataStore\\ Do not copy or delete any files in that directory. Instead, use the full path from a different directory to invoke the tool: C:\\Users\\John\u003e\"C:\\Program Files\\OSIsoft\\EdgeDataStore\\edgecmd.exe\" Help The EdgeCmd utility launches, displaying the following introductory material and a command prompt at the end: ************************************************************************************************************************ Welcome to OSIsoft Edge Data Store configuration utility. Utility version: 1.0.0.148 ************************************************************************************************************************ --------------------------------------------------------------------------------------------------------- Command-line options =\u003e \u0027Configuration\u0027, \u0027Help\u0027 --------------------------------------------------------------------------------------------------------- Please enter ID of a component you would like to configure or to get component specific help output. Example: .\\edgecmd.exe Help ComponentId .\\edgecmd.exe Configuration ComponentId To get set of components registered to the Edge Data Store please run: .\\edgecmd.exe Configuration System Components To configure the system, please use \u0027System\u0027 as the ComponentId. Example of getting System help output: .\\edgecmd.exe Help System Example of configuring System Logging level: .\\edgecmd.exe Configuration System logging LogLevel=Warning C:\\Users\\John\u003e"
                                                      },
    "V1/Overview/CommandLineLinuxQuickStart.html":  {
                                                        "href":  "V1/Overview/CommandLineLinuxQuickStart.html",
                                                        "title":  "Command line quick start - Linux",
                                                        "keywords":  "Command line quick start - Linux This topic provides quick start instructions for using EdgeCmd in the Linux environment. You can utilize the edgecmd.exe command line tool to configure Edge Data Store, in addition to curl commands and REST calls. You can install EdgeCmd from the Linux command prompt. Complete the following to access edgecmd on Linux: Open a command prompt. Enter the following command to start the edgecmd.exe tool from any directory. debian@beaglebone:~$ edgecmd help Type edgecmd help and press Enter. The EdgeCmd utility launches, displaying the following introductory material and a command prompt at the end: ************************************************************************************************************************ Welcome to OSIsoft Edge Data Store configuration utility. Utility version: 1.0.0.148 ************************************************************************************************************************ --------------------------------------------------------------------------------------------------------- Command-line options =\u003e \u0027Configuration\u0027, \u0027Help\u0027 --------------------------------------------------------------------------------------------------------- Please enter ID of a component you would like to configure or to get component specific help output. Example: ./edgecmd . edgecmd Help ComponentId ./edgecmd . edgecmd Configuration ComponentId To get set of components registered to the Edge Data Store please run: ./edgecmd . edgecmd Configuration System Components To configure the system, please use \u0027System\u0027 as the ComponentId. Example of getting System help output: ./edgecmd . edgecmd Help System Example of configuring System Logging level: ./edgecmd . edgecmd Configuration System logging LogLevel=Warning debian@beaglebone:~$"
                                                    },
    "V1/Overview/AnalyticsQuickStart.html":  {
                                                 "href":  "V1/Overview/AnalyticsQuickStart.html",
                                                 "title":  "Edge Data Store analytics quick start",
                                                 "keywords":  "Edge Data Store analytics quick start This topic provides a quick start for a very simple analytic you can write using Edge Data Store. The intended input device is an Modbus TCP EDS adapter or other sensor that outputs 4 Boolean values. The normal range of operation is that the values are neither all true or all false. If all values are true, the exception condition High is triggered. If all values are false, the exception condition Low is triggered. Any other combination of Boolean values is Normal. Three analytic streams are created to track these changes. The ValueRangeHigh stream is 1 when High and 0 when anything else. The ValueRangeLow stream is -1 when Low and 0 when anything else. The ValueRangeOut stream is -1 when Low, 0 when Normal, and 1 when High. This example assumes Edge Data Store was installed with the default port (5590): using System; using System.Collections.Generic; using System.Net.Http; using System.Text; using Newtonsoft.Json; using Newtonsoft.Json.Linq; namespace ExceptionReportingSample { class ModbusField { public string StreamId { get; set; } public int ScanRate { get; set; } } enum Alert { Normal, High, Low } class ExceptionReporting { static HttpClient _client = new HttpClient(); private static List\u003cstring\u003e StreamIds = new List\u003cstring\u003e(new string[] { \"SwitchState1\", \"SwitchState2\", \"SwitchState3\", \"SwitchState4\" }); private const string ValueRangeHigh = \"ValueRangeHigh\"; private const string ValueRangeLow = \"ValueRangeLow\"; private const string ValueRangeOut = \"ValueRangeOut\"; private const string TypeId = \"ValueRange\"; private const string ModbusComponentId = \"Modbus1\"; private const double lowValue = -1.0; private const double highValue = 1.0; private const double normalValue = 0.0; public static Alert _alert = Alert.Normal; static TimeSpan GetPollingIntervalFromModbus(string modbusComponentId, List\u003cstring\u003e StreamIds) { int pollingMilliseconds = 5000; string endpoint = $\"http://localhost:5590/api/v1/configuration/{modbusComponentId}/DataSelection\"; $\"http:  localhost:5590 api v1 configuration {modbusComponentId} DataSelection\"; string modbusConfig = _client.GetStringAsync(endpoint).Result; List\u003cModbusField\u003e values = JsonConvert.DeserializeObject\u003cList\u003cModbusField\u003e\u003e(modbusConfig); foreach (var value in values) { foreach (string StreamId in StreamIds) { if (StreamId == value.StreamId \u0026\u0026 value.ScanRate \u003c pollingMilliseconds) { pollingMilliseconds = value.ScanRate; } } } return TimeSpan.FromMilliseconds(pollingMilliseconds); } static bool GetStreamValue(string StreamId) { bool value = false; string lastValueUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/Data/Last\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} Data Last\", StreamId); string lastValueJson = _client.GetStringAsync(lastValueUri).Result; Dictionary\u003cstring, object\u003e values = JsonConvert.DeserializeObject\u003cDictionary\u003cstring, object\u003e\u003e(lastValueJson); object objValue = values[\"Value\"]; if (objValue is Boolean) value = (bool)objValue; return value; } static bool FindOrCreateType(string typeId) { string typeUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/types/{0}\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default types {0}\", typeId); HttpResponseMessage response = _client.GetAsync(typeUri).Result; if (response.IsSuccessStatusCode) return true; string typeJson = @\"{\"\"Id\"\": \"\"\" + typeId + @\"\"\",\"\"Name\"\": \"\"\" + typeId + @\"\"\",\"\"SdsTypeCode\"\": 1,\"\"Properties\"\": [{\"\"Id\"\": \"\"Time\"\",\"\"Name\"\": \"\"Time\"\",\"\"IsKey\"\": true,\"\"SdsType\"\": {\"\"SdsTypeCode\"\": 16}},{\"\"Id\"\": \"\"Measurement\"\",\"\"Name\"\": \"\"Measurement\"\",\"\"SdsType\"\": {\"\"SdsTypeCode\"\": 14}}]}\"; var content = new StringContent(typeJson, Encoding.UTF8, \"application/json\"); \"application json\"); response = _client.PostAsync(typeUri, content).Result; return response.IsSuccessStatusCode; } static bool FindOrCreateStream(string streamId, string typeId) { string streamUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} \", streamId); HttpResponseMessage response = _client.GetAsync(streamUri).Result; if (response.IsSuccessStatusCode) return true; string streamJson = @\"{\"\"Id\"\": \"\"\" + streamId + @\"\"\",\"\"Name\"\": \"\"\" + streamId + @\"\"\",\"\"TypeId\"\": \"\"\" + typeId + @\"\"\"}\"; var content = new StringContent(streamJson, Encoding.UTF8, \"application/json\"); \"application json\"); response = _client.PostAsync(streamUri, content).Result; return response.IsSuccessStatusCode; } static bool WriteStreamValue(string StreamId, double value, DateTime timestamp) { string dataUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/{0}/Data\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces default streams {0} Data\", StreamId); string dataJson = @\"[{\"\"Time\"\": \"\"\" + timestamp.ToString(\"o\") + @\"\"\",\"\"Measurement\"\":\" + value.ToString() + \"}]\"; var content = new StringContent(dataJson, Encoding.UTF8, \"application/json\"); \"application json\"); HttpResponseMessage response = _client.PostAsync(dataUri, content).Result; return response.IsSuccessStatusCode; } static Alert CheckStatus(int numberTrue) { if (numberTrue \u003e 3) return Alert.High; if (numberTrue \u003c 1) return Alert.Low; return Alert.Normal; } static bool ReportChange(Alert oldAlert, Alert newAlert) { bool success = true; DateTime now = DateTime.UtcNow; switch (oldAlert) { case Alert.Normal: if (Alert.Low == newAlert) { return WriteStreamValue(ValueRangeLow, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, lowValue, now); } if (Alert.High == newAlert) { return WriteStreamValue(ValueRangeHigh, highValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, highValue, now); } break; case Alert.High: if (Alert.Low == newAlert) { return WriteStreamValue(ValueRangeLow, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, lowValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, normalValue, now); } if (Alert.Normal == newAlert) { return WriteStreamValue(ValueRangeOut, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, normalValue, now); } break; case Alert.Low: if (Alert.Normal == newAlert) { return WriteStreamValue(ValueRangeOut, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeLow, normalValue, now); } if (Alert.High == newAlert) { return WriteStreamValue(ValueRangeLow, normalValue, now) \u0026\u0026 WriteStreamValue(ValueRangeOut, highValue, now) \u0026\u0026 WriteStreamValue(ValueRangeHigh, highValue, now); } break; default: break; } return success; } static void Main(string[] args) { TimeSpan pollingInterval = GetPollingIntervalFromModbus(ModbusComponentId, StreamIds); FindOrCreateType(TypeId); FindOrCreateStream(ValueRangeHigh, TypeId); FindOrCreateStream(ValueRangeLow, TypeId); FindOrCreateStream(ValueRangeOut, TypeId); while (true) { int numberTrue = 0; foreach (string StreamId in StreamIds) { bool value = GetStreamValue(StreamId); if (value) numberTrue++; } Alert currentAlert = CheckStatus(numberTrue); if (currentAlert != _alert) { if (ReportChange(_alert, currentAlert)) { _alert = currentAlert; } } System.Threading.Thread.Sleep(pollingInterval); Console.WriteLine(\"ValueRange should be \" + _alert); } } } }"
                                             },
    "V1/Modbus/SupportedFeaturesModbus.html":  {
                                                   "href":  "V1/Modbus/SupportedFeaturesModbus.html",
                                                   "title":  "Supported features",
                                                   "keywords":  "Supported features Register types The Modbus TCP EDS adapter supports 6 register types, corresponding to 4 function codes (1-4). Since one function code can return two types of registers, either 16-bit or 32-bit depending on the device, either the register type or the register type code is required when configuring the data selection for the adapter. The following table lists all the register types supported in the Modbus TCP EDS adapter. Register Type Register Type Code Description Function Code Coil 1 Read Coil Status 1 Discrete 2 Read Discrete Input Status 2 Holding16 3 Read 16-bit Holding Registers 3 Holding32 4 Read 32-bit Holding Registers 3 Input16 6 Read 16-bit Input Registers 4 Input32 7 Read 32-bit Input Registers 4 When reading from function codes 1 and 2 , the adapter expects these to be returned as single bits. For function codes 3 and 4 , the adapter expects 16 bits to be returned from devices that contain 16-bit registers and 32 bits to be returned from devices that contain 32-bit registers. Data types The Modbus TCP EDS adapter converts readings from single or multiple registers into the data types specified by the data type code and populates the value into streams created in the Edge Data Store. The following table lists all data types with their corresponding type codes supported by the Modbus TCP EDS adapter. Data type code Data type name Value type Register type Description 1 Boolean Bool Bool 0 = false 1 = true 10 Int16 Int16 Bool/16-bit Bool 16-bit Read 1 Modbus TCP register and interpret as a 16-bit integer. Bytes [BA] read from the device are stored as [AB]. 20 UInt16 UInt16 Bool/16-bit Bool 16-bit Read 1 Modbus TCP register and interpret as an unsigned 16-bit integer. Bytes [BA] read from the device are stored as [AB]. 30 Int32 Int32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit integer. Bytes [DCBA] read from the device are stored as [ABCD]. 31 Int32ByteSwap Int32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit integer. Bytes [BADC] read from the device are stored as [ABCD]. 100 Float32 Float32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit float. Bytes [DCBA] read from the device are stored as [ABCD]. 101 Float32ByteSwap Float32 16-bit/32-bit 16-bit 32-bit Read 32 bits from the Modbus TCP device and interpret as a 32-bit float. Bytes [BADC] read from the device are stored as [ABCD]. 110 Float64 Float64 16-bit/32-bit 16-bit 32-bit Read 64 bits from the Modbus TCP device and interpret as a 64-bit float. Bytes [HGFEDCBA] read from the device are stored as [ABCDEFGH]. 111 Float64ByteSwap Float64 16-bit/32-bit 16-bit 32-bit Read 64 bits from the Modbus TCP device and interpret as a 64-bit float. Bytes [BADCFEHG] read from the device are stored as [ABCDEFGH]. 1001 - 1250 String String 16-bit/32-bit 16-bit 32-bit 1001 reads a one-character string, 1002 reads a two-character string, and 1003 reads a three-character string and so on. Bytes [AB] are interpreted as \"AB\". 2001 - 2250 StringByteSwap String 16-bit/32-bit 16-bit 32-bit 2001 reads a one-character string, 2002 reads a two-character string, and 2003 reads a three-character string and so on. Bytes [BA] are interpreted as \"AB\". Apply bitmap The Modbus TCP EDS adapter supports applying bitmaps to the value converted from the readings from the Modbus TCP devices. A bitmap is a series of numbers used to extract and reorder bits from a word register. The format of the bitmap is uuvvwwxxyyzz, where uu, vv, ww, yy, and zz each refer to a single bit. A leading zero is required if the referenced bit is less than 10. The low-order bit is 01 and high-order bit is either 16 or 32. Up to 16 bits can be referenced for a 16-bit word (data types 10 and 20) and up to 32 bits can be referenced for a 32-bit word (data type 30 and 31). For example, the bitmap 0307120802 will map the second bit of the original word to the first bit of the new word, the eighth bit to the second bit, the twelfth bit to the third bit, and so on. The high-order bits of the new word are padded with zeros if they are not specified. Not all data types support applying bitmap. The data types supporting bitmap are: Int16 (Data type code 10) UInt16 (Data type code 20) Int32 (Data type code 30 and 31) Apply data conversion The Modbus TCP EDS adapter supports applying data conversion to the value converted from reading the Modbus TCP devices. A conversion factor and conversion offset can be specified. The conversion factor is used for scaling the value up or down, and the conversion offset is used for shifting the value. The mathematical equation used in conversion is the following: \u003cAfter Conversion\u003e = \u003cBefore Conversion\u003e /   Factor - Offset Not all data types support applying data conversion. Data types that support data conversion are: Int16 (Data type code 10) UInt16 (Data type code 20) Int32 (Data type code 30 and 31) Float32 (Data type code 100 and 101) The value with data conversion applied will always be converted to the 32-bit float type to maintain the precision of the conversion factor and conversion offset."
                                               },
    "V1/Diagnostics/Diagnostics.html":  {
                                            "href":  "V1/Diagnostics/Diagnostics.html",
                                            "title":  "Diagnostics configuration",
                                            "keywords":  "Diagnostics configuration Edge Data Store and its components produce diagnostic data which is stored locally in the Storage component, and may be queried locally or egressed to PI Web API endpoints or the OSIsoft Cloud Services or both. Diagnostic data is always produced and saved - it cannot be disabled. Diagnostic data is stored within the \u0027diagnostics\u0027 namespace of Edge Storage. Local access to this data is available through the Sds methods. Egress diagnostics data through PeriodicEgressEndpoints To egress diagnostics related data, you must specify diagnostics as the NamespaceId in the periodic egress endpoint configuration. For details and instructions, see Data egress configuration . Edge Data Store diagnostics The Diagnostics.System dynamic type includes these values which are logged in a stream with the id System.Diagnostics. This diagnostic stream contains system level information related to the host platform that Edge Data Store is running on. Type Property Description string timestamp Timestamp of event int ProcessIdentifier Process id of the host process string StartTime When the host process started long WorkingSet Amount of physical memory, in bytes, allocated for the host process double TotalProcessorTime (uom=s) Total processor time for the host process expressed in seconds double TotalUserProcessorTime (uom=s) User processor time for the host process expressed in seconds double TotalPrivilegedProcessorTime (uom=s) Privileged processor time for the host process expressed in seconds int ThreadCount Number of threads in the host process int HandleCount Number of handles opened by the host process double ManagedMemorySize (uom=MB) Number of bytes currently thought to be allocated in managed memory double PrivateMemorySize (uom=MB) Amount of paged memory, in bytes, allocated for the host process double PeakPagedMemorySize (uom=MB) Maximum amount of memory in the virtual memory paging file, in bytes, used by the host process. double StorageTotalSize (uom=MB) Total size of the storage medium in use by the Edge Data Store double StorageFreeSpace (uom=MB) Free space available EDS adapter diagnostics Each EDS adapter of the Edge Data Store produces its own diagnostics streams. Stream count The Diagnostics.StreamCountEvent dynamic type includes these values, which are logged in a stream with the id {componentid}.StreamCount. The stream count and type count include only types and streams created for sequential data received from a data source. Type Property Description string timestamp Timestamp of event int StreamCount Number of streams created by the adapter instance int TypeCount Number of types created by the adapter instance IO rate The Diagnostics.Adapter.IORate dynamic type includes these values, which are logged in a stream with the id {componentid}.IORate. IO rate includes only sequential data collected from a data source. Type Property Description string timestamp Timestamp of event double IORate 10-minute rolling average of data rate (streams/second) (streams second) Error rate The Diagnostics.Adapter.ErrorRate dynamic type includes these values, and are logged in a stream with the id {componentid}.ErrorRate. Type Property Description string timestamp Timestamp of event double ErrorRate 10-minute rolling average of error rate (streams/second) (streams second) Edge Storage diagnostics The Storage component of Edge Data Store produces the following diagnostics streams. Storage.default.default.Counts The Storage.default.default.Counts stream includes counts of the types, streams and stream views of the default namespace. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views Storage.default.diagnostics.Counts The Storage.default.default.Counts stream includes counts of the types, streams and stream views of the diagnostics namespace. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views Storage.Total.Counts The Storage.Totals.Counts stream includes counts of the types, streams and stream views of all namespaces of the storage component. Type Property Description string timestamp Timestamp of event integer TypeCount Count of types integer StreamCount Count of streams integer StreamViewCount Count of stream views"
                                        },
    "V1/Design/ScalePerformance.html":  {
                                            "href":  "V1/Design/ScalePerformance.html",
                                            "title":  "Design considerations",
                                            "keywords":  "Design considerations Before you install EDS, you need to determine your storage and throughput needs so that you select devices that meet your needs. Edge Storage role The Edge Storage component integrated with the Edge Data Store is a new item in the OSIsoft software ecosystem. It is not designed to replace any existing storage technology produced by OSIsoft. The Edge Storage component is intended as a data store that is resilient and reliable but limited in duration and scope, as appropriate for an Edge software component. The storage component is configured by default to roll off data in a FIFO (first in first out) process: as new data comes in and the size of streams exceeds the configured limits, older data is purged. If data exists in the Edge Storage component that needs to be permanently retained, you should egress it to either PI Data Archive (using the PI Web API OMF endpoint) or to OSIsoft Cloud Services, using the OCS OMF ingress endpoint. Edge Storage scale The Edge Storage component provides an appropriate level of storage performance for small devices. For the smallest of these devices, throughput may be limited to tens of events per second. For larger devices with faster processors, memory and storage, this could increase to up to 3,000 events per second. The Edge Storage component\u0027s design is focused on small devices in Edge scenarios: if high throughput or large stream counts are required, OSIsoft Cloud Services or PI Data Archive are more appropriate choices. Sizing of Edge devices For Edge Data Store, there are three supported tiers of performance: Small Devices: 1 Core CPU, 512 MB RAM. 30 events/second, events second, 200 streams total. Medium Devices: 2 Core CPU, 1 GB RAM. 300 events/second, events second, 2000 streams total. Large Devices: 4 Core CPU, 4 GB RAM, SSD storage. 3000 events/second, events second, 3000 streams total. These performance metrics assume solid state storage, which is commonly used in Edge devices."
                                        },
    "V1/Administration/LoggingConfiguration.html":  {
                                                        "href":  "V1/Administration/LoggingConfiguration.html",
                                                        "title":  "Logging configuration",
                                                        "keywords":  "Logging configuration Edge Data Store writes daily log messages to flat text files in the following locations: ??? Windows: %ProgramData%/OSIsoft/EdgeDataStore/Logs %ProgramData% OSIsoft EdgeDataStore Logs ??? Linux: /usr/share/OSIsoft/EdgeDataStore/Logs  usr share OSIsoft EdgeDataStore Logs Each message in the log displays the message severity level, timestamp, and the message itself. Default logging configuration and schema By default, logging captures Information, Warning, Error, and Critical messages in the message logs. The following logging configuration is the default for a component on install: { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } The schema file specifies how to formally describe the configuration parameters for message logging. It is located in: ??? Windows: %ProgramFiles%/OSIsoft/EdgeDataStore/Schema %ProgramFiles% OSIsoft EdgeDataStore Schema ??? Linux: /opt/OSIsoft/EdgeDataStore/Schema  opt OSIsoft EdgeDataStore Schema Log levels The logLevel sets the minimum severity for messages to be included in the logs. Messages with a severity below the level set are not included. The log levels in their increasing order of severity are as follows: Trace, Debug, Information, Warning, Error, Critical. Table: General guidelines for setting the log level. Level Description Trace Logs that contain the most detailed messages. These messages may contain sensitive application data like actual received values and should not be enabled in a production environment. Debug Logs that can be used to troubleshoot data flow issues by recording metrics and detailed flow related information. Information Logs that track the general flow of the application. Any non-repetitive general information (like version information relating to the software at startup, what external services are being used, data source connection string, number of measurements, egress URL, change of state ???Starting???, ???Stopping???, or configuration) can be useful for diagnosing potential application errors. Warning Logs that highlight an abnormal or unexpected event in the application flow, but does not otherwise cause the application execution to stop. Warning messages can indicate an unconfigured data source state, that a communication with backup failover instance has been lost, an insecure communication channel in use, or any other event that could require attention, but that does not impact data flow. Error Logs that highlight when the current flow of execution is stopped due to a failure. These should indicate a failure in the current activity, not an application-wide failure. This can indicate an invalid configuration, unavailable external endpoint, internal flow error, and so on. Critical Logs that describe an unrecoverable application or system crash, or a catastrophic failure that requires immediate attention. This can indicate application wide failures like beta timeout expired, unable to start self-hosted endpoint, unable to access vital resource (for example, Data Protection key file), and so on. Log file count limit The logFileCountLimit controls the number of days the log files are stored before they are deleted. Change logging configuration Complete the following to change the logging configuration: Update the parameters of the message logging configuration JSON file that you want as needed. For example, the Component_Logging.json file: { \"logLevel\": \"Warning\", \"logFileSizeLimitBytes\": 16777216, \"logFileCountLimit\": 30 } Save the file. Use any tool capable of making HTTP requests to execute a PUT command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging . Note: Replace \u003cComponentId\u003e with the ComponentId of the adapter or Storage, for example OpcUa1 . Example using curl (run this command from the same directory where the file is located): curl -i -d \"@Component_Logging.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/\u003cComponentId\u003e/Logging http:  localhost:5590 api v1 configuration \u003cComponentId\u003e Logging On successful execution, the log level change takes effect immediately during runtime. The other configurations (log file size and file count) get updated after Edge Data Store is restarted. Note: Any parameter not specified in the updated configuration file will revert to the default schema value."
                                                    },
    "docfx.console.2.43.2/content/index.html":  {
                                                    "href":  "docfx.console.2.43.2/content/index.html",
                                                    "title":  "This is the HOMEPAGE.",
                                                    "keywords":  "This is the HOMEPAGE . Refer to Markdown for how to write markdown files. Quick Start Notes: Add images to images folder if the file is referencing an image."
                                                },
    "V1/SDS/Filter_Expressions.html":  {
                                           "href":  "V1/SDS/Filter_Expressions.html",
                                           "title":  "Filter expressions",
                                           "keywords":  "Filter expressions You can apply filter expressions to certain read operations that return Sequential Data Store values, including: Get Values and Get Summaries . SdsTypeCodes The following types are supported for use within a filter expression: Boolean Byte Char DateTime DateTimeOffset Decimal Double Guid Int16 Int32 Int64 Sbyte Single String Timespan UInt16 UInt32 Uint64 The following types are not supported for use within a filter expression: Array IEnumerable IDictionary IList SdsType SdsTypeProperty Nullable Types Logical operators The following logical operators are supported for use within a filter expression: Operator Description eq Equal to ne Not equal ge Greater than or equal to le Less than or equal to lt Less than gt Greater than ( ) Parenthesis can be used to affect the order of the operation or Or logical operator and And logical operator not Not logical operator - Negation Logical operator examples For the following examples, you can assume that the SDS Type event includes a field named Value of type double : Value eq 1.0 Value ne 15.6 Value ge 5.0 Value le 8.0 Value gt 5.0 Value lt 4.0 Value gt 2.0 and Value lt 9.0 Value gt 6.0 or Value lt 2.0 not (Value eq 1.0) Math functions The following math functions are supported for use within a filter expression: Function Description add Addition sub Subtraction mul Multiplication div Division mod Modulo round Rounds to the nearest numeric component without a decimal, with the midpoint rounded away from 0. For example, 0.5 rounds to 1; -0.5 rounds to -1) floor Rounds down to the nearest numeric component without a decimal ceiling Rounds up to the nearest numeric component without a decimal Math function examples For the following examples, you can assume that the SDS Type event includes a field named Value of type double : Value eq (6.0 add 3.0) Value eq (6.0 sub 3.0) Value eq (6.0 mul 3.0) Value eq (6.0 div 3.0) Value eq (7.0 mod 3.0) round(Value) eq 16 floor(Value) eq 15 ceiling(Value) eq 16 String functions String operations are case sensitive. The character index in a string is 0-based. The following string functions are supported for use within a filter expression: Function Description endswith Compare the character at the end of the input string startswith Compare the character at the start of the input string length Examines the string length indexof Examines the character starting at a given index substring Examine characters within another string at a specific location contains Search for characters anywhere in another string tolower Convert characters to lowercase toupper Convert characters to uppercase trim Remove whitespace from front and end of a string concat Concatenate strings together replace Replace one set of characters with another String function examples For the following examples, you can assume that the SDS Type event includes a field named sValue of type string : Example Result endswith(sValue, \u0027XYZ\u0027) True if sValue ends with the characters ???XYZ??? startswith(sValue, \u0027Val\u0027 True if sValue starts with the characters ???Val??? length(sValue) eq 11 True if sValue is 11 characters indexof(sValue, \u0027ab\u0027) eq 4 True if the 5th and 6th characters are ???ab??? contains(sValue, \u0027ab\u0027) True if characters ???ab??? are found anywhere in sValue substring(sValue, 10) eq \u0027a b\u0027 True if ???a b??? is found in sValue at index 10 tolower(sValue) eq \u0027val5\u0027 Change sValue to lowercase and compare to ???val5??? toupper(sValue) eq \u0027ABC\u0027 Change sValue to uppercase and compare to ???ABC??? trim(sValue) eq \u0027vall22\u0027 Trim whitespace from front and end of sValue and compare to ???val22??? concat(sValue,\u0027xyz\u0027) eq \u0027dataValue_7xyz\u0027 Add characters to sValue and compare to ???dataValue_7xyz??? replace(sValue,\u0027L\u0027,\u0027D\u0027) eq \u0027Dog1\u0027 Replace any ???L??? in sValue with ???D??? and compare to ???Dog1??? DateTime functions The following DateTime functions are supported for use within a filter expression: Function Description year Get year value from DateTime month Get month value from DateTime day Get day value from DateTime hour Get hour value from DateTime minute Get minute value from DateTime second Get second value from DateTime DateTime function examples For the following examples, you can assume that the SDS Type event includes a field named TimeId of type DateTime : year(TimeId) eq 2015 month(TimeId) eq 11 day(TimeId) eq 3 hour(TimeId) eq 1 minute(TimeId) eq 5 second(TimeId) eq 3 TimeSpan functions The following TimeSpan functions are supported for use within a filter expression: Function Description years Get year value from TimeSpan days Get day value from TimeSpan hours Get hour value from TimeSpan minutes Get minute value from TimeSpan seconds Get second value from TimeSpan TimeSpan function examples For the following examples, you can assume that the SDS Type event includes a field named TimeSpanValue of type TimeSpan : years(TimeSpanValue) eq 1 days(TimeSpanValue) eq 22 hours(TimeSpanValue) eq 1 minutes(TimeSpanValue) eq 1 seconds(TimeSpanValue) eq 2"
                                       },
    "V1/SDS/Compression.html":  {
                                    "href":  "V1/SDS/Compression.html",
                                    "title":  "Compression",
                                    "keywords":  "Compression To more efficiently utilize network bandwidth, the EDS Sequential Data Store supports compression for reading data and writing data through the REST API. Supported compression schemes gzip deflate Request compression (writing data) You can compress the body content of an HTTP request by using the supported compression schemes , which allow you to send stream values to the REST API more efficiently. You must use the Content-Encoding HTTP header to specify the compression scheme of compressed-content requests. This header provides context to the API to properly decode the request content. Response compression (reading data) You can request compressed responses from the REST API by specifying one of the supported compression schemes using the Accept-Encoding HTTP header. Compressed responses from the REST API will include a Content-Encoding HTTP header indicating the compression scheme used to compress the response content. Note: Specifying a compression scheme through the use of the Accept-Encoding HTTP header does not guarantee a compressed response. Always refer to presence and value of the Content-Encoding HTTP header of the response to properly decode the response content."
                                },
    "V1/ReleaseNotes/ReleaseNotes.html":  {
                                              "href":  "V1/ReleaseNotes/ReleaseNotes.html",
                                              "title":  "Release notes",
                                              "keywords":  "Release notes Overview Edge Data Store is supported on a variety of platforms and processors. OSIsoft provides ready to use install kits for the following platforms: Windows 10 x64 - EdgeDataStore.msi (Intel/AMD (Intel AMD 64-bit processors) Debian 9 or later x64/AMD64 x64 AMD64 - EdgeDataStore_linux-x64.deb (Intel/AMD (Intel AMD 64-bit processors) Debian 9 or later ARM32 - EdgeDataStore_linux-arm.deb (Raspberry PI 2,3,4, BeagleBone devices, other ARM v7 and ARM v8 32-bit processors) Debian 9 or later arm64 - EdgeDataStore_linux-arm64.deb (Raspberry PI 3,4 (Ubuntu ARM64 Server), Google Coral Dev Board, Nvidia Nano Jetson In addition to ready to use install kits, OSIsoft also provides examples of how to create Docker containers in a separate file. tar.gz files are provided with binaries for customers who want to build their own custom installers or containers for Linux. Differences from Release Candidate 2 General Many performance and resiliency improvements were made to the product. Issues related to long-term memory growth of the product have been addressed. The \"System Reset\" and \"Storage Reset\" features of the Edge Data Store are more resilient. The installation kits for the Edge Data Store were updated to create a better experience when upgrading installed instances. The installation kits have been updated to ensure only necessary files are included. The \"OSIsoft Edge System\" product was renamed to \"OSIsoft Edge Data Store\". The EdgeCmd command line utility is now provided to allow access to and modification of Edge Data Store configuration. This utility supersedes the command line functionality that was previously available via OSIsoft.Data.System.Host. Improvements were made to ensure component health status updates may not be lost when the product is shutdown. When the reset functionality for the entire product or the storage component is invoked, the product now properly restarts. The OPCUA and Modbus adapters may now be enabled at install time of the product. The structure for health streams produced by the product has been updated. Adapter components may be added or removed at runtime and no longer require a restart of the product. Changes to the Health Endpoints configuration are now applied at runtime and no longer require a restart of the product. All endpoint configurations related to transferring data and configuration to PI Web API or OSIsoft Cloud Services have the following new properties: ValidateEndpointCertificate - Enable/Disable Enable Disable validation of endpoint certificate. Any endpoint certificate is accepted if set to false. TokenEndpoint - For use with OSIsoft Cloud Services endpoints only. Allows for alternative endpoint for retrieval of an OCS access token. Modbus Adapter Improved Modbus adapter performance when large data points are selected for data collection. Modbus data selection can be deselected via EdgeCmd, which was not supported in RC2. Modbus logs the data source and data selection configuration in verbose mode instead of information mode. OPC UA Adapter An option was added to override SDK certificate checks when NONE security is configured. Storage Backfilling can now be enabled/disabled enabled disabled by changing the value of the backfill property on an egress configuration. Previously, the configuration needed to be removed and re-added with the changed value. For Linux installations, the number of allowed file handles for the service has been tuned to allow for up to 5,000 data streams. Fixed an issue with periodic egress where, in very specific scenarios, adding values immediately after egress started would result in only the last value being sent rather than all the values just added. Attempts to update to an egress configuration with changes identical to the current configuration are now ignored. Issues related to data queries returning incorrect data were addressed. The type identifier of the underlying storage component for Edge Data Store was changed from \"EDS.Component\" to \"Storage\". Install Edge Data Store on a Device using an install kit To use any of the installers, first copy the appropriate file to the file system of the device. Windows (Windows 10 x64) Double click the EdgeDataStore.msi file in Windows Explorer or execute the file from a command prompt. You will be prompted for install location and default port, and when the install finishes, Edge Data Store will be installed and running on either the default port 5590 or the port you specified during the install. Debian 9 or later Linux (Ubuntu Raspberry PI, BeagleBone, other Debian based Linux distros) Complete the following: Open a terminal window and type: sudo apt install ./EdgeDataStore_linux_\u003ceither . EdgeDataStore_linux_\u003ceither x64 or arm depending upon processor\u003e.deb A check will be done for prerequisites. If the Linux operating system is up to date, the install will succeed. If the install fails, run the following commands from the terminal window and try the install again: sudo apt update sudo apt upgrade After the check for prerequisites succeeds, a prompt will display asking if you want to change the default port (5590). If you want to change the port, type in another port number in the acceptable range for the operating system you are using. If 5590 is acceptable, press Enter. The install will complete, and Edge Data Store will be running on your device. Optional: To verify that Edge Data Store is correctly installed, run the following script from the terminal window. Note: Depending on the processor, memory, and storage, it may take the system a few seconds to start up. curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If the installation was successful, you will get back a JSON copy of the default system configuration: { \"Modbus1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": {}, \"DataSelection\": [] }, \"OpcUa1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": {}, \"DataSelection\": [] }, \"Storage\": { \"PeriodicEgressEndpoints\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableTransactionLog\": true }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"Storage\" } ] } } If you get back an error, wait a few seconds and try it again. On a device with limited processor, memory, and slow storage, it may take some time before Edge Data Store is fully initialized and running for the first time."
                                          },
    "V1/Modbus/ModbusTCPDataSelectionConfiguration.html":  {
                                                               "href":  "V1/Modbus/ModbusTCPDataSelectionConfiguration.html",
                                                               "title":  "Data selection configuration",
                                                               "keywords":  "Data selection configuration Once a data source is configured for a Modbus TCP instance, you must configure which data is to be collected from the designated source device. Configure Modbus TCP data selection Complete the following to configure Modbus TCP data selection: Using any text editor, create a file that contains a Modbus TCP data selection in JSON form. This file can be created or copied to any directory on a device with Edge Data Store installed. For content structure, see Modbus TCP data selection examples . Update the parameters as needed. For a table of all available parameters, see Parameters for Modbus TCP data selection . Save the file as DataSelection.config.json . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/\u003cEDS http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSelection/ adapterId\u003e DataSelection  . The following example shows the HTTPS request using curl (run this command from the same directory where the file is located): curl -v -d \"@DataSelection.config.json\" -H \"Content-Type: application/json\" application json\" \"http://localhost:5590/api/v1/configuration/\u003cEDS \"http:  localhost:5590 api v1 configuration \u003cEDS adapterId\u003e/DataSelection\" adapterId\u003e DataSelection\" To see the streams that have been created in EDS storage to store the data you are writing, run the following curl script: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/ http:  localhost:5590 api v1 tenants default namespaces default streams  Modbus TCP data selection schema The following table describes the basic behavior of the Modbus_DataSelection_schema.json file. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Parameters for Modbus TCP data selection The following parameters are available for configuring Modbus TCP data selection. Parameter Required Type Nullable Description Id Optional string Yes This field is used to update an existing measurement. The ID automatically updates when there are changes to the measurement and will follow the format of \u003cUnitId \u003e. \u003cRegisterType \u003e. \u003cRegisterOffset \u003e. Selected Optional Boolean No This field is used to select or clear a measurement. To select an item, set to true. To remove an item, leave the field empty or set to false. If not configured, the default value is true. Name Optional string Yes The optional friendly name of the data item collected from the data source. If not configured, the default value will be the stream ID. UnitId Required number No Modbus TCP slave device unit ID. This must be a value between 0 and 247, inclusively. RegisterType Required number or string No Modbus TCP register type. Supported types are Coil, Discrete, Input16, Input32, Holding16 and Holding32. Input16 and Holding16 are used to read registers that have a size of 16 bits. For registers that have a size of 32 bits, use the Input32 and Holding32 register types. To represent the types, you can type in the register type ID or the exact name: 1 or Coil (Read Coil Status) 2 or Discrete (Read Discrete Input Status) 3 or Holding16 (Read 16-bit Holding Registers) 4 or Holding32 (Read 32-bit Holding Registers) 6 or Input16 (Read 16-bit Input Registers) 7 or Input32 (Read 32-bit Input Registers) RegisterOffset Required number No The 0 relative offset to the starting register for this measurement. For example, if your Holding registers start at base register 40001, the offset to this register is 0. For 40002, the offset to this register is 1. DataTypeCode Required number No An integer representing the data type that Modbus TCP EDS adapter will read starting at the register specified by the offset. Supported data types are: 1 = Boolean 10 = Int16 20 = UInt16 30 = Int32 31 = Int32ByteSwap 100 = Float32 101 = Float32ByteSwap 110 = Float64 111 = Float64ByteSwap 1001 - 1250 = String 2001 - 2250 = StringByteSwap ScanRate Required number No How often this measurement should be read from the device in milliseconds. Acceptable values are from 0 to 86400000. If 0 ms is specified, Modbus TCP EDS adapter will scan for data as fast as possible. BitMap Required string Yes The bitmap is used to extract and reorder bits from a word register. The format of the bitmap is uuvvwwxxyyzz, where uu, vv, ww, yy, and zz each refer to a single bit. A leading zero is required if the referenced bit is less than 10. The low-order bit is 01 and high-order bit is either 16 or 32. Up to 16 bits can be referenced for a 16-bit word (data types 10 and 20) and up to 32 bits can be referenced for a 32-bit word (data type 30 and 31). The bitmap 0307120802 will map the second bit of the original word to the first bit of the new word, the eighth bit to the second bit, the twelfth bit to the third bit, and so on. The high-order bits of the new word are padded with zeros if they are not specified. ConversionFactor Required number Yes This numerical value can be used to scale the raw response received from the Modbus TCP device. If this is specified, regardless of the specified data type, the value will be promoted to a float32 (single) when stored. [Result = (Value /   Conversion Factor)] ConversionOffset Required number Yes This numerical value can be used to apply an offset to the response received from the Modbus TCP device. If this is specified, regardless of the specified data type, the value will be promoted to a float32 (single) when stored. [Result = (Value - Conversion Offset)] StreamID Required string Yes The custom stream ID that will be used to create the streams. If not specified, the Modbus TCP EDS adapter will generate a default stream ID based on the measurement configuration. A properly configured custom stream ID follows these rules: Is not case-sensitive. Can contain spaces. Cannot start with two underscores (\"__\"). Can contain a maximum of 100 characters. Cannot use the following characters: /   : ? # [ ] @ ! $ \u0026 \u0027 ( ) \\ * + , ; = % \u003c \u003e | Cannot start or end with a period. Cannot contain consecutive periods. Cannot consist of only periods. Each JSON object in the file represents a measurement. You can modify the fields in each object to configure the measurement parameters. To add more measurements, you need to create more JSON objects with properly completed fields. Modbus TCP data selection examples The following are examples of valid Modbus TCP data selection configurations. Minimum data selection configuration: [ { \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 122, \"DataTypeCode\": 20, \"ScanRate\": 1000, } ] Maximum data selection configuration: [ { \"Id\": \"DataItem1\", \"Selected\": true, \"Name\": \"MyDataItem\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 123, \"DataTypeCode\": 20, \"ScanRate\": 300, \"StreamId\": \"stream.1\", \"BitMap\": \"020301\", \"ConversionFactor\": 12.3, \"ConversionOffset\": 14.5 } ]"
                                                           },
    "V1/Configuration/System.html":  {
                                         "href":  "V1/Configuration/System.html",
                                         "title":  "Edge Data Store system component",
                                         "keywords":  "Edge Data Store system component The EDS system component hosts all the other components running in Edge Data Store. It is how the port to access EDS is configured, and where adding and removing components is configured. You can configure the following facets in the system component: Logging Port Components Health Endpoints"
                                     },
    "V1/Configuration/System port configuration.html":  {
                                                            "href":  "V1/Configuration/System port configuration.html",
                                                            "title":  "System port configuration",
                                                            "keywords":  "System port configuration The System_Port.json file specifies the port on which the System is listening for REST API calls. The same port is used for configuration and for writing data to OMF and SDS. The default port is 5590. Valid ports are in the range of 1024-65535. Configure system port Before you change the default, ensure that no other service or application on the computer running Edge Data Store is using that port - only one application or service can use a port. If you change the port number through the REST API, you must restart Edge Data Store. Save the JSON containing the new port number in JSON format to a file named EdgePort.json and run the following script: curl -i -d \"@EdgePort.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/system/port http:  localhost:5590 api v1 configuration system port After the REST command completes, restart Edge Data Store for the change to take effect. System port schema The following table shows the basic behavior of the System_Port.json file. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Parameters for system port The following parameters are available for configuring system port. Parameter Required Type Nullable Description Port Required integer No The TCP port to bind Edge Data Store to. (Range [1024,65535]) Example: 5590 System port example The default System_Port.json file installed is: { \"Port\": 5590 }"
                                                        },
    "V1/Configuration/System components configuration.html":  {
                                                                  "href":  "V1/Configuration/System components configuration.html",
                                                                  "title":  "System components configuration",
                                                                  "keywords":  "System components configuration Edge Data Store includes Modbus TCP EDS adapter, OPC UA EDS adapter, and the Storage component. These components are only active if you configure the system to use them. EDS itself needs only a small amount of configuration - the list of components and the HTTP Port used for REST calls. Configure system components The default System_Components.json file for the System component contains the following information. [ { \"ComponentId\": \"Storage\", \"ComponentType\": \"EDS.Component\" } ] The Storage component is required for Edge Data Store to run. Only a single Storage component is supported. You can add additional Modbus TCP EDS adapter and OPC UA EDS adapter components if you want. To add a new component, create a JSON file with the ComponentId and ComponentType. The following example adds a Modbus TCP EDS adapter. { \"ComponentId\": \"Modbus1\", \"ComponentType\": \"Modbus\" } Note: A unique ComponentId is necessary for each component in the system. This example uses the ComponentId Modbus1 since it is the first Modbus TCP EDS adapter. Save the JSON file with the name AddComponent.json . From the same directory where the file exists, run the following curl script: curl -i -d \"@AddComponent.json\" -H \"Content-Type: application/json\" application json\" http://localhost:5590/api/v1/configuration/system/components http:  localhost:5590 api v1 configuration system components After the curl command completes successfully, you can configure or use the new component. System components schema The following table defines the basic behavior of the AddComponent.json file. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Parameters for system components The following parameters are available for configuring system components. Parameters Required Type Nullable Description ComponentId Required string Yes The ID of the component. It can be any alphanumeric string, for example Storage. ComponentType Required string Yes The type of the component, for example EDS.Component. There are three types of components: Storage, OPC UA EDS Adapter, and Modbus TCP EDS Adapter. System components example [ { \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ]"
                                                              },
    "V1/Configuration/Storage.html":  {
                                          "href":  "V1/Configuration/Storage.html",
                                          "title":  "Storage",
                                          "keywords":  "Storage The EDS storage component provides Sequential Data Store (SDS) storage, with a rich array of functionality available through REST APIs. For the storage component, you configure storage runtime , logging , and data egress . For more information, see SDS reference . The following diagram depicts the relationships between Edge Data Store functions and the EDS Storage component:"
                                      },
    "V1/Configuration/EDSOPCUAAdapter.html":  {
                                                  "href":  "V1/Configuration/EDSOPCUAAdapter.html",
                                                  "title":  "Edge Data Store OPC UA adapter",
                                                  "keywords":  "Edge Data Store OPC UA adapter EDS OPC UA adapter has three configurable facets: Data source Data selection Logging"
                                              },
    "V1/Configuration/EDSModbusTCPAdapter.html":  {
                                                      "href":  "V1/Configuration/EDSModbusTCPAdapter.html",
                                                      "title":  "Edge Data Store Modbus TCP adapter",
                                                      "keywords":  "Edge Data Store Modbus TCP adapter EDS Modbus TCP adapter has three configurable facets: Data source Data selection Logging"
                                                  },
    "V1/CommandLine/Delete configuration.html":  {
                                                     "href":  "V1/CommandLine/Delete configuration.html",
                                                     "title":  "Delete configuration",
                                                     "keywords":  "Delete configuration Delete configuration entry Complete the following to delete a configuration entry from a collection configuration in Edge Data Store. For example, you can delete a single health endpoint of the \u0027HealthEndpoints\u0027 facet within the \u0027System\u0027 component. Open command line. Type the componentId and facetName followed by the ID of the entry to be removed. Add the delete keyword and press Enter. Example: Delete endpoint_1 of the HealthEndpoints facet from the System: edgecmd Configuration System HealthEndpoints Id=endpoint_1 delete Delete configuration file Complete the following to delete a configuration file from Edge Data Store. For example, you can delete the configuration file of the \u0027HealthEndpoints\u0027 facet within the \u0027System\u0027 component. Open command line. Type the componentId and facetName . Add the delete keyword and press Enter. Example: Delete the HealthEndpoints facet configuration file: edgecmd Configuration System HealthEndpoints delete"
                                                 },
    "V1/CommandLine/CommandLine.html":  {
                                            "href":  "V1/CommandLine/CommandLine.html",
                                            "title":  "Configure Edge Data Store with EdgeCmd",
                                            "keywords":  "Configure Edge Data Store with EdgeCmd The following sections provide instructions for the configuration of Edge Data Store for Windows or Linux using EdgeCmd utility. Note: The following EdgeCmd utility locations are based on the installation instructions in EdgeCmd utility . All configuration options that can be done using REST can also be done using the EdgeCmd utility and command line arguments. Configuration and administrative REST interfaces are generally exposed through the command line. Read/write Read write capabilities to the EDS storage component, OMF ingress and SDS read/write read write capabilities are only available using the REST API."
                                        },
    "V1/Administration/Administration.html":  {
                                                  "href":  "V1/Administration/Administration.html",
                                                  "title":  "Administration",
                                                  "keywords":  "Administration You can reset EDS adapters, the EDS Storage component, and the Edge Data Store application with the EDS administration level functions. The examples in the administration topics use curl, a commonly available tool on both Windows and Linux. You can use the same operations with any programming language or tool that supports making REST calls. You can also execute Edge Data Store administration functions with the EdgeCmd command line utility. To validate successful configurations, you can accomplish data retrieval steps (GET commands) using a browser, if available on your device."
                                              },
    "V1/Overview/PIEgressQuickStart.html":  {
                                                "href":  "V1/Overview/PIEgressQuickStart.html",
                                                "title":  "PI egress quick start",
                                                "keywords":  "PI egress quick start This topic provides quick start instructions for egressing data stored in the Edge Data Store into a remote PI System. This is accomplished using PI Web API which is configured for basic authentication. Create a periodic egress configuration Complete the following to configure Edge Storage periodic egress for the PI Web API endpoint and credentials: Create a JSON file containing one or more egress endpoints, based on the following example: [{ \"Id\": \"PWA\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cunique stream prefix\u003e\", \"TypePrefix\": \"\u003cunique type prefix\u003e\", \"Endpoint\": \"https://\u003cyour \"https:  \u003cyour PI Web API Server\u003e/piwebapi/omf/\", Server\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null, \"ValidateEndpointCertificate\": true, \"TokenEndpoint\": null }] Add the server name, username, and password of your PI Web API server into the \"Endpoint\" definition. You must specify a valid user account that can write data via PI Web API using Basic authentication. For examples, see Data egress configuration . Note: StreamPrefix and TypePrefix values can be used to ensure uniqueness on the destination system, if required. The StreamPrefix value will create unique PI Points on the PI System. If you want to only send specific streams, edit the \"EgressFilter\" value. Examples of more advanced scenarios are in the Egress section of this documentation. Save the JSON file with the name PeriodicEgressEndpoints.json. To configure Edge Storage to send data to the PI System, run the following curl script from the same directory where you saved the JSON file. Note: You can run the file and curl script from any directory on the device as long as the file and the curl script are run from the same directory. curl -i -d \"@PeriodicEgressEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/storage/PeriodicEgressEndpoints/ http:  localhost:5590 api v1 configuration storage PeriodicEgressEndpoints  When the command completes successfully, data will start being sent to the PI System."
                                            },
    "V1/Overview/Performance.html":  {
                                         "href":  "V1/Overview/Performance.html",
                                         "title":  "Performance",
                                         "keywords":  "Performance Edge Data Store is designed to run on a variety of small hardware and software platforms, and to support running customer applications on the same platform. In order to assist in determining Edge Data Store is designed to run on a variety of small hardware and software platforms, and to support running customer applications on the same platform. To assist in determining the correct hardware and software configuration for a specific use, Edge Data Store has been tested on a variety of different hardware platforms with the supported ingress protocols at different event rates. Summary tables are provided below for each of the different protocols on different hardware platforms. This topic is provided to assist in determining hardware and software that can be used for specific applications. Edge Data Store Platforms For this release of Edge Data Store performance measurements platforms have been divided into three categories with the maximum supported Ingress rate for each category: Small devices (e.g. 1 core CPU, 512 MB RAM): 30 events /   sec Medium devices (e.g. 1 - 2 core CPU, 1 ??? 2 GB RAM): 300 events /   sec Large devices (e.g. 2 ??? 4 core CPU, 4 GB RAM and larger): 3,000 events /   sec It is possible that specific hardware and software configurations might support lower values. These are upper limits of what is supported for this release of the Edge Data Store. Ingress Performance Data in the Edge Data Store can be ingressed using OMF, OPC UA or Modbus TCP. Each of these methods has different performance profiles, so they are treated separately. When selecting the hardware to host Edge Data Store, there are some general principles that come out of our testing: EDS Adapters use less CPU than OMF. In general across all platforms CPU use for EDS OPC UA and EDS Modbus TCP adapters is about half that of OMF ingress at the same event rates. Memory use is largely determined by the number of streams of data being written to. More streams of data will require additional RAM memory (in addition to storage space) to host additional streams. Windows is slightly more efficient than Linux on similar hardware. Both memory and CPU are slightly lower when running on Windows 10 than on Linux on the AMD64/Intel AMD64 Intel x64 platform where both are supported. SSD storage is recommended for maximum performance and reliability. Our testing was done across all storage media - SSDs, HDDs, eMMC, and SD cards. EDS testing was successful on all those platforms but for maximum performance and reliability SSD storage is the best choice. In the tables below are some representative performance numbers based on internal testing of the Edge Data Store. Performance on any specific platform is expected to vary from the tables below, and the numbers below are only estimates of what can be expected on a real device in production. The columns of the table are: Size - Small, Medium, or Large based on the definition earlier in this topic Computer Type - Maker of the hardware tested Storage - hard drive (HDD), solid state drive (SSD), eMMC, or SD card RAM - total amount of RAM OS - Linux or Windows Cores - number of processor cores in the CPU Events Per Second - number of events per second being ingressed Stream Count - total number of streams on the device in the default namespace Max RAM MB - maximum memory used by EDS in this test Max CPU % - maximum CPU percentage used by EDS in this test Memory and CPU use metrics were computed using a one-minute average of memory and CPU use, so there may be momentary spikes of CPU and memory use that exceed the values shown. OMF Ingress Performance Size Computer Type Storage Process RAM OS Cores Events Second Stream Count Max RAM MB Max CPU % Small BeagleBone SD Card ARM32 512 MB Linux 1 2 30 130 90 Medium Raspberry PI 3 SD Card ARM32 1 GB Linux 4 300 300 202 50 Large Toshiba HDD x64 4 GB Linux 2 1500 3000 912 90 Modbus TCP Ingress Performance Size Computer Type Storage Process RAM OS Cores Events Second Stream Count Max RAM MB Max CPU % Small BeagleBone SD Card ARM32 512 MB Linux 1 15 30 120 65 Medium Raspberry PI 3 SD Card ARM32 1 GB Linux 4 38 300 200 30 Large Dell SSD x64 8 GB Linux 4 1500 3000 950 90 OPC UA Ingress Performance Size Computer Type Storage Process RAM OS Cores Events Second Stream Count Max RAM MB Max CPU % Small BeagleBone SD Card ARM32 512 MB Linux 1 50 50 150 95 Medium Raspberry PI 3 SD Card ARM32 1 GB Linux 4 300 300 220 30 Large Dell SSD x64 8 GB Linux 4 3000 3000 1100 34 Periodic Egress Performance An important part of periodic egress performance is the amount of network bandwidth available between the device hosting the Edge Data Store and the PI Web API or OCS endpoint. The performance numbers below reflect having access to a 1 GB LAN connection and a high speed connection to the Internet. If the device is located in an location with limited network bandwidth a lower level of performance can be expected. Egress has a much lower performance impact on Edge Data Store than Ingress. Generally the peformance impact of egress on memory and CPU use is generally only a small percentage of the CPU and memory use of Ingress, so is not a major factor in system design. Periodic Egress Performance to PI Web API Performance testing of Periodic Egress between Edge Data Store and PI Web API was done with a 1 GB network connection between the Edge Data Store computer and PI Web API with PI Web API hosted on a Xeon based server class machine that also included a local PI Data Archive installation. The Edge Data Store test device was set to backfill, and several million events were sent to the PI Web API. In all cases egress performance exceeded 10,000 events per second, which exceeds the maximum ingress rate for Edge Data Store of 3,000 events per second. In addition extended tests for several weeks with an egress rate of 3,000 events per second. Periodic Egress Performance to OSIsoft Cloud Services (OCS) Performance testing of Periodic Egress between Edge Data Store and OCS was done with a 1 GB network connection between the Edge Data Store computer and a high speed Internet connection. The Edge Data Store test device was set to backfill, and several million events were sent to OCS. In all cases egress performance exceeded 10,000 events per second, which exceeds the maximum ingress rate for Edge Data Store of 3,000 events per second. In addition extended tests for several weeks with a lower egress rate."
                                     },
    "V1/Overview/OpcUaQuickStart.html":  {
                                             "href":  "V1/Overview/OpcUaQuickStart.html",
                                             "title":  "OPC UA EDS adapter quick start",
                                             "keywords":  "OPC UA EDS adapter quick start This topic provides quick start instructions for setting up an OPC UA EDS adapter. You can add a single OPC UA EDS adapter during Edge Data Store installation. For more information, see Install Edge Data Store . The following diagram depicts the data flow of a single OPC UA EDS adapter: Configure an OPC UA data source Create a file in JSON format describing the location of the OPC UA data source. The adapter installed during installation is named OpcUa1 in this example. Modify the following values to match your environment. { \"EndpointUrl\": \"opc.tcp://\u003cip \"opc.tcp:  \u003cip address\u003e:\u003cport - often 62541\u003e/\u003cserver 62541\u003e \u003cserver path\u003e\", \"UseSecureConnection\": false, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": \"OpcUa\" } Enter the correct IP address and port for your OPC UA data source. Save the file with the name OpcUa1Datasource.json. Run the following curl script from the same directory where the file is located. Note: You should run the script on the same computer where the Edge Data Store is installed. curl -i -d \"@OpcUa1Datasource.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Datasource http:  localhost:5590 api v1 configuration OpcUa1 Datasource When the command completes successfully (a 204 is returned by curl), your OPC UA data source has been created. If you receive a 400 error, check the data source JSON file for errors. If you receive a 404 or 500 error, check to make sure Edge Data Store is running on your computer. Configure OPC UA data selection When you create the data source file, the OPC UA adapter auto generates the data selection file, which lists all available streams in the designated data source. To configure the data selection file, complete the following: Save the data selection to your local device for editing. Select each of the streams you want to ingress to Edge Data Store. All steams listed in the auto generated data selection file are initially set to deselect. Save the following JSON content in a text file and name it OpcUa1Dataselection.json. [{ \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"StreamId\": null } ] Run the following curl script to configure Edge Data Store to collect OPC UA data values: curl -i -d \"@OpcUa1Dataselection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/OpcUa1/Dataselection http:  localhost:5590 api v1 configuration OpcUa1 Dataselection"
                                         },
    "V1/Overview/OMFQuickStart.html":  {
                                           "href":  "V1/Overview/OMFQuickStart.html",
                                           "title":  "OMF quick start",
                                           "keywords":  "OMF quick start This topic provides quick start instructions for getting data into the Edge Storage component using the OSIsoft Message Format (OMF), and then retrieving that data using the Sequential Data Store (SDS) REST API. Both OMF data ingress and SDS data retrieval are accomplished using REST APIs. Create an OMF type The first step in OMF data ingress is to create an OMF type that describes the format of the data to be stored in a container. In this example, the data to be written is a timestamp and a numeric value. Create an OMF JSON file describing the type as follows: [{ \"id\": \"MyCustomType\", \"classification\": \"dynamic\", \"type\": \"object\", \"properties\": { \"Timestamp\": { \"type\": \"string\", \"format\": \"date-time\", \"isindex\": true }, \"Value\": { \"type\": \"number\", \"format\": \"float32\" } } }] The value is indexed by a timestamp, and the numeric value that will be stored is a 32-bit floating point value. In order to create the OMF type in Edge Storage, store the JSON file with the name OmfCreateType.json on the local device. Run the following curl command: curl -i -d \"@OmfCreateType.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: type\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  When this command completes successfully, an SDS type with the same name will have been created on the server. Any number of containers can be created from the type, as long as they use a timestamp as an index and have a 32-bit floating point value. You only need to send the Type definition the first time you send data, but it does not cause an error if you resend the same definition at a later time. Create an OMF container The next step in writing OMF data is to create a container. As with an OMF type, this only needs to be done once before sending data events, and resending the same definition repeatedly does not cause an error. Create an OMF JSON file as follows: [{ \"id\": \"MyCustomContainer\", \"typeid\": \"MyCustomType\" }] This container references the OMF type that was created earlier, and an error will occur if the type does not exist when the container is created. To create the OMF container in the Edge Storage, store the JSON file with the name OmfCreateContainer.json on the local device. To create the SDS stream to store data defined by the type, run the following curl command: curl -i -d \"@OmfCreateContainer.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: container\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  Write data events to the OMF container Create an OMF JSON file to define data events to be stored in the SDS Stream created in the previous steps. For best performance, you should batch OMF values together, as in the following example: [{ \"containerid\": \"MyCustomContainer\", \"values\": [{ \"Timestamp\": \"2019-07-16T15:18:24.9870136Z\", \"Value\": 12345.6789 }, { \"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789 } ] }] To write the data in the Edge Storage, store the JSON file with the name OmfCreateDataEvents.json on the local device. To write data values to the SDS stream, run the following curl command: curl -i -d \"@OmfCreateDataEvents.json\" -H \"Content-Type: application/json\" application json\" -H \"producertoken: x \" -H \"omfversion: 1.1\" -H \"action: create\" -H \"messageformat: json\" -H \"messagetype: data\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/omf/ http:  localhost:5590 api v1 tenants default namespaces default omf  Read Last Data written using SDS Complete the following to use the SDS REST API to read back the data written to the server. Start the curl command line tool. Execute the following curl command to return the last value written: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data Last Sample output: {\"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789} Read a range of data events written using SDS Complete the following to use the SDS REST API to read back the data written to the server. Start the curl command line tool. Execute the following curl command to return up to 100 values after the startIndex specified: curl \"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/MyCustomContainer/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" \"http:  localhost:5590 api v1 tenants default namespaces default streams MyCustomContainer Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" Sample output: [{\"Timestamp\": \"2019-07-16T15:18:24.9870136Z\",\"Value\": 12345.6789}, {\"Timestamp\": \"2019-07-16T15:18:25.9870136Z\", \"Value\": 12346.6789}] Both values that were entered are returned. This command returns up to 100 values after the specified timestamp."
                                       },
    "V1/Overview/OCSEgressQuickStart.html":  {
                                                 "href":  "V1/Overview/OCSEgressQuickStart.html",
                                                 "title":  "OCS egress quick start",
                                                 "keywords":  "OCS egress quick start This topic provides quick start instructions for getting data stored in the Edge Data Store into OCS. You can accomplish this by using the OCS OMF endpoint which is configured for OCS authentication. Create a periodic egress configuration Create a JSON file containing one or more egress endpoints, based on the following example: [{ \"Id\": \"OCS\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"https://\u003cyour \"https:  \u003cyour OCS OMF endpoint\u003e\", \"ClientId\": \"\u003cyour OCS ClientId\u003e\", \"ClientSecret\": \"\u003cyour OCS ClientSecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null, \"ValidateEndpointCertificate\": true, \"TokenEndpoint\": null }] Type the URL of your OCS OMF endpoint into the \"Endpoint\": value in the preceding JSON file. Type a ClientId and ClientSecret that can write data to your OCS tenant and namespace in the \"ClientId\": and \"ClientSecret\": values in the preceding JSON file. Note: If required, use the StreamPrefix and TypePrefix to ensure uniqueness on the destination system. If a StreamPrefix is specified, it is used to create a unique stream id on OCS. This configuration is set up to send all stream data to OCS. If you want to only send specific streams, edit the EgressFilter value. For examples of more advanced scenarios, see Data egress configuration . Save the JSON file with the name PeriodicEgressEndpoints.json to any directory on the device with Edge Data Store installed. To configure the Edge Storage component to send data to OCS, run the following curl script from the directory where you saved the file. curl -i -d \"@PeriodicEgressEndpoints.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/storage/PeriodicEgressEndpoints/ http:  localhost:5590 api v1 configuration storage PeriodicEgressEndpoints  When this command completes successfully, data will start being egressed to OCS."
                                             },
    "V1/Overview/ModbusQuickStart.html":  {
                                              "href":  "V1/Overview/ModbusQuickStart.html",
                                              "title":  "Modbus TCP EDS adapter quick start",
                                              "keywords":  "Modbus TCP EDS adapter quick start This topic provides quick start instructions for setting up the Modbus TCP EDS adapter. You can add a single Modbus TCP EDS adapter during Edge Data Store installation. You can add a single Modbus TCP EDS adapter during Edge Data Store installation. For more information, see Install Edge Data Store . The following diagram depicts the data flow of a single Modbus TCP EDS adapter: Configure a Modbus TCP data source Create a file in JSON format describing the location of the Modbus TCP data source. The adapter installed during installation is named Modbus1 in this example. Modify the following values to match your environment. { \"IpAddress\": \"\u003cModbus IP Address\u003e\", \"Port\": \u003cPort - usually 502\u003e, \"ConnectTimeout\": 15000, \"ReconnectInterval\": 5000, \"RequestTimeout\": 9000, \"DelayBetweenRequests\": 0, \"MaxResponseDataLength\": 250 } Enter the correct IP address and port for your Modbus data source. Save the file with the name Modbus1DataSource.json. Run the following curl script from the same directory where the file is located. Note: You should run the script on the same computer where the Edge Data Store is installed. curl -i -d \"@Modbus1Datasource.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/Modbus1/Datasource http:  localhost:5590 api v1 configuration Modbus1 Datasource When the command completes successfully (a 204 is returned by curl), your Modbus TCP data source has been created. If you get a 400 error, check the JSON file for errors. If you get a 404 or 500 error, check that Edge Data Store is running on your device. Configure Modbus TCP data selection After you create the data source file, you select the streams you want to store in Edge Data Store by configuring Modbus data selection. To configure the data selection file, complete the following: Create a file in JSON format to define each stream you want to ingress to Edge Data Store. Save the following JSON content in a text file and name it Modbus1Dataselection.json. Modify the following values to match your environment: [{ \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 2, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 3, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 4, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 5, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 } ] Run the following curl script to configure Edge Data Store to collect Modbus TCP data values: curl -i -d \"@Modbus1Dataselection.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/Modbus1/Dataselection http:  localhost:5590 api v1 configuration Modbus1 Dataselection"
                                          },
    "V1/Overview/EdgeSystemOverview.html":  {
                                                "href":  "V1/Overview/EdgeSystemOverview.html",
                                                "title":  "Edge Data Store",
                                                "keywords":  "Edge Data Store Edge Data Store (EDS) is an embedded data server that runs on Linux and Windows. EDS provides a lightweight data collection and storage application designed to enable the capturing of data for historical storage and analysis at the edge of networks. A storage component based on sequential data storage technology is provided. You can configure and administer EDS through REST programming, configuration, administrative interfaces, and the EdgeCmd command line tool. EDS complements existing OSIsoft products, and is designed for small devices. You can install and run it on 64-bit Intel/AMD Intel AMD compatible and 32-bit ARM v7/v8 v7 v8 compatible chips. While not a replacement for a PI System, EDS augments the PI System by providing historical data access in situations where deploying a full PI System is impractical. EDS provides native capability, via its Egress functionality, to send data to a PI System or to OSIsoft Cloud Services for long term historical storage and analysis. EDS provides the following capabilities: OMF data ingress Edge connectivity through Modbus TCP and OPC UA Configurable data storage Data egress to PI Web API and OCS REST API to enable custom applications for visualization and analytics on Edge Data Store on either Linux, Windows, or both in a variety of programming languages Edge Data Store architecture The following diagram depicts the relationships of architectural components to one another in the Edge Data Store: Edge Data Store data flow Edge Data Store can egress data to both PI Data Archive and OSIsoft Cloud Services. For more information, see PI egress quick start and OCS egress quick start . The following diagram depicts the flow of data in Edge Data Store: Edge Data Store components The following diagram depicts the relationship of key functions to relevant components of the Edge Data Store: Data ingress to Edge Data Store Edge Data Store can ingress data in a number of ways. There are two built-in adapters: EDS Modbus TCP and EDS OPC UA . Data can also be ingressed using OSIsoft Message Format (OMF) and the Sequential Data Store SDS REST APIs. The following diagram depicts an OMF data ingress scenario in the Edge Data Store: During installation of Edge Data Store, you can choose to install either an EDS Modbus TCP adapter or an EDS OPC UA adapter, or both. The EDS Modbus and EDS OPC UA adapters require configuration of data source and data selection before they can collect data in Edge Data Store. You can use OMF data ingress once Edge Data Store is installed, with no further configuration steps. Edge Data Store is composed of components, and is designed to allow additional EDS adapters at a later point. Local data read and write access You can access all data in Edge Data Store by using the Sequential Data Store SDS quick start REST API. You can use this data for local applications that perform analytics or visualization. Example EDS visualization application The following diagram depicts the flow of data from a customer visualization application into Edge Data Store, via either OMF or SDS REST calls: Example EDS analytics application The following diagram depicts the flow of data from a customer analytics application into Edge Data Store, via either OMF or SDS REST calls:"
                                            },
    "V1/OpcUa/SupportedFeaturesOPCUA.html":  {
                                                 "href":  "V1/OpcUa/SupportedFeaturesOPCUA.html",
                                                 "title":  "Supported features",
                                                 "keywords":  "Supported features Data types The following table lists OPC UA variable types that the OPC UA EDS adapter supports data collection from and types of streams that are going to be created in Edge Data Store. OPC UA data type Stream data type Boolean Boolean Byte Int16 SByte Int16 Int16 Int16 UInt16 UInt16 Int32 Int32 UInt32 UInt32 Int64 Int64 UInt64 UInt64 Float Float32 Double Float64 DateTime DateTime String String Export operation The OPC UA EDS adapter is able to export available OPC UA dynamic variables by browsing the OPC UA hierarchies or sub-hierarchies. Export operation actions To limit browsing, specify a comma-separated collection of nodeIds in data source configuration (RootNodeIds). Note: They are treated as roots from where the adapter starts the browse operation. The adapter triggers an export operation after a successful connection to the OPC UA server when the data selection file does not exist in configuration directory. Copy the exported data selection JSON file from the directory or retrieve it using a REST API call. Optional: To avoid a potentially long and expensive browse operation, create the data selection file manually. Configure it before you configure the data source or push both in one configuration call together."
                                             },
    "V1/Configuration/Schemas/Modbus_schema.html":  {
                                                        "href":  "V1/Configuration/Schemas/Modbus_schema.html",
                                                        "title":  "ModbusConfiguration schema",
                                                        "keywords":  "ModbusConfiguration schema Abstract Extensible Status Identifiable Custom Properties Additional Properties Defined In Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_Logging_schema.json Modbus TCP configuration properties Property Type Required Nullable Defined by Logging ModbusLoggingConfiguration Optional Yes EdgeLoggerConfiguration DataSource DataSourceConfiguration Optional Yes ComponentsConfiguration DataSelection [ModbusDataSelectionConfiguration] Optional Yes DataSelectionConfiguration"
                                                    },
    "V1/Configuration/Schemas/Modbus_Logging_schema.html":  {
                                                                "href":  "V1/Configuration/Schemas/Modbus_Logging_schema.html",
                                                                "title":  "ModbusLoggerConfiguration Schema",
                                                                "keywords":  "ModbusLoggerConfiguration Schema The Modbus TCP logger configuration schema specifies how to formally describe the Modbus TCP logging parameters. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden ModbusLoggerConfiguration Properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) LogFileCountLimit LogFileCountLimit is optional type: integer defined in this schema LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer defined in this schema LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference defined in this schema LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional LogFileCountLimit LogFileCountLimit is optional type: integer LogFileCountLimit type integer , nullable LogFileSizeLimitBytes LogFileSizeLimitBytes is optional type: integer LogFileSizeLimitBytes type integer , nullable LogLevel LogLevel is optional type: reference LogLevel type ??? #/definitions/EdgeLogLevel # definitions EdgeLogLevel"
                                                            },
    "V1/Configuration/Schemas/Modbus_DataSource_schema.html":  {
                                                                   "href":  "V1/Configuration/Schemas/Modbus_DataSource_schema.html",
                                                                   "title":  "Sample Modbus TCP data source configuration",
                                                                   "keywords":  "Sample Modbus TCP data source configuration { \"IpAddress\": \"\u003cModbus IP Address\u003e\", \"Port\": \u003cPort - usually 502\u003e, \"ConnectTimeout\": 15000, \"ReconnectInterval\": 5000, \"RequestTimeout\": 9000, \"DelayBetweenRequests\": 0, \"MaxResponseDataLength\": 250 } Modbus TCP data source configuration schema The Modbus TCP data source configuration schema specifies how to formally describe the Modbus TCP data source parameters. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_DataSource_schema.json DataSourceConfiguration properties Property Type Required Nullable Defined by ConnectTimeout integer Optional No DataSourceConfiguration (this schema) DelayBetweenRequests integer Optional No DataSourceConfiguration (this schema) IpAddress string Optional Yes DataSourceConfiguration (this schema) MaxResponseDataLength integer Optional No DataSourceConfiguration (this schema) Port integer Optional No DataSourceConfiguration (this schema) ReconnectInterval integer Optional No DataSourceConfiguration (this schema) RequestTimeout integer Optional No DataSourceConfiguration (this schema) StreamIdPrefix string Optional Yes DataSourceConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required ConnectTimeout integer Optional DelayBetweenRequests integer Optional IpAddress string Optional MaxResponseDataLength integer Optional Port integer Optional ReconnectInterval integer Optional RequestTimeout integer Optional StreamIdPrefix string Optional"
                                                               },
    "V1/Configuration/Schemas/Modbus_DataSelection_schema.html":  {
                                                                      "href":  "V1/Configuration/Schemas/Modbus_DataSelection_schema.html",
                                                                      "title":  "Sample Modbus TCP data selection configuration",
                                                                      "keywords":  "Sample Modbus TCP data selection configuration [{ \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 2, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 3, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 4, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 5, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 } ] Modbus TCP data selection configuration schema The Modbus TCP data selection configuration schema specifies how to formally describe the data selection parameters for Modbus TCP. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_DataSelection_schema.json Modbus TCP data selection configuration properties Property Type Required Nullable Defined by BitMap string Optional Yes DataSelectionConfiguration (this schema) ConversionFactor number Optional Yes DataSelectionConfiguration (this schema) ConversionOffset number Optional Yes DataSelectionConfiguration (this schema) DataTypeCode integer Optional No DataSelectionConfiguration (this schema) Name string Optional Yes DataSelectionConfiguration (this schema) RegisterOffset integer Optional No DataSelectionConfiguration (this schema) RegisterType reference #/definitions/ModbusRegisterType # definitions ModbusRegisterType Optional No DataSelectionConfiguration (this schema) ScanRate integer Optional No DataSelectionConfiguration (this schema) Selected boolean Optional No DataSelectionConfiguration (this schema) StreamId string Optional Yes DataSelectionConfiguration (this schema) UnitId integer Optional No DataSelectionConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required BitMap string Optional ConversionFactor number Optional ConversionOffset number Optional DataTypeCode integer Optional Name string Optional RegisterOffset integer Optional RegisterType Optional ScanRate integer Optional Selected boolean Optional StreamId string Optional UnitId integer Optional"
                                                                  },
    "V1/Reference/Edgecmd commands.html":  {
                                               "href":  "V1/Reference/Edgecmd commands.html",
                                               "title":  "EdgeCmd commands",
                                               "keywords":  "EdgeCmd commands The following tables provide an overview of available edgecmd commands that you can use with components of Edge Data Store. Every command that you use with the EdgeCmd utility has to be preceded by edgecmd . Help edgecmd command Description Examples edgecmd Help Display general instructions on how to use the EdgeCmd utility. edgecmd Help \u003ccomponentName\u003e Display help for a specific Edge Data Store component. edgecmd Help System edgecmd Help \u003ccomponentName\u003e \u003cfacetName\u003e Display help for a specific facet of an Edge Data store component. edgecmd Help System Port Configuration System edgecmd command Description Examples edgecmd Configuration Display the entire configuration for every Edge Data Store component. edgecmd Configuration System Components Display the components that are currently configured. edgecmd Configuration System Components componentId=\u003ccomponentId\u003e componentType=\u003ccomponentType\u003e Add a new component. edgecmd Configuration System Components componentId=Modbus1 componentType=Modbus edgecmd Configuration System Components id=\u003ccomponentId\u003e delete Delete a component. edgecmd Configuration System Components id=Modbus1 delete Components edgecmd command Description Examples edgecmd Configuration \u003ccomponentId\u003e Display component specific configuration. edgecmd Configuration System or edgecmd Configuration OpcUa1 edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e Display facet specific configuration of an Edge Data Store component. edgecmd Configuration Storage Logging edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e id=\u003cIndexToRetrieve\u003e Display the configuration of specific entry of a facet. edgecmd Configuration Storage PeriodicEgressEndpoints id=Endpoint1 edgecmd Configuration \u003ccomponentId\u003e DataSource Configure the data source for either the Modbus TCP EDS adapter or the OPC UA EDS adapter. For examples, see OPC UA data source configuration and Modbus TCP data source configuration . edgecmd Configuration \u003ccomponentId\u003e DataSelection Configure the data selection for either the Modbus TCP EDS adapter or the OPC UA EDS adapter. For examples, see OPC UA data selection configuration and Modbus TCP data selection configuration . edgecmd Configuration \u003ccomponentId\u003e Logging Configure logging for either the Modbus TCP EDS adapter or the OPC UA EDS adapter. For examples, see Logging configuration . Configuration with JSON files edgecmd command Description Examples edgecmd Configuration file=\u003cPathToJsonFile\u003e Import a bulk configuration through a JSON file. edgecmd Configuration file=\"~/Bulk_Storage_Runtime.json\" file=\"~ Bulk_Storage_Runtime.json\" edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e file=\u003cPathToJsonFile\u003e Import a facet specific configuration file for a component. edgecmd Configuration Modbus1 DataSource file=\"~/Modbus_DataSource.json\" file=\"~ Modbus_DataSource.json\""
                                           },
    "V1/Installation/Verify installation.html":  {
                                                     "href":  "V1/Installation/Verify installation.html",
                                                     "title":  "Verify installation",
                                                     "keywords":  "Verify installation On a device with limited processor, memory, and slow storage, it may take some time before the Edge Data Store is fully initialized and running for the first time. After allowing time for start up, use the following steps to verify that Edge Data Store is correctly installed. Run the following script: curl http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration If you receive an error, wait a few seconds and try the script again. If the installation was successful, a JSON copy of the default system configuration is returned: { \"Storage\": { \"PeriodicEgressEndpoints\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableTransactionLog\": true }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ] } }"
                                                 },
    "V1/Installation/Uninstall Edge Data Store.html":  {
                                                           "href":  "V1/Installation/Uninstall Edge Data Store.html",
                                                           "title":  "Uninstall Edge Data Store",
                                                           "keywords":  "Uninstall Edge Data Store Uninstall from Windows To remove the EdgeDataStore program files from a Windows device, use the Windows Control Panel uninstall application process. The configuration, data, and log files are not removed by the uninstall process. Optional: To remove data, configuration and log files, remove the directory C:\\ProgramData\\OSIsoft\\EdgeDataStore . This will delete all data stored in the Edge Storage component in addition to configuration and log files. Uninstall from Linux To remove Edge Data Store software from a Linux device, open a terminal window and run the following command: sudo apt remove osisoft.edgedatastore Optional: To remove data, configuration, and log files, remove the directory /usr/share/OSIsoft/EdgeDataStore/  usr share OSIsoft EdgeDataStore  . This will delete all data stored in the Edge Storage component, in addition to configuration and log files. Alternatively, you can run the following command: sudo rm -r /usr/share/OSIsoft/EdgeDataStore/  usr share OSIsoft EdgeDataStore "
                                                       },
    "V1/Configuration/System health endpoints configuration.html":  {
                                                                        "href":  "V1/Configuration/System health endpoints configuration.html",
                                                                        "title":  "Health endpoints configuration",
                                                                        "keywords":  "Health endpoints configuration Edge Data Store sends health information for all of its OSIsoft Adapters. Edge Data Store has the ability to report health of the components to an OMF endpoint capable of receiving health messages. To enable this functionality, you must configure HealthEndpoints. Configure system health endpoints Complete the following to configure system health endpoints Create a JSON file containing system health endpoints. For content structure, see System health endpoints example . Update the parameters as needed. For a table of all available parameters, see Parameters . Save the file as System_HealthEndpoints.config.json . Use any Configuration tool capable of making HTTP requests to execute a POST command with the contents of that file to the following endpoint: http://localhost:5590/api/v1/configuration/System/HealthEndpoints http:  localhost:5590 api v1 configuration System HealthEndpoints . Example using curl (run this command from the same directory where the file is located): curl -v -d \"@System_HealthEndpoints.json\" -H \"Content-Type: application/json\" application json\" http://localhost:5590/api/v1/configuration/System/HealthEndpoints http:  localhost:5590 api v1 configuration System HealthEndpoints System health endpoints schema The following table defines the basic behavior of the System_HealthEndpoints.config.json file. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Parameters The following parameters are available for configuring system health endpoints. Parameter Required Type Nullable Description Buffering Optional reference No Sets the buffering type for messages to this endpoint. Options are memory, disk, or none. The default is none. ClientId Optional string Yes The Client ID used for authentication to OSIsoft Cloud Services. ClientSecret Optional string Yes The Client Secret used for authentication to OSIsoft Cloud Services. Endpoint Required string Yes The URL of the ingress point which accepts OMF health messages. Id Optional string Yes The ID of the health endpoint configuration. The ID can be any alphanumeric string; for example, Endpoint1. If you do not specify an ID, Edge Data Store generates one automatically. MaxBufferSizeMB Optional integer No The limit on the maximum megabytes of data to buffer for messages to this endpoint if an integer is \u003e 0. This parameter is useful if you want to limit memory or disk usage growth in the event of disconnection to the endpoint. If the buffer is full, old messages will be discarded for new messages. The default is 0. Password Optional string Yes The password used for authentication to PI Web API OMF endpoint. UserName Optional string Yes The user name used for authentication to PI Web API OMF endpoint. ValidateEndpointCertificate Optional Boolean No The OSIsoft Adapter validates the endpoint certificate if set to true (recommended). If set to false, the OSIsoft Adapter accepts any endpoint certificate. OSIsoft strongly recommends using disabled endpoint certificate validation for testing purposes only. System health endpoints example [{ \"endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"buffering\": 0, \"maxBufferSizeMB\": 0 }, { \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"buffering\": 0, \"maxBufferSizeMB\": 0 } ]"
                                                                    },
    "V1/Configuration/System configuration.html":  {
                                                       "href":  "V1/Configuration/System configuration.html",
                                                       "title":  "System configuration",
                                                       "keywords":  "System configuration Edge Data Store uses JSON configuration files in a protected directory on Windows and Linux to store configuration that is read on startup. While the files are accessible to view, OSIsoft recommends that you use REST or the EdgeCmd command line tool to make any changes to the files. As part of making Edge Data Store as secure as possible, any passwords or secrets that you configure are stored in encrypted form (with cryptographic key material stored separately in a secure location.) If you edit the files directly, the system may not work as expected. Note: You can edit any single component or facet of the system using REST, but also configure the system as a whole with a single REST call."
                                                   },
    "V1/Configuration/Storage runtime config.html":  {
                                                         "href":  "V1/Configuration/Storage runtime config.html",
                                                         "title":  "Storage runtime configuration",
                                                         "keywords":  "Storage runtime configuration Edge Data Store provides a mechanism for configuring runtime characteristics of the storage component. Note: The configured defaults are sufficient for most scenarios. You should modify these values only after consultation with OSIsoft Support personnel. Configure storage runtime To update the storage runtime configuration, complete the following: Create a JSON file with the storage runtime configuration. Note: See the following Parameters table for all available runtime parameters to define. See the following Examples section for an example of a valid runtime configuration file. Save the JSON file with the name Storage_Runtime.config.json. From the same directory where the file exists, run the following curl script: curl -i -d \"@Storage_Runtime.config.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration/storage/Runtime http:  localhost:5590 api v1 configuration storage Runtime Note: The @ symbol is a required prefix for the above command. Parameters Parameter Required Type Description IngressDebugExpiration Required string If set, defines how long OMF ingress debug files should be produced. StreamStorageLimitMb Required integer The maximum size in megabytes that a stream can reach. StreamStorageTargetMb Required integer The size in megabytes that a stream will be reduced to after StreamStorageLimitMb size is reached for a single stream. EnableTransactionLog No Boolean Enables or disables the transaction log. The transaction log helps to ensure no data is lost should a device lose power. TransactionLogLimitMB No integer Maximum size for transaction log file. Transaction log files larger than this size will be deleted, resulting is loss of data should the device lose power. CheckpointRateInSec No integer How often to flush new data to store. Examples The following is a valid runtime configuration example. { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableTransactionLog\": true } Storage runtime schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Storage_Runtime_schema.json Storage runtime configuration properties Property Type Required Nullable Defined by IngressDebugExpiration string Required No StorageRuntimeConfiguration (this schema) StreamStorageLimitMb integer Required No StorageRuntimeConfiguration (this schema) StreamStorageTargetMb integer Required No StorageRuntimeConfiguration (this schema) EnableTransactionLog bool No No StorageRuntimeConfiguration (this schema) TransactionLogLimitMB integer No No StorageRuntimeConfiguration (this schema) CheckpointRateInSec integer No No StorageRuntimeConfiguration (this schema) IngressDebugExpiration Ingress Debug Expiration is a property that can be used when debugging OMF. If the date and time is the future incoming OMF messages will be logged until the date and time specified. Once the configured time is past OMF messages will no longer be logged for debugging purposes. Debugging will be enabled for all incoming OMF messages, and HTTP request and response content will be stored to disk for review. The property represents the date and time when debugging should no longer be enabled. You can also disable debugging if you set the value to null . Examples of valid strings representing date and time: UTC: ???yyyy-mm-ddThh:mm:ssZ??? Local: ???mm-dd-yyyy hh:mm:ss??? The content of an incoming OMF message, including the headers, will be written to a file in the Logs directory. For an active application, this file can become quite large. As a result, debug information is stored to disk in another format than usual log messages. A single file is written to the usual logs directory for every incoming OMF type, container, and data message. IngressDebugExpiration type string format: date-time ??? date and time (according to RFC 3339, section 5.6 ) minimum length: 1 character StreamStorageLimitMb StreamStorageLimitMb is the maximum size in megabytes that a stream can reach. When a stream exceeds the size specified, older data will be deleted from the file. Data will be removed from the stream until the stream is at or below the StreamStorageTargetMb value. It is recommended that the target value be smaller than the limit since trimming can be an expensive operation and should be done infrequently. StreamStorageLimitMb type integer minimum value: 2 maximum value: 2147483647 StreamStorageTargetMb StreamStorageTargetMb is the size in megabytes that a stream will be reduced to after StreamStorageLimitMb size is reached for a single stream. When a stream exceeds the size specified, older data will be deleted from the file. Data will be removed from the stream until the stream is at or below the StreamStorageTargetMb value. It is recommended that the target value be smaller than the limit since trimming can be an expensive operation and should be done infrequently. StreamStorageTargetMb type integer minimum value: 1 maximum value: 2147483647 EnableTransactionLog Defines whether the Storage component maintains a transaction log between checkpoint operations. The transaction log helps the product reduce data loss, should the host device lose power. EnableTransactionLog type bool TransactionLogLimitMB Defines the maximum size, in MB, of a transaction log. Should a transaction log exceed this size, it will be deleted, thus reducing the amount of data that can be recovered should the host device lose power. TransactionLogLimitMB type integer minimum value: 1 maximum value: 2147483647 CheckpointRateInSec Defines how often the storage component ensures recent data and configuration changes are flushed to storage. A setting of 0 disables checkpointing. Disabling checkpointing reduces the resiliency of the product and thus data loss can occur should the host device lose power. The value is expressed in seconds. CheckpointRateInSec type integer minimum value: 0 maximum value: 86400 Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required IngressDebugExpiration string Required StreamStorageLimitMb integer Required StreamStorageTargetMb integer Required EnableTransactionLog bool Optional TransactionLogLimitMB integer Optional CheckpointRateInSec integer Optional"
                                                     },
    "V1/Configuration/Logging configuration.html":  {
                                                        "href":  "V1/Configuration/Logging configuration.html",
                                                        "title":  "Logging configuration",
                                                        "keywords":  "Logging configuration Each message in the log displays the message severity level, timestamp, and the message itself. By default, Edge Data Store captures Information, Warning and Error messages in the message log. Note: The individual logging files of Edge Data Store and the components are the following: Edge Data Store: System_Logging.json Storage: Storage_Logging.json OSIsoft Adapter for OPC UA: OpcUa1_Logging.json OSIsoft Adapter for Modbus TCP: Modbus1_Logging.json If you have more than one OSIsoft Adapter of the same kind configured, the default file name will incrementally change, for example OpcUa2_Logging.json Configure logging To adjust the message logging behavior, complete the following: Using any text editor, open the componentId_Logging.json file that you want. Change the values as needed, so it looks similar to the Logging example . Use any tool capable of making HTTP requests to execute a POST command with the contents of that file to the respective endpoint. Example using curl for the Edge Data Store logging endpoint (run this command from the same directory where the file is located): curl -v -d \"@System_Logging.json\" -H \"Content-Type application/json\" application json\" http://localhost:5590/api/v1/configuration/System/Logging http:  localhost:5590 api v1 configuration System Logging Note: The other endpoints are the following: Edge Data Store: http://localhost:5590/api/v1/configuration/System/Logging http:  localhost:5590 api v1 configuration System Logging Storage: http://localhost:5590/api/v1/configuration/Storage/Logging http:  localhost:5590 api v1 configuration Storage Logging OSIsoft Adapter for OPC UA: http://localhost:5590/api/v1/configuration/OpcUa1/Logging http:  localhost:5590 api v1 configuration OpcUa1 Logging OSIsoft Adapter for Modbus TCP: http://localhost:5590/api/v1/configuration/Modbus1/Logging http:  localhost:5590 api v1 configuration Modbus1 Logging Logging schema The following table defines the basic behavior of any of the componentId_Logging.json files. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Parameters for logging The following parameters are available for configuring logging. Parameter Required Type Nullable Description LogFileCountLimit Optional integer Yes The log level settings that you want. The following options are available: Verbose - Captures all messages: Verbose, Debug, Information, Warning and Error Debug - Captures most messages: Debug, Information, Warning and Error Information - Captures most messages: Information, Warning and Error Warning - Captures only Warning and Error messages Error - Captures Error messages only LogFileSizeLimitBytes Optional integer Yes The maximum size in bytes of log files that the service will create for the component. It must be a positive integer. LogLevel Optional reference No The maximum number of log files that the service will create for the component. It must be a positive integer. Logging example { \"LogLevel\": \"Information\", \"LogFileSizeLimitBytes\": 34636833, \"LogFileCountLimit\": 31 }"
                                                    },
    "V1/Configuration/EdgeSystemConfiguration.html":  {
                                                          "href":  "V1/Configuration/EdgeSystemConfiguration.html",
                                                          "title":  "Edge Data Store configuration",
                                                          "keywords":  "Edge Data Store configuration This topic gives examples of a minimum and a maximum Edge Data Store configuration. Configure minimum Edge Data Store The following JSON file represents minimal configuration of an Edge Data Store. There are no Modbus TCP EDS adapter or OPC UA EDS adapter components, and the Storage component configurations are set to the default. If you configure a system with this JSON file, any existing Modbus TCP EDS adapter or OPC UA EDS adapter components will be disabled and removed. No storage data will be deleted or modified, and OMF and SDS data access will not be impacted. Save or copy the example JSON in a file named EdgeMinimumConfiguration.json in any directory on a device with Edge Data Store installed. Run the following curl command from the directory where the file is located: curl -i -d \"@EdgeMinimumConfiguration.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration The following will be set as the configuration of a running Edge Data Store. The configuration takes effect immediately after the command completes. { \"Storage\": { \"PeriodicEgressEndpoints\": [], \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\", \"checkpointRateInSec\": 30, \"transactionLogLimitMB\": 250, \"enableTransactionLog\": true }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ] } } This example results in a minimal configuration of Edge Data Store. It only supports OMF and SDS operations using REST. No egress is configured, so no data will be forwarded to either OCS or PI Web API . Configure maximum Edge Data Store The following JSON file represents maximum configuration of an Edge Data Store. There are Modbus TCP EDS adapter components and OPC UA EDS adapter components, and egress is configured to send to both PI Web API and OCS from both the default (operational data) and diagnostics (diagnostic data) namespace. Fill in any credentials or IP addresses with appropriate values for your environment. Save the edited version of the previous JSON in a file named EdgeMaximumConfiguration.json in any directory. Run the following curl command from the same directory where the file is located: curl -i -d \"@EdgeMaximumConfiguration.json\" -H \"Content-Type: application/json\" application json\" -X PUT http://localhost:5590/api/v1/configuration http:  localhost:5590 api v1 configuration The following will be set as the configuration for the running Edge Data Store. The configuration takes effect immediately after the command completes. { \"Modbus1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"IpAddress\": \"\u003cModbus IP address\u003e\", \"Port\": 502, \"ConnectTimeout\": 15000, \"ReconnectInterval\": 5000, \"RequestTimeout\": 9000, \"DelayBetweenRequests\": 0, \"MaxResponseDataLength\": 250 }, \"DataSelection\": [{ \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 1, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 2, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 3, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 4, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 }, { \"Selected\": \"true\", \"UnitId\": 1, \"RegisterType\": 3, \"RegisterOffset\": 5, \"DataTypeCode\": 20, \"BitMap\": \"16151413\", \"ConversionFactor\": 2, \"ConversionOffset\": 3.4, \"ScanRate\": 500 } ] }, \"Storage\": { \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\" }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"PeriodicEgressEndpoints\": [{ \"Id\": \"OCS\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"\u003cOCS OMF URL for your tenant and namespace\u003e\", \"ClientId\": \"\u003cOCS ClientId\u003e\", \"ClientSecret\": \"\u003cOCS ClientSecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null }, { \"Id\": \"PWA\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"https://\u003cyour \"https:  \u003cyour PI Web API server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null }, { \"Id\": \"OCSDiag\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"\u003cOCS OMF URL for your tenant and namespace\u003e\", \"ClientId\": \"\u003cOCS ClientId\u003e\", \"ClientSecret\": \"\u003cOCS ClientSecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null }, { \"Id\": \"PWADiag\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"ChangeMe\", \"TypePrefix\": \"ChangeMe\", \"Endpoint\": \"https://\u003cyour \"https:  \u003cyour PI Web API server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null } ] }, \"OpcUa1\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"DataSource\": { \"EndpointUrl\": \"opc.tcp://\u003cOPC \"opc.tcp:  \u003cOPC UA server IP and port\u003e/OSIsoftTestServer\", port\u003e OSIsoftTestServer\", \"UseSecureConnection\": false, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": \"OpcUa\" }, \"DataSelection\": [{ \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Cold Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Power\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_001.Power\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Efficiency\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_001.Efficiency\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Flowrate\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_001.Flowrate\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Power\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_002.Power\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Efficiency\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_002.Efficiency\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Flowrate\", \"NodeId\": \"ns=2;s=Line1.SF_Pump_002.Flowrate\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Level\", \"NodeId\": \"ns=2;s=Line1.Tank1.Level\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Mass\", \"NodeId\": \"ns=2;s=Line1.Tank1.Mass\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Volume\", \"NodeId\": \"ns=2;s=Line1.Tank1.Volume\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Level\", \"NodeId\": \"ns=2;s=Line1.Tank2.Level\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Mass\", \"NodeId\": \"ns=2;s=Line1.Tank2.Mass\", \"StreamId\": null }, { \"Selected\": false, \"Name\": \"Volume\", \"NodeId\": \"ns=2;s=Line1.Tank2.Volume\", \"StreamId\": null } ] }, \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"Components\": [{ \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ], \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 } } }"
                                                      },
    "V1/CommandLine/EdgeCmd utility.html":  {
                                                "href":  "V1/CommandLine/EdgeCmd utility.html",
                                                "title":  "EdgeCmd utility",
                                                "keywords":  "EdgeCmd utility With the EdgeCmd utility, you can configure and administer Edge Data Store on Linux and Windows just like with REST and command line arguments. Note: Configuration and administrative REST interfaces are generally exposed through the command line. Read/write Read write capabilities to the EDS storage component, OMF ingress, and SDS read/write read write capabilities are only available using the REST API. Install EdgeCmd utility The following sections provide instructions to install the EdgeCmd utility on Windows or Linux. Windows Note: You must have administrative privileges to run the installer. Complete the following to install the EdgeCmd utility on Windows: Copy the EdgeCmd.msi file to the file system of the device. To start the installer, double-click the EdgeCmd.msi file in Windows Explorer. Note: To change the install path from the default path of C:\\Program Files\\OSIsoft\\EdgeCmd, enter the following command in the command prompt and update the \u003cfile_path\u003e. OSIsoft recommends you use the default value. msiexec /i  i EdgeCmd.msi INSTALLFOLDER=\u003cfile_path\u003e Note: INSTALLFOLDER must be in all caps as shown in the preceding example. The EdgeCmd utility is installed on your device. Linux Note: You must have administrative privileges to install the software, for example root or sudo privilege. Complete the following to install the EdgeCmd utility on Linux: Open a terminal window and type the sudo command for the appropriate EdgeCmd deb file for your processor: Debian 9 or later (Intel/AMD (Intel AMD 64-bit processors) sudo apt install ./EdgeCmd_linux-x64.deb . EdgeCmd_linux-x64.deb Debian 9 or later (ARM32, Raspberry PI 2,3,4: Raspbian, BeagleBone) sudo apt install ./EdgeCmd_linux-arm.deb . EdgeCmd_linux-arm.deb Debian 9 or later (Raspberry PI 3,4: Ubuntu ARM64 Server, Google Coral Dev Board, Nvidia Nano Jetson) sudo apt install ./EdgeCmd_linux-arm64.deb . EdgeCmd_linux-arm64.deb A validation check for prerequisites will be completed. If the Linux OS is up to date, the install will complete and the EdgeCmd utility will be available on your device. If the install fails, run the following commands from the terminal window and try the install again: sudo apt update sudo apt upgrade"
                                            },
    "V1/CommandLine/Configure Edge Data Store components.html":  {
                                                                     "href":  "V1/CommandLine/Configure Edge Data Store components.html",
                                                                     "title":  "Configure Edge Data Store components",
                                                                     "keywords":  "Configure Edge Data Store components The EdgeCmd utility enables you to add, configure, and delete Edge Data Store components. View configured components Complete the following to view the components currently configured on Edge Data Store. Open command line. Type the following in the command line and press Enter. edgecmd Configuration System Components Add components Complete the following to add a new component. Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e and \u003ccomponentType\u003e with the values that you want and press Enter. Note: Valid component types are Modbus and OpcUa . If you are trying to register a Modbus EDS adapter, use Modbus and if you are trying to register an OPC UA adapter, use OpcUa . edgecmd Configuration System Components componentId=\u003ccomponentId\u003e componentType=\u003ccomponentType\u003e Example : Modbus adapter component registration edgecmd Configuration System Components componentId=Modbus1 componentType=Modbus Configure components The Modbus TCP EDS adapter and OPC UA EDS adapter each have three configurable facets: data source, data selection, and logging. Complete the following to configure a facet. Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e and \u003cfacetName\u003e with the values that you want. edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e Add key=value pairs to specify which values of the facet that you want to configure are to be changed and press Enter. Example : Configuration of the data source facet of a Modbus adapter edgecmd Configuration Modbus1 DataSource IpAddress=117.23.45.110 port=502 ConnectTimeout=15000 StreamIdPrefix=\"DataSource1\" For detailed information on how to configure each adapter, see OPC UA EDS adapter and Modbus TCP EDS adapter . Delete components Complete the following to delete components from the Edge Data Store. Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e with the ID of the component that you want to delete, and press Enter. edgecmd Configuration System Components id=\u003ccomponentId\u003e delete Note: You cannot delete the Storage component because it is required for Edge Data Store to operate."
                                                                 },
    "V1/Administration/Retrieve product version information.html":  {
                                                                        "href":  "V1/Administration/Retrieve product version information.html",
                                                                        "title":  "Retrieve product version information",
                                                                        "keywords":  "Retrieve product version information The product version information includes the Edge Data Store version number, the .NET Core version, the Core CLR version, and the operating system. This information can be useful for troubleshooting purposes. Complete the following to retrieve the product version of the Edge Data Store: Start any Configuration tool capable of making HTTP requests. Execute a GET command to the following endpoint: http://localhost:5590/api/v1/diagnostics/productinformation http:  localhost:5590 api v1 diagnostics productinformation Example using curl: curl -v http://localhost:5590/api/v1/Diagnostics/ProductInformation http:  localhost:5590 api v1 Diagnostics ProductInformation"
                                                                    },
    "V1/Edge_Data_Store.html":  {
                                    "href":  "V1/Edge_Data_Store.html",
                                    "title":  "Edge Data Store",
                                    "keywords":  "Edge Data Store Edge Data Store (EDS) is an embedded data server that runs on Linux and Windows. EDS provides a lightweight data collection and storage application designed to enable the capturing of data for historical storage and analysis at the edge of networks. A storage component based on sequential data storage technology is provided. You can configure and administer EDS through REST programming, configuration, administrative interfaces, and the EdgeCmd command line tool. EDS complements existing OSIsoft products, and is designed for small devices. You can install and run it on 64-bit Intel/AMD Intel AMD compatible, 32-bit ARM v7/v8 v7 v8 compatible, and 64-bit ARM v8 compatible chips. While not a replacement for a PI System, EDS augments the PI System by providing historical data access in situations where deploying a full PI System is impractical. EDS provides native capability, via its Egress functionality, to send data to a PI System or to OSIsoft Cloud Services for long term historical storage and analysis. EDS provides the following capabilities: OMF data ingress Edge connectivity through Modbus TCP and OPC UA Configurable data storage Data egress to PI Web API and OCS REST API to enable custom applications for visualization and analytics on Edge Data Store Edge Data Store architecture The following diagram depicts the relationships of architectural components to one another in the Edge Data Store: Edge Data Store data flow The following diagram depicts the flow of data in the Edge Data Store: Edge Data Store components The following diagram depicts the relationship of key functions to relevant components of the Edge Data Store: Data ingress to Edge Data Store Edge Data Store can ingress data in a number of ways. There are two built-in adapters: Modbus TCP EDS adapter and OPC UA EDS adapter. Data can also be ingressed using OSIsoft Message Format (OMF) and the Sequential Data Store (SDS) REST APIs. The following diagram depicts an OMF data ingress scenario in the Edge Data Store: During installation of Edge Data Store, you can choose to install either a Modbus TCP EDS adapter or an OPC UA EDS adapter, or both. The Modbus TCP and OPC UA EDS adapters require configuration of data source and data selection before they can collect data in Edge Data Store. You can use OMF data ingress once Edge Data Store is installed, with no further configuration steps. Edge Data Store is composed of components, and is designed to allow additional EDS adapters at a later point. Local data read and write access You can access all data in Edge Data Store by using the Sequential Data Store (SDS) REST API. You can use this data for local applications that perform analytics or visualization. Example EDS visualization application The following diagram depicts the flow of data from a customer visualization application into Edge Data Store, via either OMF or SDS REST calls: Example EDS analytics application The following diagram depicts the flow of data from a customer analytics application into Edge Data Store, via either OMF or SDS REST calls: Data egress from Edge Data Store Edge Data Store can send data to both PI Data Archive using PI Web API and OSIsoft Cloud Services (OCS). After Edge Data Store is installed, additional configuration is necessary to send data to both OCS and PI Web API. For information about using a componet of EDS, see the corresponding quick start guide for that component."
                                },
    "V1/Troubleshooting/Troubleshooting.html":  {
                                                    "href":  "V1/Troubleshooting/Troubleshooting.html",
                                                    "title":  "Troubleshoot Edge Data Store",
                                                    "keywords":  "Troubleshoot Edge Data Store You have both local and remote means of diagnosing any issues you encounter while using or developing against Edge Data Store. Edge Data Store supports a diagnostics namespace that is used to store streams containing diagnostic information from Edge Data Store itself. As with any other stream data stored in the Edge Storage component, you can egress this to either a PI Web Server or OSIsoft Cloud Services to monitor the state of a system remotely. In addition to diagnostics data, all components in Edge Data Store support OMF health messages. You configure health messages with the Health endpoint configuration , to send health data to either PI Web Server or OSIsoft Cloud Service endpoints for remote monitoring of devices. OMF ingress Complete the following when a custom application fails to write stream data to EDS: Verify the custom application is sending OMF messages in the correct order: 1) OMF type, 2) OMF container, 3) OMF data. Note: OMF messages must be sent in the correct order to be ingressed into Edge Data Store. Refer to logging of warnings, errors, and messages for help with diagnosing these issues. OMF ingress logging Ingress logging messages provide a record of ingress events. Complete the following to capture maximum information for troubleshooting: Refer to Logging Configuration to set logging parameters. For maximum message logging information, set the log level to Trace . OMF ingress message debugging Debugging helps to troubleshoot problems between an OMF application and Edge Data Store. Complete the following to enable debugging: Refer to Storage Runtime Configuration to enable debugging. Set an appropriate time value for the IngressDebugExpiration property. Note: You can also disable debugging by setting the expiration value to null . Examples of valid strings representing date and time: UTC: ???yyyy-mm-ddThh:mm:ssZ??? Local: ???mm-dd-yyyy hh:mm:ss??? Periodic egress EDS periodic egress extracts data from SDS streams and sends the appropriate sequences of type, container, and data OMF messages on startup. Note: If you see unexpected data in an OCS or PI System, check if multiple devices are writing to the same SDS stream. Check all egress configuration files in Edge Data Store to verify whether any endpoints are duplicated. A duplicate endpoint means that more than one device is egressing data to it, resulting in unexpected data. Assign stream prefixes in the periodic egress endpoint configuration to ensure that output data streams are logically separated in the systems of record. Note: Type prefixes may be helpful if you have changed a stream type definition on EDS. OMF types on both OCS and the PI System are immutable once created. If the type of the data stream changes, it is best to either delete the old type definition (if nothing is still using it) or add a type prefix to create a new unique type that will be used by new streams egressing from EDS to the systems of record. Periodic egress logging Egress logging messages provide a record of egress events. Complete the following to capture maximum information for troubleshooting: Refer to Logging Configuration to set logging parameters. For maximum message logging information, set the log level to Trace . Periodic egress debugging Debugging helps to troubleshoot problems between Edge Data Store and the egress destination. Complete the following to enable debugging: Refer to Data egress configuration to enable debugging. Set an appropriate time value for the IngressDebugExpiration property. Note: You can also disable debugging by setting the expiration value to null . Examples of valid strings representing date and time: UTC: ???yyyy-mm-ddThh:mm:ssZ??? Local: ???mm-dd-yyyy hh:mm:ss??? Debugging folder/file folder file structure The overall number and content length of each request/response request response pair can be quite large. Debug information is therefore stored to disk in a separate location from the typical log messages. Debug folders and files will be created under the Edge Data Store data folder as follows: Windows: %programdata%\\OSIsoft\\EdgeDataStore\\Storage\\egressdump\\{tenantId}\\{namespaceId}\\{egressId}\\{omfType}\\{Ticks}-{Guid}-{Req/Res}.txt %programdata%\\OSIsoft\\EdgeDataStore\\Storage\\egressdump\\{tenantId}\\{namespaceId}\\{egressId}\\{omfType}\\{Ticks}-{Guid}-{Req Res}.txt Linux: /usr/share/OSIsoft/EdgeDataStore/Storage/egressdump/{tenantId}/{namespaceId}/{egressId}/{omfType}/{Ticks}-{Guid}-{Req/Res}.txt  usr share OSIsoft EdgeDataStore Storage egressdump {tenantId} {namespaceId} {egressId} {omfType} {Ticks}-{Guid}-{Req Res}.txt The OMF specific elements of the file structure are defined in the following table: Element Represents omfType The OMF message type: Type, Container, or Data. Ticks The time in milliseconds (tick count) for UTC DateTime when determined message would be written to disk. Guid The unique GUID for each request/response request response pair. Req/Res Req Res Whether the message was HTTP request or response."
                                                },
    "V1/Overview/VisualizationQuickStart.html":  {
                                                     "href":  "V1/Overview/VisualizationQuickStart.html",
                                                     "title":  "Edge Data Store visualization quick start",
                                                     "keywords":  "Edge Data Store visualization quick start This topic provides a quick start for displaying data from the EDS storage component on the local device where Edge Data Store is installed. The following example should run on any device supported by Edge Data Store, and iterates through all streams in the default namespace, continually displaying the latest values to the screen. using System; using System.Collections.Generic; using System.Net.Http; using Newtonsoft.Json; namespace EdgeDataScroller { class EdgeStream { public string Id { get; set; } } class DataScroller { static HttpClient _client = new HttpClient(); private static Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e GetDataForNamespace(string ns) { Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e outputs = new Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e(); string uri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/{0}/streams\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces {0} streams\", ns); string json = _client.GetStringAsync(uri).Result; List\u003cEdgeStream\u003e streams = JsonConvert.DeserializeObject\u003cList\u003cEdgeStream\u003e\u003e(json); foreach (var stream in streams) { string lastValueUri = string.Format(\"http://localhost:5590/api/v1/tenants/default/namespaces/{0}/streams/{1}/Data/Last\", string.Format(\"http:  localhost:5590 api v1 tenants default namespaces {0} streams {1} Data Last\", ns, stream.Id.Trim()); string lastValueJson = _client.GetStringAsync(lastValueUri).Result; Dictionary\u003cstring, object\u003e values = JsonConvert.DeserializeObject\u003cDictionary\u003cstring, object\u003e\u003e(lastValueJson); outputs.Add(stream.Id.Trim(), values); } return outputs; } static void DisplayData(List\u003cstring\u003e namespaces, TimeSpan interval) { Dictionary\u003cstring, Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e\u003e outputs = new Dictionary\u003cstring, Dictionary\u003cstring, Dictionary\u003cstring, object\u003e\u003e\u003e(); Console.WriteLine(\"Data Displayed at \" + DateTime.UtcNow.ToString(\"o\")); foreach (string ns in namespaces) { outputs.Add(ns, GetDataForNamespace(ns)); } foreach (string ns in outputs.Keys) { foreach (string stream in outputs[ns].Keys) { if (null == outputs[ns][stream]) { Console.WriteLine(\"No values for \" + stream); continue; } foreach (string field in outputs[ns][stream].Keys) { object obj = outputs[ns][stream][field]; string value = obj.ToString(); if (obj is DateTime) { value = ((DateTime)obj).ToString(\"o\"); } Console.WriteLine($\"{ns}.{stream}.{field} = {value}\"); } } Console.WriteLine(\"****\"); } Console.WriteLine(string.Empty); System.Threading.Thread.Sleep(interval); } static void Main(string[] args) { List\u003cstring\u003e namespaces = new List\u003cstring\u003e(); namespaces.Add(\"default\"); TimeSpan interval = TimeSpan.FromSeconds(5.0); if (null != args \u0026\u0026 args.Length \u003e 0) { string choice = args[0].Trim().ToLowerInvariant(); if (choice == \"all\") { namespaces.Add(\"diagnostics\"); } if (choice == \"diagnostics\") { namespaces.Clear(); namespaces.Add(\"diagnostics\"); } if (args.Length \u003e 1) { string newInterval = args[1].Trim(); double newValue = -1.0; if (double.TryParse(newInterval, out newValue)) { interval = TimeSpan.FromSeconds(newValue); } } } while (true) DisplayData(namespaces, interval); } } }"
                                                 },
    "V1/Overview/SDSQuickStart.html":  {
                                           "href":  "V1/Overview/SDSQuickStart.html",
                                           "title":  "SDS quick start",
                                           "keywords":  "SDS quick start This topic provides quick start instructions to get data into the EDS storage component using the Sequential Data Store (SDS) REST API. SDS is the same technology used in OCS for storing data, so the usage of the REST APIs is very similar to OCS for reading and writing data. All data from all sources on Edge Data Store (Modbus TCP, OPC UA, OMF, SDS) can be read using the SDS REST APIs on the local device, in the default tenant and the default namespace. Create an SDS type Complete the following steps to create an SDS type that describes the format of the data to be stored in a container. Create a JSON file using the example below: { \"Id\": \"Simple\", \"Name\": \"Simple\", \"SdsTypeCode\": 1, \"Properties\": [ { \"Id\": \"Time\", \"Name\": \"Time\", \"IsKey\": true, \"SdsType\": { \"SdsTypeCode\": 16 } }, { \"Id\": \"Measurement\", \"Name\": \"Measurement\", \"SdsType\": { \"SdsTypeCode\": 14 } } ] } Note: The data to be written is a timestamp and numeric value. It is indexed by a timestamp, and the numeric value that will be stored is a 64-bit floating point value. Save the JSON file the name SDSCreateType.json. Run the following curl script: curl -i -d \"@SDSCreateType.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/types/Simple http:  localhost:5590 api v1 tenants default namespaces default types Simple When this script completes successfully, an SDS type with the same name is created on the server. You can create any number of containers from a single type, as long as they use a timestamp as an index and a 64-bit floating point value. You only need to create a type the first time you send data with a custom application. It does not cause an error to resend the same definition at a later time. Create an SDS stream Complete the following steps to create an SDS stream. Create a JSON file using the example below: { \"Id\": \"Simple\", \"Name\": \"Simple\", \"TypeId\": \"Simple\" } Note: This stream references the type you created earlier. An error will occur if the type does not exist when the stream is created. As with an SDS type, you only need to create a stream once before sending data events. Resending the same definition repeatedly does not cause an error. Save the JSON file with the name SDSCreateStream.json. Run the following curl script: curl -i -d \"@SDSCreateStream.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple http:  localhost:5590 api v1 tenants default namespaces default streams Simple When this script completes successfully, an SDS stream is created on the server to store data defined by the specified type. Write data events to the SDS stream After you have created a type and container, you can write data using SDS. Complete the following steps to write data to a stream. Create a JSON file using the example below: [{ \"Time\": \"2017-11-23T17:00:00Z\", \"Measurement\": 50.0 }, { \"Time\": \"2017-11-23T18:00:00Z\", \"Measurement\": 60.0 }] Note: This example includes two data events that will be stored in the SDS Stream you created in the previous steps. For optimal performance, you should batch SDS values when writing them. Save the JSON file with the name SDSWriteData.json. Run the following curl script: curl -i -d \"@SDSWriteData.json\" -H \"Content-Type: application/json\" application json\" -X POST http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data When this script completes successfully, two values are written to the SDS stream. Read last data written using SDS Use the SDS REST API to read back data which has been written to the server. The following is an example curl script that reads back the last value entered: curl http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data/Last http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data Last Use the following GET command to return the last value written: {\"Time\":\"2017-11-23T18:00:00Z\",\"Measurement\":60.0} Read a range of data events written using SDS Use the SDS REST API to read back data which has been written to the server. The following is an example curl script that reads back a time range of values entered: curl \"http://localhost:5590/api/v1/tenants/default/namespaces/default/streams/Simple/Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" \"http:  localhost:5590 api v1 tenants default namespaces default streams Simple Data?startIndex=2017-07-08T13:00:00Z\u0026count=100\" [{\"Time\":\"2017-11-23T17:00:00Z\",\"Measurement\":50.0},{\"Time\":\"2017-11-23T18:00:00Z\",\"Measurement\":60.0}] Both values that were entered were returned. A maximum of 100 values after the specified timestamp will be returned."
                                       },
    "V1/Overview/Quick start guides.html":  {
                                                "href":  "V1/Overview/Quick start guides.html",
                                                "title":  "Quick start guides",
                                                "keywords":  "Quick start guides The quick start guides in this section provide basic instructions to get started with Edge Data Store, its components and utilities. Each quick start guide provides guidance for setup, configuration and basic operation. Refer to the respective configuration sections for detailed instructions on each EDS component or utility. The examples in each quick start guide use curl, a commonly available tool on both Windows and Linux. You can use the same operations with any programming language or tool that supports making REST calls. You can also configure Edge Data Store components with the EdgeCmd utility. If available on your device, use a browser to complete data retrieval steps (GET commands) to validate successful configurations. All quick start guide examples assume Edge Data Store has been installed and is accessible through a REST API using the default installed port (5590)."
                                            },
    "V1/Configuration/Schemas/System_HealthEndpoints_schema.html":  {
                                                                        "href":  "V1/Configuration/Schemas/System_HealthEndpoints_schema.html",
                                                                        "title":  "Sample OMF health endpoint configuration",
                                                                        "keywords":  "Sample OMF health endpoint configuration The OMF health endpoint configuration schema specifies how to formally describe the OMF health endpoint parameters. [{ \"endpoint\": \"https://\u003cpi \"https:  \u003cpi web api server\u003e/piwebapi/omf/\", server\u003e piwebapi omf \", \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"buffering\": 0, \"maxBufferSizeMB\": 0 }, { \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"buffering\": 0, \"maxBufferSizeMB\": 0 } ] OMF health endpoint configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden System_HealthEndpoints_schema.json OMF health endpoint configuration properties Property Type Required Nullable Defined by Buffering reference #/definitions/BufferType # definitions BufferType Optional No OmfHealthEndpointConfiguration (this schema) ClientId string Optional Yes OmfHealthEndpointConfiguration (this schema) ClientSecret string Optional Yes OmfHealthEndpointConfiguration (this schema) Endpoint string Optional Yes OmfHealthEndpointConfiguration (this schema) Id string Optional Yes OmfHealthEndpointConfiguration (this schema) MaxBufferSizeMB integer Optional No OmfHealthEndpointConfiguration (this schema) Password string Optional Yes OmfHealthEndpointConfiguration (this schema) UserName string Optional Yes OmfHealthEndpointConfiguration (this schema) ValidateEndpointCertificate boolean Optional No OmfHealthEndpointConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Buffering reference #/definitions/BufferType # definitions BufferType Optional ClientId string Optional ClientSecret string Optional Endpoint string Optional Id string Optional MaxBufferSizeMB integer Optional Password string Optional UserName string Optional ValidateEndpointCertificate boolean Optional"
                                                                    },
    "V1/Configuration/Schemas/Storage_Runtime_schema.html":  {
                                                                 "href":  "V1/Configuration/Schemas/Storage_Runtime_schema.html",
                                                                 "title":  "Sample storage runtime configuration",
                                                                 "keywords":  "Sample storage runtime configuration { \"StreamStorageLimitMb\": 2, \"StreamStorageTargetMb\": 1, \"IngressDebugExpiration\": \"0001-01-01T00:00:00\" } Storage runtime configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Storage_Runtime_schema.json Storage runtime configuration properties Property Type Required Nullable Defined by IngressDebugExpiration string Required No StorageRuntimeConfiguration (this schema) StreamStorageLimitMb integer Required No StorageRuntimeConfiguration (this schema) StreamStorageTargetMb integer Required No StorageRuntimeConfiguration (this schema) IngressDebugExpiration Ingress Debug Expiration is a property that can be used when debugging OMF. If the date and time is the future incoming OMF messages will be logged until the date and time specified. Once the configured time is past OMF messages will no longer be logged for debugging purposes. IngressDebugExpiration type string format: date-time ??? date and time (according to RFC 3339, section 5.6 ) minimum length: 1 characters StreamStorageLimitMb StreamStorageLimitMb is the maximum size in megabytes that a stream can reach. When a stream exceeds the size specified, older data will be deleted from the file. Data will be removed from the stream until the stream is at or below the StreamStorageTargetMb value. It is recommended that the target value be smaller than the limit since trimming can be an expensive operation and should be done infrequently. StreamStorageLimitMb type integer minimum value: 2 maximum value: 2147483647 StreamStorageTargetMb StreamStorageTargetMb is the size in megabytes that a stream will be reduced to after StreamStorageLimitMb size is reached for a single stream. When a stream exceeds the size specified, older data will be deleted from the file. Data will be removed from the stream until the stream is at or below the StreamStorageTargetMb value. It is recommended that the target value be smaller than the limit since trimming can be an expensive operation and should be done infrequently. StreamStorageTargetMb type integer minimum value: 1 maximum value: 2147483647 Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required IngressDebugExpiration string Required StreamStorageLimitMb integer Required StreamStorageTargetMb integer Required"
                                                             },
    "V1/Configuration/Schemas/Storage_PeriodicEgressEndpoints_schema.html":  {
                                                                                 "href":  "V1/Configuration/Schemas/Storage_PeriodicEgressEndpoints_schema.html",
                                                                                 "title":  "Periodic egress configuration schema",
                                                                                 "keywords":  "Periodic egress configuration schema This schema is used to configure data egress from Edge Data Store to a PI Server or to OCS. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Storage_PeriodicEgressEndpoints_schema.json Sample periodic egress configuration [{ \"Id\": \"OCS Data\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"PI Web API Data\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"default\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cyourserver\u003e/piwebapi/omf/\", \"https:  \u003cyourserver\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"OCS Diagnostics\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cOCS \"https:  \u003cOCS OMF endpoint\u003e\", \"ClientId\": \"\u003cclientid\u003e\", \"ClientSecret\": \"\u003cclientsecret\u003e\", \"UserName\": null, \"Password\": null, \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true }, { \"Id\": \"PI Web API Diagnostics\", \"ExecutionPeriod\": \"00:00:50\", \"Name\": null, \"NamespaceId\": \"diagnostics\", \"Description\": null, \"Enabled\": true, \"Backfill\": false, \"EgressFilter\": \"\", \"StreamPrefix\": \"\u003cmakeunique\u003e\", \"TypePrefix\": \"\u003cmakeunique\u003e\", \"Endpoint\": \"https://\u003cyourserver\u003e/piwebapi/omf/\", \"https:  \u003cyourserver\u003e piwebapi omf \", \"ClientId\": null, \"ClientSecret\": null, \"UserName\": \"\u003cusername\u003e\", \"Password\": \"\u003cpassword\u003e\", \"DebugExpiration\": null, \"TokenEndpoint\": null, \"ValidateEndpointCertificate\": true } ] Periodic egress configuration properties Property Type Required Nullable Defined by Backfill boolean Optional No PeriodicEgressConfiguration (this schema) ClientId string Optional Yes PeriodicEgressConfiguration (this schema) ClientSecret string Optional Yes PeriodicEgressConfiguration (this schema) DebugExpiration string Optional Yes PeriodicEgressConfiguration (this schema) Description string Optional Yes PeriodicEgressConfiguration (this schema) EgressFilter string Optional Yes PeriodicEgressConfiguration (this schema) Enabled boolean Optional No PeriodicEgressConfiguration (this schema) Endpoint string Required No PeriodicEgressConfiguration (this schema) ExecutionPeriod string Required No PeriodicEgressConfiguration (this schema) Id string Optional Yes PeriodicEgressConfiguration (this schema) Name string Optional Yes PeriodicEgressConfiguration (this schema) NamespaceId string Optional Yes PeriodicEgressConfiguration (this schema) Password string Optional Yes PeriodicEgressConfiguration (this schema) StreamPrefix string Optional Yes PeriodicEgressConfiguration (this schema) TokenEndpoint string Optional Yes PeriodicEgressConfiguration (this schema) TypePrefix string Optional Yes PeriodicEgressConfiguration (this schema) UserName string Optional Yes PeriodicEgressConfiguration (this schema) ValidateEndpointCertificate boolean Optional No PeriodicEgressConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Backfill boolean Optional ClientId string Optional ClientSecret string Optional DebugExpiration string Optional Description string Optional EgressFilter string Optional Enabled boolean Optional Endpoint string Required ExecutionPeriod string Required Id string Optional Name string Optional NamespaceId string Optional Password string Optional StreamPrefix string Optional TokenEndpoint string Optional TypePrefix string Optional UserName string Optional ValidateEndpointCertificate boolean Optional"
                                                                             },
    "V1/Configuration/Schemas/Storage_Logging_schema.html":  {
                                                                 "href":  "V1/Configuration/Schemas/Storage_Logging_schema.html",
                                                                 "title":  "Storage logging configuration schema",
                                                                 "keywords":  "Storage logging configuration schema The Storage logging configuration schema specifies how to formally describe the logging parameters for Storage. Abstract Extensible Status Identifiable Custom properties Additional properties Can be instantiated Yes Experimental No Forbidden Forbidden Storage logging configuration properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional"
                                                             },
    "V1/Configuration/Schemas/OpcUa_schema.html":  {
                                                       "href":  "V1/Configuration/Schemas/OpcUa_schema.html",
                                                       "title":  "OPC UA configuration schema",
                                                       "keywords":  "OPC UA configuration schema Abstract Extensible Status Identifiable Custom Properties Additional Properties Defined In Can be instantiated Yes Experimental No Forbidden Forbidden OpcUa_Logging_schema.json OPC UA configuration properties Property Type Required Nullable Defined by Logging OpcUaLoggingConfiguration Optional Yes EdgeLoggerConfiguration DataSource DataSourceConfiguration Optional Yes ComponentsConfiguration DataSelection [OpcUaDataSelectionConfiguration] Optional Yes DataSelectionConfiguration"
                                                   },
    "V1/Configuration/Schemas/OpcUa_Logging_schema.html":  {
                                                               "href":  "V1/Configuration/Schemas/OpcUa_Logging_schema.html",
                                                               "title":  "OPC UA logging configuration schema",
                                                               "keywords":  "OPC UA logging configuration schema The OPC UA logger configuration schema specifies how to formally describe the logging parameters for OPC UA. Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden OpcUa_Logging_schema.json OPC UA logging configuration properties Property Type Required Nullable Defined by LogFileCountLimit integer Optional Yes EdgeLoggerConfiguration (this schema) LogFileSizeLimitBytes integer Optional Yes EdgeLoggerConfiguration (this schema) LogLevel reference Optional No EdgeLoggerConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required LogFileCountLimit integer Optional LogFileSizeLimitBytes integer Optional LogLevel Optional"
                                                           },
    "V1/Configuration/Schemas/OpcUa_DataSource_schema.html":  {
                                                                  "href":  "V1/Configuration/Schemas/OpcUa_DataSource_schema.html",
                                                                  "title":  "Sample OPC UA data source configuration",
                                                                  "keywords":  "Sample OPC UA data source configuration The OPC UA data source configuration schema specifies how to formally describe the data source parameters for OPC UA. { \"EndpointUrl\": \"opc.tcp://\u003cip \"opc.tcp:  \u003cip address\u003e:\u003cport - often 62541\u003e/\u003cserver 62541\u003e \u003cserver path\u003e\", \"UseSecureConnection\": false, \"UserName\": null, \"Password\": null, \"RootNodeIds\": null, \"IncomingTimestamp\": \"Source\", \"StreamIdPrefix\": \"OpcUa\" } OPC UA data source configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden OpcUa_DataSource_schema.json OPC UA data source configuration properties Property Type Required Nullable Defined by EndpointUrl string Optional Yes DataSourceConfiguration (this schema) IncomingTimestamp reference Optional No DataSourceConfiguration (this schema) Password string Optional Yes DataSourceConfiguration (this schema) RootNodeIds string Optional Yes DataSourceConfiguration (this schema) StreamIdPrefix string Optional Yes DataSourceConfiguration (this schema) UseSecureConnection boolean Optional No DataSourceConfiguration (this schema) UserName string Optional Yes DataSourceConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required EndpointUrl string Optional IncomingTimestamp Optional Password string Optional RootNodeIds string Optional StreamIdPrefix string Optional UseSecureConnection boolean Optional UserName string Optional"
                                                              },
    "V1/Configuration/Schemas/OpcUa_DataSelection_schema.html":  {
                                                                     "href":  "V1/Configuration/Schemas/OpcUa_DataSelection_schema.html",
                                                                     "title":  "Sample OPC UA data selection configuration",
                                                                     "keywords":  "Sample OPC UA data selection configuration The OPC UA data selection configuration schema specifies how to formally describe the data selection parameters for OPC UA. [{ \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1001.HotSideOutletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Cold Side Inlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.ColdSideInletTemperature\", \"StreamId\": null }, { \"Selected\": true, \"Name\": \"Hot Side Outlet Temperature\", \"NodeId\": \"ns=2;s=Line1.HeatExchanger1002.HotSideOutletTemperature\", \"StreamId\": null } ] OPC UA data selection configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden OpcUa_DataSelection_schema.json OPC UA data selection configuration properties Property Type Required Nullable Defined by Name string Optional Yes DataCollectionItem (this schema) NodeId string Optional Yes DataCollectionItem (this schema) Selected boolean Optional No DataCollectionItem (this schema) StreamId string Optional Yes DataCollectionItem (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Name string Optional NodeId string Optional Selected boolean Optional StreamId string Optional"
                                                                 },
    "V1/Configuration/Schemas/System_schema.html":  {
                                                        "href":  "V1/Configuration/Schemas/System_schema.html",
                                                        "title":  "Sample system configuration",
                                                        "keywords":  "Sample system configuration \"System\": { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"Components\": [{ \"componentId\": \"OpcUa1\", \"componentType\": \"OpcUa\" }, { \"componentId\": \"Modbus1\", \"componentType\": \"Modbus\" }, { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ], \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 } } System configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_Logging_schema.json System configuration properties Property Type Required Nullable Defined by Logging SystemLoggingConfiguration Optional Yes EdgeLoggerConfiguration Components [SystemComponentsConfiguration] Optional Yes ComponentsConfiguration HealthEndpoints [SystemHealthEndpointsConfiguration] Optional Yes HealthEndpointsConfiguration Port SystemPortConfiguration Optional Yes PortConfiguration"
                                                    },
    "V1/Configuration/Schemas/System_Port_schema.html":  {
                                                             "href":  "V1/Configuration/Schemas/System_Port_schema.html",
                                                             "title":  "Sample system port configuration",
                                                             "keywords":  "Sample system port configuration { \"Port\": 5590 } System port configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden System_Port_schema.json System port configuration properties Property Type Required Nullable Defined by Port integer minimum value: 1024 maximum value: 65535 Optional No PortConfiguration (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required Port integer Optional"
                                                         },
    "V1/Configuration/Schemas/System_Components_schema.html":  {
                                                                   "href":  "V1/Configuration/Schemas/System_Components_schema.html",
                                                                   "title":  "Sample Edge Data Store components configuration",
                                                                   "keywords":  "Sample Edge Data Store components configuration [ { \"ComponentId\": \"OpcUa1\", \"ComponentType\": \"OpcUa\" }, { \"ComponentId\": \"Modbus1\", \"ComponentType\": \"Modbus\" }, { \"ComponentId\": \"Storage\", \"ComponentType\": \"EDS.Component\" } ] Edge Data Store components configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden System_Components_schema.json Edge Data Store components configuration properties Property Type Required Nullable Group ComponentId string Optional Yes #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig ComponentType string Optional Yes #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig Edge Data Store configuration properties Property Type Required Nullable Defined by ComponentConfigurations reference #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig Optional Yes EdgeDataStoreConfig (this schema) Note: All of the following requirements need to be fulfilled. Requirement 1 ??? #/definitions/EdgeConfigurationBase # definitions EdgeConfigurationBase Requirement 2 object with following properties: Property Type Required ComponentConfigurations reference #/definitions/EdgeComponentConfig # definitions EdgeComponentConfig Optional"
                                                               },
    "V1/Configuration/Schemas/Storage_schema.html":  {
                                                         "href":  "V1/Configuration/Schemas/Storage_schema.html",
                                                         "title":  "Sample Storage configuration",
                                                         "keywords":  "Sample Storage configuration \"Storage\": { \"Runtime\": { \"streamStorageLimitMb\": 2, \"streamStorageTargetMb\": 1, \"ingressDebugExpiration\": \"0001-01-01T00:00:00\" }, \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"PeriodicEgressEndpoints\": [] } Storage configuration schema Abstract Extensible Status Identifiable Custom properties Additional properties Defined in Can be instantiated Yes Experimental No Forbidden Forbidden Modbus_Logging_schema.json Storage configuration properties Property Type Required Nullable Defined by Runtime StorageRuntimeConfiguration Optional Yes StorageRuntimeConfiguration Logging StorageLoggingConfiguration Optional Yes StorageLoggingConfiguration PeriodicEgressEndpoints [PeriodicEgressEndpointsConfiguration] Optional Yes PeriodicEgressEndpointsConfiguration"
                                                     },
    "V1/Configuration/Configuration.html":  {
                                                "href":  "V1/Configuration/Configuration.html",
                                                "title":  "Configuration",
                                                "keywords":  "Configuration You configure each functional area of Edge Data Store to define connectivity and system behavior appropriate to your particular use case. The topics contained in this section provide instructions for each area. The examples in the Configuration topics use curl, a commonly available tool on both Windows and Linux. You can use the same operations with any programming language or tool that supports making REST calls. You can also configure Edge Data Store components with the EdgeCmd command line utility. To validate successful configurations, you can accomplish data retrieval steps (GET commands) using a browser, if available on your device. For more information on tools for Edge Data Store configuration, see Configuration tools ."
                                            },
    "V1/CommandLine/Retrieve existing system configuration.html":  {
                                                                       "href":  "V1/CommandLine/Retrieve existing system configuration.html",
                                                                       "title":  "Retrieve existing system configuration",
                                                                       "keywords":  "Retrieve existing system configuration You can use the EdgeCmd utility to view the configuration for each part of Edge Data Store. View components configuration Complete the following to view the configuration of every component in Edge Data Store: Open command line. Type the following in the command line and press Enter. edgecmd Configuration View a specific component configuration Complete the following to view the configuration of a specific component: Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e with the ID of the component, and press Enter. edgecmd Configuration \u003ccomponentId\u003e For more information, see the example, View the configuration of the System component . View a specific facet configuration Complete the following to view the configuration of a specific facet of a component: Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e and \u003cfacetName\u003e with the ID of the component and the facet name, and press Enter. edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e For more information, see the example, View the configuration of the Logging facet within the Storage component . View a specific facet entry configuration Complete the following to view the configuration of a specific facet entry of a component: Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e and \u003cfacetName\u003e with the ID of the component and the facet name. edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e id=IndexToRetrieve Add the key=value pairs for the facet to configure, for example id=IndexToRetrieve , and press Enter. For more information, see the example, View the configuration of a specific entry in the PeriodicEgressEndpoint facet within the Storage component . Examples View the configuration of the System component edgecmd Configuration System { \"Logging\": { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 }, \"HealthEndpoints\": [], \"Port\": { \"port\": 5590 }, \"Components\": [ { \"componentId\": \"Storage\", \"componentType\": \"EDS.Component\" } ] } View the configuration of the Logging facet within the Storage component edgecmd Configuration Storage Logging { \"logLevel\": \"Information\", \"logFileSizeLimitBytes\": 34636833, \"logFileCountLimit\": 31 } View the configuration of a specific entry in the PeriodicEgressEndpoint facet within the Storage component edgecmd Configuration Storage PeriodicEgressEndpoints id=Endpoint_1 { \"id\": \"Endpoint_1\", \"executionPeriod\": \"2.00:00:00\", \"name\": null, \"description\": null, \"enabled\": true, \"endpoint\": \"http://localhost:5590\", \"http:  localhost:5590\", \"clientId\": null, \"clientSecret\": null, \"userName\": \"user_54\", \"password\": \"***************\", \"validateEndpointCertificate\": true, \"tokenEndpoint\": null, \"debugExpiration\": null, \"namespaceId\": \"default\", \"backfill\": false, \"egressFilter\": null, \"streamPrefix\": null, \"typePrefix\": null }"
                                                                   },
    "V1/CommandLine/Retrieve EdgeCmd utility help.html":  {
                                                              "href":  "V1/CommandLine/Retrieve EdgeCmd utility help.html",
                                                              "title":  "Retrieve EdgeCmd utility help",
                                                              "keywords":  "Retrieve EdgeCmd utility help The EdgeCmd utility provides a \u0027Help\u0027 utility with instructions on how to use EdgeCmd utility. View general help instructions Complete the following to view help instructions on how to use the Edgecmd utility: Open command line. Type the following in the command line and press Enter. edgecmd Help View configuration facet help instructions Complete the following to view help instructions for a configuration facet that a registered component supports: Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e with the value that you want, and press Enter. edgecmd Help \u003ccomponentId\u003e For more information, see the example, Help for the System component . Note: The help output also provides examples of commands that you can run to configure the component. View specific configuration facet help instructions Complete the following to view help instructions for a specific facet within a component: Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e and \u003cfacetName\u003e with the value that you want, and press Enter. edgecmd Help \u003ccomponentId\u003e \u003cfacetName\u003e For more information, see the example, Help for the Port facet within the System component . Examples Help for the System component: edgecmd Help System --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027Logging\u0027 --------------------------------------------------------------------------------------------------------- LogLevel [Required] Desired log level settings. Options: Verbose, Information, Warning, Error, Fatal. LogFileSizeLimitBytes [Required] Maximum size in bytes of log files that the service will create for this component. Must be no less than 1000. LogFileCountLimit [Required] Maximum number of log files that the service will create for this component. Must be a positive integer. Example: ./edgecmd . edgecmd Configuration System Logging LogLevel=Warning Example: ./edgecmd . edgecmd Configuration System Logging LogFileSizeLimitBytes=32768 Example: ./edgecmd . edgecmd Configuration System Logging LogFileCountLimit=5 --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027HealthEndpoints\u0027 --------------------------------------------------------------------------------------------------------- Id [Optional] Id of existing configuration to be edited of removed. Endpoint [Required] URL of OMF destination UserName [Required group 1] User name used for authentication to PI Web API OMF endpoint. Password [Required group 1] Password used for authentication to PI Web API OMF endpoint. ClientId [Required group 2] Client ID used for authentication to OSIsoft Cloud Services. ClientSecret [Required group 2] Client Secret used for authentication to OSIsoft Cloud Services. Buffering [Optional] Set the buffering type for messages to this endpoint. Options are \u0027memory\u0027, \u0027disk\u0027 or \u0027none\u0027. Defaults to \u0027none\u0027. MaxBufferSizeMB [Optional] If an integer \u003e0, this is the limit on the maximum megabytes of data to buffer for messages to this endpoint. Useful for limiting memory or disk usage growth in the event of disconnection to the endpoint. If the buffer is full, old messages will be discarded for new messages. Defaults to 0. ValidateEndpointCertificate [Optional] If true, endpoint certificate will be validated (recommended). If false, any endpoint certificate will be accepted. OSIsoft strongly recommends using disabled endpoint certificate validation for testing purposes only. Note: Only one Required group must be specified. Group 1 for PI Web API or Group 2 for OCS. Example: ./edgecmd . edgecmd Configuration System HealthEndpoints Endpoint=endpointURL UserName=UserName Password=Password --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027Port\u0027 --------------------------------------------------------------------------------------------------------- Port [Required] The TCP port to bind this application host to (Range [1024,65535]) Example: ./edgecmd . edgecmd Configuration System Port Port=5590 --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027Components\u0027 --------------------------------------------------------------------------------------------------------- ComponentId [Required] ID of the hosted component. ComponentType [Required] Type of the hosted component. Example: ./edgecmd . edgecmd Configuration System Components ComponentId=Modus1 ComponentType=Modbus Help for the Port facet within the System component edgecmd Help System Port --------------------------------------------------------------------------------------------------------- Component System command-line options =\u003e \u0027Port\u0027 --------------------------------------------------------------------------------------------------------- Port [Required] The TCP port to bind this application host to (Range [1024,65535]) Example: ./edgecmd . edgecmd Configuration System Port Port=5590"
                                                          },
    "V1/CommandLine/Configure Edge Data Store.html":  {
                                                          "href":  "V1/CommandLine/Configure Edge Data Store.html",
                                                          "title":  "Configure Edge Data Store",
                                                          "keywords":  "Configure Edge Data Store Change all values of a facet Complete the following to change all values of a facet: Open command line. Type the componentId and facetName , followed by key=value pairs that you want to change. Then press Enter. Example: Change all values in the \u0027Logging\u0027 facet: edgecmd Configuration Storage Logging LogLevel=Warning LogFileSizeLimitBytes=32768 LogFileCountLimit=5 Configure key=value pairs in a facet Complete the following to configure any number of valid key=value pairs in a facet: Open command line. Type the componentId and facetName followed by the key=value pairs that you want to change, and press Enter. Example: Change a single value in the \u0027Logging\u0027 facet: edgecmd Configuration Storage Logging LogFileCountLimit=5 Add an entry to a collection configuration Complete the following to add an entry to a collection configuration: Open command line. Type the componentId and facetName followed by the key=value pairs, and press Enter. Example: Add the \u0027Health Endpoints\u0027 facet to the \u0027System\u0027 component: edgecmd Configuration System HealthEndpoints Id=endpoint_1 Endpoint=endpointURL UserName=UserName Password=Password Note: If an entry with the specified ID already exists, it will be updated based on the new key=value pairs. Configure with JSON Files You can also configure Edge Data Store by a JSON file input into the EdgeCmd application. A file import completely replaces the existing configurations; therefore, you cannot use it to change individual values in a facet without modifying others. Import bulk configuration Complete the following to import a bulk configuration: Open command line. Type the following in the command line, replacing \u003cPathToJsonFile\u003e with the path to the file, and press Enter. edgecmd Configuration file=\u003cPathToJsonFile\u003e Import facet specific configuration Complete the following to import a facet specific configuration file for a component: Open command line. Type the following in the command line, replacing \u003ccomponentId\u003e with the ID of the component, \u003cfacetName\u003e with the name of the facet, and \u003cPathToJsonFile\u003e with the path to the file. Then press Enter. edgecmd Configuration \u003ccomponentId\u003e \u003cfacetName\u003e file=\u003cPathToJsonFile\u003e Import facets configuration in bulk Complete the following to import a file with configuration for individual facets as a bulk file import operation: Note: The file must contain only information for the given component ID. Open command line. Type the file name as shown in the Bulk_Storage_Runtime.json example and press Enter. edgecmd Configuration file=\"~/Bulk_Storage_Runtime.json\" file=\"~ Bulk_Storage_Runtime.json\" Example: { \"Storage\": { \"Runtime\": { \"StreamStorageLimitMb\": 66, \"StreamStorageTargetMb\": 33, \"IngressDebugExpiration\": \"2020-07-08T01:00:00\", \"CheckpointRateInSec\": 6, \"TransactionLogLimitMB\": 350, \"EnableTransactionLog\": true } } } Note: The command only affects the specified key-value pairs for the \u0027Runtime\u0027 facet in the \u0027Storage\u0027 component, it does not change any other components or facets; however, import affects all key-value pairs in the facet. If you import the following example JSON file, the \u0027StreamStorageLimitMb\u0027 and \u0027StreamStorageTargetMb\u0027 values will be modified and the remaining values in the \u0027Runtime\u0027 facet will be reset to their default values (IngressDebugExpiration, CheckpointRateInSec, TransactionLogLimitMB, and EnableTransactionLog). { \"Storage\": { \"Runtime\": { \"StreamStorageLimitMb\": 66, \"StreamStorageTargetMb\": 33, } } }"
                                                      }
}
